{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bienvenue dans le notebook consacré au pipeline final du résumé"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "from tqdm import tqdm\r\n",
    "import pickle\r\n",
    "\r\n",
    "import os\r\n",
    "os.chdir(\"c:\\\\Users\\\\theo.roudil-valentin\\\\Documents\\\\Resume\\\\MLSUM\") # A modifier, et mettre votre dossier correspondant !\r\n",
    "\r\n",
    "from transformers import CamembertTokenizer,CamembertModel\r\n",
    "tok=CamembertTokenizer('MLSUM_tokenizer.model')\r\n",
    "camem=CamembertModel.from_pretrained(\"camembert-base\")\r\n",
    "\r\n",
    "import gensim\r\n",
    "W2V=gensim.models.Word2Vec.load(\"W2V_all.model\")\r\n",
    "\r\n",
    "import fats #On importe le module contenant toutes les fonctions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exemple de la fonction finale de résumé !\r\n",
    "Comme vous pouvez le constater, la fonction de résumé a beaucoup d'options, et c'est normal !\r\n",
    "Dans l'idéal, vous pouvez sélectionner le modèle qui vous convient le mieux. J'ai essayé de laisser un maximum de choix aux utilisateurs. \r\n",
    "\r\n",
    "**Attention** Cette fonction est __profonde__, autrement dit, elle est basée sur un grand nombre d'autres fonctions ou suites ou classes de fonctions, qui sont toutes dans le module **fats.py** (à télécharger impérativement donc). La modification, ou même la compréhension de cette fonction et de tout ce sur quoi elle repose vous demandera du temps. J'ai, dans la mesure du possible, laisser le maximum d'indication. Les fonctions du module proviennent toutes d'un travail préliminaire, dont vous pouvez retrouver les codes dans le dossier **Exploration**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Il faut que P soit une liste de listes de phrases, donc on se débrouille pour que ce soit le cas\r\n",
    "P= #Ajoutez votre propre liste, ou décommenter les suivantes (par deux) pour exemple\r\n",
    "\r\n",
    "#P=pickle.load(open('Paragraphes_exemple.pickle','rb'))\r\n",
    "\r\n",
    "#P=pickle.load(open('liste.pickle','rb')) #liste de Paragraphes\r\n",
    "#P=[i.split('.') for i in P.tolist()] # à activer si on a une liste de paragraphes qui ne soit pas de type liste (array), à commenter sinon\r\n",
    "\r\n",
    "# On lance par exemple pour les 3 premiers éléments\r\n",
    "resu,text_2=fats.Resume(texte=P[:3],\r\n",
    "                 DL=False, # True si on veut utiliser des modèles de Deep Learning, False sinon\r\n",
    "                 cpu=1, #le nombre de cpu à utiliser, préférez peu de CPU, pour la mémoire\r\n",
    "                 type_='TextRankBert', #le nom du modèle, si DL=False\r\n",
    "                 k=2, #Le nombre de phrases\r\n",
    "                 choose_model=None, #le nom du modèle de Deep Learning, le cas échéant\r\n",
    "                 tok='MLSUM_tokenizer.model', #le nom du tokenizer\r\n",
    "                 modele=camem, #modèle CamemBERT ou W2V selon le modèle choisi\r\n",
    "                 get_score_only=False,# est-ce qu'on veut juste le score et pas directement les phrases\r\n",
    "                 s=True,vs=12000,sp=1,tr=False,t=True,seuil=2,lem=False,sc=3)\r\n",
    "print(resu) \r\n",
    "#Pourquoi qu'un seul résultat ici ? Car P[1] et P[2] ne sont que des espaces ! Voilà pouvoir on refournit le texte correspondant, sans les espaces vides etc...\r\n",
    "text_2"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['commune saint yrieix perche etude impact']]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "resu"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Résolution (ne pas tenir compte)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "p,_=fats.make_text(P[:10],s=False)\r\n",
    "len(p)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "p[:10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#resu=fats.Resume(Paragraphes,DL=False,cpu=2,type_='TextRankBert',k=2,modele=camem,tok=\"MLSUM_tokenizer.model\",get_score_only=True)\r\n",
    "#Paragraphes[251+47]\r\n",
    "# longueur=[len(i) for i in Paragraphes]\r\n",
    "# import numpy as np\r\n",
    "# print(np.max(longueur))\r\n",
    "# seuil=80\r\n",
    "# nul=len([i for i in longueur if i>seuil])\r\n",
    "# print(nul)\r\n",
    "# print(round(nul/len(Paragraphes)*100,2),\"%\")\r\n",
    "\r\n",
    "# import matplotlib.pyplot as plt\r\n",
    "# fig,ax=plt.subplots(figsize=(16,12))\r\n",
    "# ax.hist(longueur)\r\n",
    "\r\n",
    "# ME=fats.Make_Embedding(tok=CamembertTokenizer(\"MLSUM_tokenizer.model\"),cpu=1)\r\n",
    "# input_ids,att_mask=ME.make_token(Paragraphes[251+47],1)\r\n",
    "# print(\"OK\")\r\n",
    "\r\n",
    "# import torch\r\n",
    "# c=camem(torch.tensor(input_ids[0:int(len(input_ids)/2)]),\r\n",
    "#            torch.tensor(att_mask[0:int(len(input_ids)/2)])).last_hidden_state.detach()\r\n",
    "\r\n",
    "# if len(input_ids)>70: #Au-delà de 70 phrases, camembert plante. On vérifie que le paragraphe en contient moins puis\r\n",
    "#     #on va découper le paragraphe pour que chaque bout fasse moins de 70\r\n",
    "#     # d'abord il convient de trouver le chiffre tq len(input_ids)/chiffre<70\r\n",
    "#     for i in range(2,100):\r\n",
    "#         if (len(input_ids)/i)<70:\r\n",
    "#             h=i\r\n",
    "#             print(h)\r\n",
    "#             break\r\n",
    "#         else:\r\n",
    "#             continue\r\n",
    "#     #une fois qu'on a ce chiffre h on y va :\r\n",
    "#     embeddings=[]\r\n",
    "#     for i in range(h):\r\n",
    "#         x_1=int(len(input_ids)*(i/h))\r\n",
    "#         x_2=int(len(input_ids)*((i+1)/h))\r\n",
    "#         print(x_1,x_2)\r\n",
    "#         embeddings.append(camem(torch.tensor(input_ids[x_1:x_2]),\r\n",
    "#            torch.tensor(att_mask[x_1:x_2])).last_hidden_state.detach())\r\n",
    "#     embeddings=torch.cat(embeddings)\r\n",
    " \r\n",
    "# MEB=fats.TextRank(\"MLSUM_tokenizer.model\",cpu=1).make_embedding_bert\r\n",
    "# MEB(Paragraphes[251+47],camem)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# P_=[i for i in P if not i[0].isspace()]\n",
    "# P_\n",
    "\n",
    "# for i in WC.remove_empty(P):\n",
    "#     try:\n",
    "#         mk(i)\n",
    "#         continue\n",
    "#     except:\n",
    "#         print(i,P.index(i))\n",
    "#         break\n",
    "\n",
    "# from joblib import Parallel,delayed\n",
    "# from functools import partial\n",
    "# mk=partial(fats.make_text,j=1,s=True,t=True,seuil=2,lem=False,sc=3)\n",
    "# #P_2=mk(P)\n",
    "# try:\n",
    "#     Text=Parallel(n_jobs=1)(delayed(mk)(t) for t in P)\n",
    "# except:\n",
    "#     Text=Parallel(n_jobs=1)(delayed(mk)(t) for t in WC.remove_empty(P))\n",
    "# text=[Text[i][0] for i in range(len(Text))]\n",
    "# mur=partial(fats.make_U_resume,type_='TextRankBert',k=2,cpu=1,modele=camem,tok_path='MLSUM_tokenizer.model',get_score_only=False)\n",
    "# mur(text[0])\n",
    "\n",
    "# for i in WC.remove_empty(text):\n",
    "#     try:\n",
    "#         mur(i)\n",
    "#     except:\n",
    "#         print(WC.remove_empty(text).index(i))\n",
    "#         break\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rendu=pickle.load(open('BSR_sortie.pickle','rb'))#[] if \n",
    "print(len(rendu))\n",
    "ms=fats.BERTScore(tok=\"MLSUM_tokenizer.model\",cpu=1).make_score\n",
    "\n",
    "#pas=1\n",
    "#s\n",
    "for i in tqdm(range(len(rendu),len(Paragraphes))):\n",
    "    resu=ms(Paragraphes[i])\n",
    "    #fats.Resume(Paragraphes[(i*2):(i+pas)*2],DL=False,cpu=1,type_='TextRankBert',k=2,modele=camem,tok=\"MLSUM_tokenizer.model\",get_score_only=True)\n",
    "    rendu.append(resu)\n",
    "               \n",
    "    if i%50==0:\n",
    "        pickle.dump(rendu,open('BSR_sortie.pickle','wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.7 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "interpreter": {
   "hash": "e34048b0732ca5da544928c261c6b0ec51b7f57de61b26cf2eebb756a9ee889a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}