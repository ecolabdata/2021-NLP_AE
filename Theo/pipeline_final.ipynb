{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a470b12-e901-4904-ac3b-810372a88ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import operator\n",
    "import psutil\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from transformers import CamembertTokenizer,CamembertModel\n",
    "from joblib import Parallel,delayed\n",
    "#import os\n",
    "#os.chdir(\"home/jovyan/work\")\n",
    "tok=CamembertTokenizer('MLSUM_tokenizer.model')\n",
    "camem=CamembertModel.from_pretrained(\"camembert-base\")\n",
    "\n",
    "W2V=gensim.models.Word2Vec.load(\"W2V_all.model\")\n",
    "import time\n",
    "import fats\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172e71c5-45a8-469b-9b0b-b8ceb127a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paragraphes= pickle.load(open('Paragraphes.pickle','rb'))#[i.split('.') for i in test.text.values.tolist()]\n",
    "print(Paragraphes[:10])\n",
    "Paragraphes=functools.reduce(operator.iconcat, Paragraphes, [])\n",
    "len(Paragraphes)\n",
    "# Paragraphes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b17fae-936b-4f3d-874b-6c178c522a72",
   "metadata": {},
   "source": [
    "# A copier-coller dans le code final Ruben !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aaa3faf-3bc8-4efb-ba59-eac0680c7d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['commune saint yrieix perche etude impact']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P=pickle.load(open('liste.pickle','rb')) #liste de Paragraphes\n",
    "# Il faut que P soit une liste de listes de phrases, donc on se débrouille pour que ce soit le cas\n",
    "P=[i.split('.') for i in P.tolist()] # à activer si on a une liste de paragraphes, à commenter sinon\n",
    "\n",
    "resu,text_2=fats.Resume(P[:3],\n",
    "                 DL=False, # True si on veut utiliser des modèles de Deep Learning, False sinon\n",
    "                 cpu=1, #le nombre de cpu à utiliser, préférez peu de CPU, pour la mémoire\n",
    "                 type_='TextRankBert', #le nom du modèle, si DL=False\n",
    "                 k=2, #Le nombre de phrases\n",
    "                 choose_model=None, #le nom du modèle de Deep Learning, le cas échéant\n",
    "                 tok='MLSUM_tokenizer.model', #le nom du tokenizer\n",
    "                 modele=camem, #modèle CamemBERT ou W2V selon le modèle choisi\n",
    "                 get_score_only=True,# est-ce qu'on veut juste le score et pas directement les phrases\n",
    "                 s=True)\n",
    "print(resu) \n",
    "#Pourquoi qu'un seul résultat ici ? Car P[1] et P[2] ne sont que des espaces ! Voilà pouvoir on refournit le texte correspondant, sans les espaces vides etc...\n",
    "text_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855fbd04-5472-40d5-b77b-62c66a1913cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "resu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c205df-0f0f-46a0-be0a-12cf97be50ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "p,_=fats.make_text(P[:10],s=False)\n",
    "len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f623a-6e5c-4ca0-b043-ef932c5b78b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9951d22-e52c-457f-9e4c-3bf869adc32e",
   "metadata": {},
   "source": [
    "## Résolution (ne pas tenir compte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97452e9-e0e0-4ef1-b5a4-4bab174631d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resu=fats.Resume(Paragraphes,DL=False,cpu=2,type_='TextRankBert',k=2,modele=camem,tok=\"MLSUM_tokenizer.model\",get_score_only=True)\n",
    "#Paragraphes[251+47]\n",
    "# longueur=[len(i) for i in Paragraphes]\n",
    "# import numpy as np\n",
    "# print(np.max(longueur))\n",
    "# seuil=80\n",
    "# nul=len([i for i in longueur if i>seuil])\n",
    "# print(nul)\n",
    "# print(round(nul/len(Paragraphes)*100,2),\"%\")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# fig,ax=plt.subplots(figsize=(16,12))\n",
    "# ax.hist(longueur)\n",
    "\n",
    "# ME=fats.Make_Embedding(tok=CamembertTokenizer(\"MLSUM_tokenizer.model\"),cpu=1)\n",
    "# input_ids,att_mask=ME.make_token(Paragraphes[251+47],1)\n",
    "# print(\"OK\")\n",
    "\n",
    "# import torch\n",
    "# c=camem(torch.tensor(input_ids[0:int(len(input_ids)/2)]),\n",
    "#            torch.tensor(att_mask[0:int(len(input_ids)/2)])).last_hidden_state.detach()\n",
    "\n",
    "# if len(input_ids)>70: #Au-delà de 70 phrases, camembert plante. On vérifie que le paragraphe en contient moins puis\n",
    "#     #on va découper le paragraphe pour que chaque bout fasse moins de 70\n",
    "#     # d'abord il convient de trouver le chiffre tq len(input_ids)/chiffre<70\n",
    "#     for i in range(2,100):\n",
    "#         if (len(input_ids)/i)<70:\n",
    "#             h=i\n",
    "#             print(h)\n",
    "#             break\n",
    "#         else:\n",
    "#             continue\n",
    "#     #une fois qu'on a ce chiffre h on y va :\n",
    "#     embeddings=[]\n",
    "#     for i in range(h):\n",
    "#         x_1=int(len(input_ids)*(i/h))\n",
    "#         x_2=int(len(input_ids)*((i+1)/h))\n",
    "#         print(x_1,x_2)\n",
    "#         embeddings.append(camem(torch.tensor(input_ids[x_1:x_2]),\n",
    "#            torch.tensor(att_mask[x_1:x_2])).last_hidden_state.detach())\n",
    "#     embeddings=torch.cat(embeddings)\n",
    " \n",
    "# MEB=fats.TextRank(\"MLSUM_tokenizer.model\",cpu=1).make_embedding_bert\n",
    "# MEB(Paragraphes[251+47],camem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7169b-e155-43cf-9220-e1b7f44878aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P_=[i for i in P if not i[0].isspace()]\n",
    "# P_\n",
    "\n",
    "# for i in WC.remove_empty(P):\n",
    "#     try:\n",
    "#         mk(i)\n",
    "#         continue\n",
    "#     except:\n",
    "#         print(i,P.index(i))\n",
    "#         break\n",
    "\n",
    "# from joblib import Parallel,delayed\n",
    "# from functools import partial\n",
    "# mk=partial(fats.make_text,j=1,s=True,t=True,seuil=2,lem=False,sc=3)\n",
    "# #P_2=mk(P)\n",
    "# try:\n",
    "#     Text=Parallel(n_jobs=1)(delayed(mk)(t) for t in P)\n",
    "# except:\n",
    "#     Text=Parallel(n_jobs=1)(delayed(mk)(t) for t in WC.remove_empty(P))\n",
    "# text=[Text[i][0] for i in range(len(Text))]\n",
    "# mur=partial(fats.make_U_resume,type_='TextRankBert',k=2,cpu=1,modele=camem,tok_path='MLSUM_tokenizer.model',get_score_only=False)\n",
    "# mur(text[0])\n",
    "\n",
    "# for i in WC.remove_empty(text):\n",
    "#     try:\n",
    "#         mur(i)\n",
    "#     except:\n",
    "#         print(WC.remove_empty(text).index(i))\n",
    "#         break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c15370-f0c2-46c5-a14d-6fd4c1bfda02",
   "metadata": {},
   "outputs": [],
   "source": [
    "rendu=pickle.load(open('BSR_sortie.pickle','rb'))#[] if \n",
    "print(len(rendu))\n",
    "ms=fats.BERTScore(tok=\"MLSUM_tokenizer.model\",cpu=1).make_score\n",
    "\n",
    "#pas=1\n",
    "#s\n",
    "for i in tqdm(range(len(rendu),len(Paragraphes))):\n",
    "    resu=ms(Paragraphes[i])\n",
    "    #fats.Resume(Paragraphes[(i*2):(i+pas)*2],DL=False,cpu=1,type_='TextRankBert',k=2,modele=camem,tok=\"MLSUM_tokenizer.model\",get_score_only=True)\n",
    "    rendu.append(resu)\n",
    "               \n",
    "    if i%50==0:\n",
    "        pickle.dump(rendu,open('BSR_sortie.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a070f61c-9de3-4c66-8559-f9e4db59504a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
