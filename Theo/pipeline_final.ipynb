{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bienvenue dans le notebook consacré au pipeline final du résumé"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "from tqdm import tqdm\r\n",
    "import pickle\r\n",
    "\r\n",
    "import os\r\n",
    "os.chdir(\"c:\\\\Users\\\\theo.roudil-valentin\\\\Documents\\\\Codes\") # A modifier, et mettre votre dossier correspondant !\r\n",
    "\r\n",
    "try:\r\n",
    "    os.chdir(\"2021-NLP_AE\\\\Theo\")\r\n",
    "except:\r\n",
    "    raise ValueError(\"Merci de décommenter la ligne verte au-dessus, et d'indiquer correctement le chemin vers le dossier contenant le git\")\r\n",
    "\r\n",
    "from transformers import CamembertTokenizer,CamembertModel\r\n",
    "tok=CamembertTokenizer('Model\\\\MLSUM_tokenizer.model')\r\n",
    "camem=CamembertModel.from_pretrained(\"camembert-base\")\r\n",
    "\r\n",
    "# import gensim\r\n",
    "# W2V=gensim.models.Word2Vec.load(\"Model\\\\W2V_all.model\")\r\n",
    "#Malheureusement ce modèle est trop gros pour être mis sur github, nous vous invitons à demander aux auteurs\r\n",
    "# de vous l'envoyer. Il en va également pour les modèles SMHA, Net. \r\n",
    "# Vous pouvez néanmoins faire tourner le pipeline sur le modèle CamemBERT, en réduisant le nombre de paragraphes\r\n",
    "# que vous donnez à la fonction, car CamemBERT est très gourmand en mémoire RAM !\r\n",
    "\r\n",
    "import fats #On importe le module contenant toutes les fonctions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exemple de la fonction finale de résumé !\r\n",
    "Comme vous pouvez le constater, la fonction de résumé a beaucoup d'options, et c'est normal !\r\n",
    "Dans l'idéal, vous pouvez sélectionner le modèle qui vous convient le mieux. J'ai essayé de laisser un maximum de choix aux utilisateurs. \r\n",
    "\r\n",
    "**Attention** Cette fonction est __profonde__, autrement dit, elle est basée sur un grand nombre d'autres fonctions ou suites ou classes de fonctions, qui sont toutes dans le module **fats.py** (à télécharger impérativement donc). La modification, ou même la compréhension de cette fonction et de tout ce sur quoi elle repose vous demandera du temps. J'ai, dans la mesure du possible, laisser le maximum d'indication. Les fonctions du module proviennent toutes d'un travail préliminaire, dont vous pouvez retrouver les codes dans le dossier **Exploration**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Il faut que P soit une liste de listes de phrases, donc on se débrouille pour que ce soit le cas\r\n",
    "P=pickle.load(open('test/Paragraphes_exemple.pickle','rb')) #Ajoutez votre propre liste, ou décommenter les suivantes (par deux) pour exemple\r\n",
    "\r\n",
    "#P=pickle.load(open('Paragraphes_exemple.pickle','rb'))\r\n",
    "\r\n",
    "#P=pickle.load(open('liste.pickle','rb')) #liste de Paragraphes\r\n",
    "#P=[i.split('.') for i in P.tolist()] # à activer si on a une liste de paragraphes qui ne soit pas de type liste (array), à commenter sinon\r\n",
    "\r\n",
    "# On lance par exemple pour les 3 premiers éléments\r\n",
    "resu,text_2=fats.Resume(texte=P[:3],\r\n",
    "                 DL=False, # True si on veut utiliser des modèles de Deep Learning, False sinon\r\n",
    "                 cpu=1, #le nombre de cpu à utiliser, préférez peu de CPU, pour la mémoire\r\n",
    "                 type_='TextRankBert', #le nom du modèle, si DL=False\r\n",
    "                 k=2, #Le nombre de phrases\r\n",
    "                 choose_model=None, #le nom du modèle de Deep Learning, le cas échéant\r\n",
    "                 tok='MLSUM_tokenizer.model', #le nom du tokenizer\r\n",
    "                 modele=camem, #modèle CamemBERT ou W2V selon le modèle choisi\r\n",
    "                 get_score_only=False,# est-ce qu'on veut juste le score et pas directement les phrases\r\n",
    "                 s=True,vs=12000,sp=1,tr=False,t=True,seuil=2,lem=False,sc=3,\r\n",
    "                 weights=True, # Pour les modèles TextRank, est-ce qu'on modifie les poids des phrases, \r\n",
    "                 # en augmentant les phrases présentes aux extrémités ? Oui\r\n",
    "                 alpha=0.2, # Par combien les augmente-t-on ?\r\n",
    "                 frac=0.25) # Quelle partie de la phrase surpondèrons-nous ? frac% du début et (1-frac)% de la fin\r\n",
    "                 \r\n",
    "print(resu) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Le processing du text a pris : 0.0 minutes\n",
      "La production des résumés a pris : 0.43 minutes\n",
      "[['les reds mohamed salah buteur sur penalty apres min ont etouffe tottenham lors une finale anglaise flamboyante samedi juin ligue des champions decrochant madrid leur sixieme couronne continentale', 'opportunite etait belle mohamed salah sorti sur blessure apres trente minutes dernier face real transforme force face hugo lloris pour glacer les supporteurs des spurs places derriere cage'], ['les reds pour pas decouvrir ont allonge jeu outre intenable salah seuls les lateraux trent alexander arnold une frappe trop croisee andy robertson tir claque par lloris ont instille quelques frissons dans torpeur ambiante', 'origi delivre liverpool son cote tottenham est enferre passer par axe mais ses approximations techniques ont empeche avoir des situations franchement brulantes exceptes deux tirs trop enleves sissoko christian eriksen contre mal negocie par son heung min'], ['maintenu dans ses textes cette specialite nationale une jubilation savoir que ecriture offre gouter', 'etait francais developpait les consequences intellectuelles stylistiques cette filiation']]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.7 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "interpreter": {
   "hash": "e34048b0732ca5da544928c261c6b0ec51b7f57de61b26cf2eebb756a9ee889a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}