# Dossier Th√©o - R√©sum√© automatique

Date de la derni√®re modification : **18/08/2021**

Bienvenue dans le dossier de Th√©o Roudil-Valentin contenant tous les travaux concernant le r√©sum√© automatique.

Vous trouverez tous les √©l√©ments de codes permettant de produire des r√©sum√©s.

# Table des mati√®res
1. [Contenu du dossier](#contenu)
2. [Prise en main](#prise)
3. [Le r√©sum√© automatique](#resume)
    1. [Approches](#approches)  
        a.[Deep Learning Oriented Extractive Summarizer](#appDL)  
        b.[TextRank Extractive Summarizer](#appTR)  
        c.[BertScore](#appBS)  
        d.[Lead-3 & RandomSummary](#appL3)  
    2. [R√©sultats](#resultats)  
4. [Sources et citations](#source) 
5. [Contacts](#contact)

**French Automatic Text Summarizer (fats)**  
Le code __fats.py__ est le module regroupant un ensemble de classes et fonctions li√© au projet notamment pour le nettoyage, la pr√©paration du texte et le d√©veloppement et l'application des mod√®les. Il est indispensable pour tous les fichiers qui se trouvent dans ce dossier. 


## Listes des codes et applications :
<a name="contenu"></a>

Ce dossier contient des codes et un dossier :
* **Exploration** : qui contient l'ensemble des codes pr√©liminaires qui ont amen√© au travail abouti que vous avez ci plus haut. Je les laisse √† but informatif et de compr√©hension.
* **Model** : qui contient certains des mod√®les n√©cessaires pour faire tourner les fonctions de r√©sum√©s. Malheureusement, √©tant donn√© la limite d'espace de git, je n'ai pas pu mettre tous les mod√®les disponibles. Si vous fa√Ætes partie du CGDD, vous pouvez y acc√©der via le fichier setup.py. Si vous fa√Ætes partie du MTE, vous devez pouvoir en demander l'acc√®s, ou demander √† l'√©quipe de l'Ecolab de vous fournir les donn√©es. Si vous √™tes ext√©rieur, vous pouvez demander √† l'√©quipe de vous les envoyer par un lien de t√©l√©chargement.

* __Note_technique.pdf__ : note concernant la strat√©gie envisag√©e pour le traitement Deep Learning du r√©sum√©, expliquant l'esprit et la m√©thode du travail. Elle n'est plus vraiment √† jour, et √† ce titre, les explications ci-dessous sont plus r√©centes, mais elle √©claire mieux de mani√®re conceptuelle les probl√®mes que nous rencontrons et les √©ventuelles solutions. De m√™me, une grande partie concerne les briques fondamentales des mod√®les que l'on va utiliser : les mod√®les BERT, l'attention, les transformers etc...
* __Paragraphes_exemple.pickle__ : exemple de paragraphes pour les deux pipelines r√©sum√©s. 
* __pipeline_final.ipynb__ : code permettant de lancer la fonction finale de r√©sum√©. Vous avez un large choix de mod√®les, qui sont disponibles dans le dossier [Model](./Model). De plu vous avez un exemple pr√©sent dans ce dossier.
* __pipeline_DL.ipynb__ : code permettant de lancer la fonction de r√©sum√© Deep Learning.
* __fats.py__ : fichier **module**, c'est-√†-dire comportant l'aboutissement de tout le travail fonctionnel sur le r√©sum√©. Il rassemble toutes les fonctions utiles pour cela. Il est appel√© tr√®s souvent au sein des codes aboutis, donc pensez √† bien le mettre dans votre dossier.

## Prise en main 
<a name="prise"></a>
Pour prendre en main ce dossier vous devez :
* **1.** d'abord cloner le repository en local (dans votre invite de commandes windows, mettez vous dans votre dossier choisi et entrez : **git clone https://github.com/ecolabdata/2021-NLP_AE.git** , attention au proxy si vous √™tes au bureau üòâ ! ) ;
* **2.** avoir install√© une version de python (conseil : la 3.6.7 64-bit) ;
* (**2.1** conseil √©galement : cr√©ez-vous un environnement virtuel, c'est plus sain, vous pouvez chercher sur le net c'est assez direct.) ;
* **3.** avoir install√© une interface python : par exemple Jupyter (classique), je conseille VS Code, qui est vraiment tr√®s utile, agr√©able et polyvalent ;
* **4.** avoir install√© les packages du document __requirements.txt__ disponible ;

Une fois que vous avez fait cela, vous avez (normalement) tout bon pour lancer le projet et ses exemples.  
Vous pouvez alors ouvrir votre interface python (Jupyter, VS Code ou autre), puis lancer les cellules (Ctrl + Enter ou shift + enter).  
Vous devrez probablement √† un moment donn√© renseigner votre chemin vers votre dossier ou vous avez t√©l√©charg√© les donn√©es, l'emplacement sera indiqu√© directement dans le code.  
N'oubliez cependant pas que tous les '\\' doivent √™tre soit remplac√©s par des '\\\\' ou des '/' ! Sinon python ne pourra pas trouver le chemin.  

Une fois que cela est fait, vous pouvez lancer les exemples.  
Vous pourrez de m√™me introduire vos propres donn√©es (attention au format et √† la dimension, mais cela est indiqu√© dans les pipelines).

## L'approche du r√©sum√© automatique
<a name="resume"></a>

Le r√©sum√© pratiqu√© ici est extractif, en accord avec les auditeurs de la DREAL Bretagne. C'est-√†-dire que l'on s√©lectionne les phrases les plus pertinentes de chaque paragraphe.

Nous avons d√©velopp√©s ici 4 types de mod√®les. Certains sont assez simples, d'autres bas√©s sur des techniques modernes de Deep Learning.  

### 1. Les approches et les m√©thodes associ√©es
<a name="approches"></a>
* **1.1** Une famille de mod√®les bas√©es sur du __Deep Learning__
* **1.2** Un mod√®le utilisant l'algorithme TextRank
* **1.3** Un mod√®le bas√© sur la similarit√© de l'embedding des phrases
* **1.4** Enfin une famille de mod√®le __benchmark__ pour la comparaison

Avant de d√©velopper des mod√®les d'extraction, il convient de nettoyer les phrases des paragraphes que l'on va tenter de r√©sum√© automatiquement. En effet, ces phrases sont pleines de marques de ponctuations, d'accents etc... qui peuvent venir rendre plus difficile l'apprentissage ou l'inf√©rence des mod√®les.  
C'est pourquoi nous avons enlev√© les √©l√©ments suivants :
* les accents
* la ponctuation
* les chiffres
* les articles et autres mots __vides__ (c'est-√†-dire pr√©sent trop souvent pour apporter de l'information)

#### 1.1 - Deep Learning Oriented Extractive Summarizer (DLOES)
<a name="appDL"></a>
Ces derni√®res ann√©es, les techniques de Deep Learning appliqu√©es au traitement du langage naturel se sont largement d√©velopp√©s et proposent des outils d√©sormais tr√®s puissants. Dans cette m√™me veine, l'Ecolab a d√©cid√© de tenter la cr√©ation d'un mod√®le de Deep Learning pour extraire les phrases importantes d'un paragraphe.  

Le travail d√©bute par plusieurs √©tapes de pre-processing :

1. Tout d'abord, il convient __tokenizer__ les mots, c'est-√†-dire de les couper en bouts (des __tokens__) encore plus petits. Pour cela on utilise un [_tokenizer_](https://github.com/google/sentencepiece), c'est-√†-dire un mod√®le capable de rep√©rer et d√©couper les mots au bonne endroit. 
2. Ensuite, nous avons transform√© ces listes de tokens en vecteur via un _embedding_, celui du mod√®le [CamemBERT](https://huggingface.co/transformers/model_doc/camembert.html).
3. Une fois ces repr√©sentations des phrases sous forme de vecteur r√©cup√©r√©es, nous pouvons les introduire dans les diff√©rents mod√®les de DL que nous avons construit.

Une des limites des mod√®les [BERT](https://github.com/google-research/bert) est la dimension fixe des objets en entr√©e : 512. Autrement dit, ces mod√®les qui re√ßoivent des vecteurs de 512 tokens, nous obligent par la m√™me occasion √† contraindre nos phrases √† faire 512 tokens. Cependant, il n'y a, a priori, aucune raison pour que ce soit le cas, le nombre des tokens des phrases pr√©sentes dans nos paragraphes n'a aucune raison d'√™tre √©gal ou inf√©rieur √† 512. Par cons√©quent, il faut trouver un moyen pour outrepasser cette limitation.  
Il est par exemple possible de couper le paragraphe et ne prendre que ses 512 premiers tokens, mais ce serait une grande limitation, et occulterait une trop grande partie de l'information disponible. Comme notre recherche a pour but le d√©veloppement d'un produit utilisable par les auditeurs de la DREAL Bretagne, nous ne pouvons nous permettre une telle perte.  
Une seconde approche serait de couper en parties √©gales les paragraphes, pour avoir des vecteurs poss√©dant la m√™me taille par paragraphes, ou une taille proche. Mais cela √©loignerait et d√©couperait trop l'information qui serait disparate.  
Enfin, une autre approche, celle que nous avons choisi, est celle de concat√©ner les tokens des phrases jusqu'√† atteindre la phrase dont les tokens feraient basculer le vecteur √† une dimension sup√©rieure √† 512. Par exemple, soit un paragraphe compos√© de 10 phrases, nous tokenisons au fur et √† mesure les phrases lorsque √† la phrase 6, la somme des tokens est de 509. Nous sommes donc tr√®s proche de 512. Disons que la phrase 7 poss√®de 15 tokens, si on l'ajoute au vecteur pr√©sent nous aurons une dimension de 509+15>512. On clot donc le vecteur pr√©sent, en le pavant de 1 jusqu'√† 512 (soit trois tokens 1) puis on cr√©e un nouveau vecteur, auquel on ajoutera la phrase 7 ainsi que les suivantes, ainsi de suie.  
Le pavement de 1 peut para√Ætre √©tonnant, mais c'est quelque chose de tr√®s classique dans les mod√®les BERT, o√π lorsque il n'y a pas assez de tokens pour atteindre 512, on rajoute des 1 pour indiquer au mod√®le qu'ils ne sont pas des vrais tokens informatifs. D'ailleurs, l'entr√©e de ces mod√®les n'est pas constitu√©e que d'un vecteur de tokens, mais √©galement d'un masque compos√© de 0 et de 1 pour de nouveau indiquer au mod√®le les tokens (indices ici) qui sont informatifs et les autres qui sont ici pour paver, c'est-√†-dire remplir les trous. Par exemple dans notre cas, le vecteur de dimension 509 aura un masque associ√© de 1 en position de 0 √† 509, puis de 0 de 510 √† 512.  
Par cons√©quent, certains paragraphes sont donc s√©par√©s sur plusieurs vecteurs. Bien entendu, cette approche √† des limites, qui sont proches de la seconde approche propos√©e. Nous en sommes conscients, mais c'est pour nous l'approche qui est le meilleur compromis.

Une fois ces vecteurs de tokens pr√™ts, nous les ins√©rons dans CamemBERT, puis prenons la derni√®re couche cach√©e, de dimension 512x768, que nous ins√©rons ensuite dans les mod√®les d√©velopp√©s ci-dessous.

Ceux-ci sont au nombre de 4. Ils sont relativement rudimentaires, par manque de temps, mais proposent et utilisent, pour certains, des m√©thodes assez modernes. Ils sont tous √©crits en [torch](https://pytorch.org/) et sont tous disponibles dans le module __fats.py__ pr√©sent√© plus haut.

* **Simple Linear Model** : une structure d'une seule couche lin√©aire suivie d'une LeakyRelu.
* **Multi Linear Model** : trois couches lin√©aires successives, dont les deux premi√®res sont suivies d'une Softmax.
* **Convolutional Network** : une premi√®re couche de convolution (1D) suivie d'une LeakyRelu et d'un pooling, une deuxi√®me convolution (1D) toujours associ√©e √† un LeakyRelu, deux couches lin√©aires et LeakyRelu pour finir sur une lin√©aire.
* **SelfMultiHeadAttention Model** : une couche de Self Multi Head Attention suivie d'une LayerNormalization, puis de deux couches lin√©aires toutes deux suivies d'une LeakyRelu

La fonction de perte est une perte [L1](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html) re-pond√©r√©e pour sur-pond√©rer les tokens de d√©but de phrases importantes, qui sont donc g√©n√©ralement 3 sur une dimension de 512. Leur faible nombre m'a pouss√© √† les surpond√©rer pour faire en sorte que le mod√®le se concentre sur ces tokens. 
L'optimiseur est un [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)

Les mod√®les sont entra√Æn√©s sur les donn√©es [MLSUM](https://github.com/huggingface/datasets/tree/master/datasets/mlsum), sur des batch de taille 64, sur un GPU NVIDIA disponible sur le datalab [Onyxia](https://datalab.sspcloud.fr/home) du [SSP Cloud](https://www.sspcloud.fr/).

#### 1.2 - TextRank for Extractive Summarizer (TRES)
<a name="appTR"></a>
Nous avons appliqu√© le mod√®le TextRank, d√©rivation du mod√®le PageRank de Google sur des donn√©es textuelles pour de l'extraction de mots-cl√©s ou phrases, √† nos donn√©es MLSUM. Ce dernier mod√®le est bas√© sur la th√©orie des graphes et propose d'√©tudier les relations entre diff√©rents objets. TextRank prend comme objets des phrases ou des mots par exemples. Dans notre cas, nous sommes int√©ress√©s par l'extraction de phrases. Donc nos objets, formellement nos sommets (__vertices__), sont donc nos phrases de paragraphe. L'algorithme va donc chercher des liens d'importance au sein d'un r√©seau. Avant toute chose il convient donc d'obtenir ce r√©seau.

Dans le cas de l'extraction de phrases, cela peut √™tre une matrice de similarit√© entre les phrases. Pour cela nous avons besoin d'un embedding. Nous avons choisi deux embeddings diff√©rents, qui aboutissent donc √† deux mod√®les diff√©rents : TRW (pour TextRankWord2Vec) bas√© sur l'embedding de Word2Vec, et TRB (pour TextRankBert), quant √† lui bas√© sur CamemBERT. Une fois les embeddings obtenus, il suffit de calculer la similarit√©, cosinus dans notre cas, entre les phrases pour obtenir la matrice, soit le r√©seau.

Une fois le r√©seau obtenu, nous avons appliqu√© l'algorithme PageRank disponible via le module [networkx](https://networkx.org/). 

Dans notre cas, le graphe est pond√©r√©, car la relation entre deux phrases n'est pas binaire, mais repr√©sent√© la force de la similarit√©. Le score pond√©r√© est introduit par les auteurs de TextRank (voir [Sources](#sources)).
#### 1.3 - BertScore
<a name="appBS"></a>
Ce mod√®le propose d'extraire les phrases les plus importantes de mani√®re rudimentaire. Comme expliqu√© plus haut, les phrases doivent passer par un processus pour √™tre transform√©s en vecteurs. D'abord la tokenization, puis l'embedding via CamemBERT. L'id√©e est de choisir les phrases qui repr√©sentent le mieux l'id√©e g√©n√©rale du paragraphe. En termes vectorielles, supposons qu'il existe un vecteur qui repr√©sente parfaitement __l'id√©e principale__ du paragraphe, on cherche √† extraire les vecteurs qui sont les plus proches de ce __vecteur id√©e principale__ et ainsi faire √©merger les phrases les plus importantes.  
La question d√©sormais est : **comment obtenir ce vecteur id√©e g√©n√©rale** ? Il convient d'en chercher une approximation _suffisante_, c'est-√†-dire suffisamment bonne pour que les r√©sum√©s aient du sens et soient utilisables. Notre proposition est d'utiliser la moyenne des repr√©sentations vectorielles des phrases comme approximation de l'id√©e g√©n√©rale.   
Ce mod√®le repose donc sur l'hypoth√®se suivante : **la moyenne des repr√©sentations vectorielles des phrases constitue une approximation suffisamment bonne du vecteur de l'id√©e principale.**

Une fois ce proxy obtenu, nous pouvons calculer la similarit√© cosinus de chaque phrase √† cette _id√©e principale_. Le r√©sum√© consiste alors en les phrases les plus proches de cette derni√®re.

#### 1.4 - Lead-3 et RandomSummary
<a name="appL3"></a>
Le premier consiste simplement √† prendre les trois premi√®res phrases d‚Äôun paragraphe. L‚Äôhypoth√®se sous jacente est que l‚Äôinformation importante est souvent √©nonc√©e d√®s le d√©but d‚Äôun paragraphe et ce pour annoncer au lecteur la teneur principale du paragraphe consid√©r√©.
Le second, RandomSummary (RS), consiste √† s√©lectionner les phrases al√©atoirement dans le paragraphe. Il constitue ainsi un le premier plancher que les diff√©rents mod√®les pr√©c√©dents devront surpasser. En effet, ces derniers doivent, pour √™tre int√©ressants, √™tre plus performant que le hasard. Le mod√®le Lead-3 constitue donc un second plancher qualitatif √† d√©passer.

### 2. R√©sultats 
<a name="resultats"></a>
|M√©triques| L3  | RS  | TRW  | TRB  | BS  | Multi | Simple | SMHA | Net |
|---|---|---|---|---|---|---|---|---|---|
True Positive Mean|0.2291|0.1726|0.2166|0.1686|0.1517|0.2029|0.2015|0.1696|0.1812|
False Positive Mean| 0.1273  | 0.1275 | 0.1254  |  0.1364 | 0.1335  |0.1333|0.1325|0.1389|0.1367|
False Negative Mean|  0.7709 |  0.8274 | 0.7834  | 0.8314  |  0.8483 |0.7971|0.7985|0.8304|0.8188|
Precision| 0.2291  | 0.1858  | 0.2166  |  0.1686 | 0.1517  |0.2029|0.2015|0.1696|0.1812|
Recall   |0.2291|0.1726|0.2166|0.1686|0.1517|0.2029|0.2015|0.1696|0.1812|
F1       |0.2291|0.1770|0.2166|0.1686|0.1517|0.2029|0.2015|0.1696|0.1812|
Rang |1|6|2|8|9|3|4|7|5|

## Sources et citations
<a name="sources"/></a>

[Camembert: a tasty french language model](https://arxiv.org/abs/1911.03894)  
[SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://arxiv.org/abs/1808.06226)  
[MLSUM: The Multilingual Summarization Corpus](https://arxiv.org/abs/2004.14900)  
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)  
[Decoupled Weight Decay Regularization (AdamW)](https://arxiv.org/abs/1711.05101)  
[TextRank: Bringing Order into Texts](https://aclanthology.org/W04-3252.pdf)  
[The anatomy of a large-scale hypertextual Web search engine (PageRank)](https://www.sciencedirect.com/science/article/pii/S016975529800110X)  
[Neural Machine Translation by Jointly Learning to Align and Translate (Attention)](https://arxiv.org/abs/1409.0473)  
[Attention Is All You Need (Transformers)](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)  
## Contacts
<a name="contact"></a>
[Th√©o Roudil-Valentin](mailto:theo.roudil-valentin@ensae.fr)
