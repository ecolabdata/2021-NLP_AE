# Dossier Th√©o - R√©sum√© automatique

Date de la derni√®re modification : **03/08/2021**

Bienvenue dans le dossier de Th√©o Roudil-Valentin contenant tous les travaux concernant le r√©sum√© automatique.

Vous trouverez tous les √©l√©ments de codes permettant de produire des r√©sum√©s.

**French Automatic Text Summarizer (fats)**  
Le code __fats.py__ est le module regroupant un ensemble de classes et fonctions li√© au projet notamment pour le nettoyage, la pr√©paration du texte et le d√©veloppement et l'application des mod√®les. Il est indispensable pour tous les fichiers qui se trouvent dans ce dossier. 

Ce dossier contient des codes et un dossier :
* **Exploration** : qui contient l'ensemble des codes pr√©liminaires qui ont amen√© au travail abouti que vous avez ci plus haut. Je les laisse √† but informatif et de compr√©hension.

## Listes des codes et applications :

* __Note_technique.pdf__ : note concernant la strat√©gie envisag√©e pour le traitement Deep Learning du r√©sum√©, expliquant l'esprit et la m√©thode du travail.
* __pipeline_final.ipynb__ :
* __pipeline_DL.ipynb__ :
* __fats.py__ : fichier **module**, c'est-√†-dire comportant l'aboutissement de tout le travail fonctionnel sur le r√©sum√©. Il rassemble toutes les fonctions utiles pour cela. Il est appel√© tr√®s souvent au sein des codes aboutis, donc pensez √† bien le mettre dans votre dossier.

## Prise en main 

Pour prendre en main ce dossier vous devez :
* **1.** d'abord cloner le repository en local (dans votre invite de commandes windows, mettez vous dans votre dossier choisi et entrez : **git clone https://github.com/ecolabdata/2021-NLP_AE.git** , attention au proxy si vous √™tes au bureau üòâ ! ) ;
* **2.** avoir install√© une version de python (conseil : la 3.6.7 64-bit) ;
* (**2.1** conseil √©galement : cr√©ez-vous un environnement virtuel, c'est plus sain, vous pouvez chercher sur le net c'est assez direct.) ;
* **3.** avoir install√© une interface python : par exemple Jupyter (classique), je conseille VS Code, qui est vraiment tr√®s utile, agr√©able et polyvalent ;
* **4.** avoir install√© les packages du document __requirements.txt__ disponible ;

Une fois que vous avez fait cela, vous avez (normalement) tout bon pour lancer le projet et ses exemples.  
Vous pouvez alors ouvrir votre interface python (Jupyter, VS Code ou autre), puis lancer les cellules (Ctrl + Enter ou shift + enter).  
Vous devrez probablement √† un moment donn√© renseigner votre chemin vers votre dossier ou vous avez t√©l√©charg√© les donn√©es, l'emplacement sera indiqu√© directement dans le code.  
N'oubliez cependant pas que tous les '\\' doivent √™tre soit remplac√©s par des '\\\\' ou des '/' ! Sinon python ne pourra pas trouver le chemin.  

Une fois que cela est fait, vous pouvez lancer les exemples.  
Vous pourrez de m√™me introduire vos propres donn√©es (attention au format et √† la dimension, mais cela est indiqu√© dans les pipelines).

## L'approche du r√©sum√© automatique

petite intro

### 1. Les approches et les m√©thodes associ√©es

* **1.1** Une famille de mod√®les bas√©es sur du __Deep Learning__
* **1.2** Un mod√®le utilisant l'algorithme TextRank
* **1.3** Un mod√®le bas√© sur la similarit√© de l'embedding des phrases
* **1.4** Enfin une famille de mod√®le __benchmark__ pour la comparaison

Avant de d√©velopper des mod√®les d'extraction, il convient de nettoyer les phrases des paragraphes que l'on va tenter de r√©sum√© automatiquement. En effet, ces phrases sont pleines de marques de ponctuations, d'accents etc... qui peuvent venir rendre plus difficile l'apprentissage ou l'inf√©rence des mod√®les.  
C'est pourquoi nous avons enlev√© les √©l√©ments suivants :
* les accents
* la ponctuation
* les chiffres
* les articles et autres mots __vides__ (c'est-√†-dire pr√©sent trop souvent pour apporter de l'information)
* 

#### 1.1 - Deep Learning Oriented Extractive Summarizer (DLOES)
Ces derni√®res ann√©es, les techniques de Deep Learning appliqu√©es au traitement du langage naturel se sont largement d√©velopp√©s et proposent des outils d√©sormais tr√®s puissants. Dans cette m√™me veine, l'Ecolab a d√©cid√© de tenter la cr√©ation d'un mod√®le de Deep Learning pour extraire les phrases importantes d'un paragraphe.  

Le travail d√©bute par plusieurs √©tapes de pre-processing :

1. Tout d'abord, il convient __tokenizer__ les mots, c'est-√†-dire de les couper en bouts (des __tokens__) encore plus petits. Pour cela on utilise un [_tokenizer_](https://github.com/google/sentencepiece), c'est-√†-dire un mod√®le capable de rep√©rer et d√©couper les mots au bonne endroit. 
2. Ensuite, nous avons transform√© ces listes de tokens en vecteur via un _embedding_, celui du mod√®le [CamemBERT](https://huggingface.co/transformers/model_doc/camembert.html).
3. Une fois ces repr√©sentations des phrases sous forme de vecteur r√©cup√©r√©es, nous pouvons les introduire dans les diff√©rents mod√®les de DL que nous avons construit.

Une des limites des mod√®les [BERT](https://github.com/google-research/bert) est la dimension fixe des objets en entr√©e : 512. Autrement dit, ces mod√®les qui re√ßoivent des vecteurs de 512 tokens, nous obligent par la m√™me occasion √† contraindre nos phrases √† faire 512 tokens. Cependant, il n'y a, a priori, aucune raison pour que ce soit le cas, le nombre des tokens des phrases pr√©sentes dans nos paragraphes n'a aucune raison d'√™tre √©gal ou inf√©rieur √† 512. Par cons√©quent, il faut trouver un moyen pour outrepasser cette limitation.  
Il est par exemple possible de couper le paragraphe et ne prendre que ses 512 premiers tokens, mais ce serait une grande limitation, et occulterait une trop grande partie de l'information disponible. Comme notre recherche a pour but le d√©veloppement d'un produit utilisable par les auditeurs de la DREAL Bretagne, nous ne pouvons nous permettre une telle perte.  
Une seconde approche serait de couper en parties √©gales les paragraphes, pour avoir des vecteurs poss√©dant la m√™me taille par paragraphes, ou une taille proche. Mais cela √©loignerait et d√©couperait trop l'information qui serait disparate.  
Enfin, une autre approche, celle que nous avons choisi, est celle de concat√©ner les tokens des phrases jusqu'√† atteindre la phrase dont les tokens feraient basculer le vecteur √† une dimension sup√©rieure √† 512. Par exemple, soit un paragraphe compos√© de 10 phrases, nous tokenisons au fur et √† mesure les phrases lorsque √† la phrase 6, la somme des tokens est de 509. Nous sommes donc tr√®s proche de 512. Disons que la phrase 7 poss√®de 15 tokens, si on l'ajoute au vecteur pr√©sent nous aurons une dimension de 509+15>512. On clot donc le vecteur pr√©sent, en le pavant de 1 jusqu'√† 512 (soit trois tokens 1) puis on cr√©e un nouveau vecteur, auquel on ajoutera la phrase 7 ainsi que les suivantes, ainsi de suie.  
Le pavement de 1 peut para√Ætre √©tonnant, mais c'est quelque chose de tr√®s classique dans les mod√®les BERT, o√π lorsque il n'y a pas assez de tokens pour atteindre 512, on rajoute des 1 pour indiquer au mod√®le qu'ils ne sont pas des vrais tokens informatifs. D'ailleurs, l'entr√©e de ces mod√®les n'est pas constitu√©e que d'un vecteur de tokens, mais √©galement d'un masque compos√© de 0 et de 1 pour de nouveau indiquer au mod√®le les tokens (indices ici) qui sont informatifs et les autres qui sont ici pour paver, c'est-√†-dire remplir les trous. Par exemple dans notre cas, le vecteur de dimension 509 aura un masque associ√© de 1 en position de 0 √† 509, puis de 0 de 510 √† 512.  
Par cons√©quent, certains paragraphes sont donc s√©par√©s sur plusieurs vecteurs. Bien entendu, cette approche √† des limites, qui sont proches de la seconde approche propos√©e. Nous en sommes conscients, mais c'est pour nous l'approche qui est le meilleur compromis.

Les mod√®les que nous avons d√©velopp√© sont au nombre de 4. Ils sont relativement rudimentaires, par manque de temps, mais proposent et utilisent, pour certains, des m√©thodes assez modernes. Ils sont tous √©crits en [torch](https://pytorch.org/) et sont tous disponibles dans le module __fats.py__ pr√©sent√© plus haut.

* **Simple Linear Model** : une structure d'une seule couche lin√©aire suivie d'une LeakyRelu.
* **Multi Linear Model** : trois couches lin√©aires successives, dont les deux premi√®res sont suivies d'une Softmax.
* **Convolutional Network** : une premi√®re couche de convolution (1D) suivie d'une LeakyRelu et d'un pooling, une deuxi√®me convolution (1D) toujours associ√©e √† un LeakyRelu, deux couches lin√©aires et LeakyRelu pour finir sur une lin√©aire.
* **SelfMultiHeadAttention Model** : une couche de Self Multi Head Attention suivie d'une LayerNormalization, puis de deux couches lin√©aires toutes deux suivies d'une LeakyRelu

La fonction de perte est une perte L1 re-pond√©r√©e pour sur-pond√©rer les tokens de d√©but de phrases importantes, qui sont donc g√©n√©ralement 3 sur une dimension de 512. Leur faible nombre m'a pouss√© √† les surpond√©rer pour faire en sorte que le mod√®le se concentre sur ces tokens. 

Les mod√®les sont entra√Æn√©s sur les donn√©es [MLSUM](https://github.com/huggingface/datasets/tree/master/datasets/mlsum), sur des batch de taille 64, sur un GPU NVIDIA disponible sur le datalab [Onyxia](https://datalab.sspcloud.fr/home) du [SSP Cloud](https://www.sspcloud.fr/).

#### 1.2 - TextRank for Extractive Summarizer (TRES)
#### 1.3 - BertScore
Ce mod√®le propose d'extraire les phrases les plus importantes de mani√®re rudimentaire. Comme expliqu√© plus haut, les phrases doivent passer par un processus pour √™tre transform√©s en vecteurs. D'abord la tokenization, puis l'embedding via CamemBERT. L'id√©e est de choisir les phrases qui repr√©sentent le mieux l'id√©e g√©n√©rale du paragraphe. En termes vectorielles, supposons qu'il existe un vecteur qui repr√©sente parfaitement _l'id√©e g√©n√©rale_ du paragraphe, on cherche √† extraire les vecteurs qui sont les plus proches de ce _vecteur id√©e g√©n√©rale_ et ainsi faire √©merger les phrases les plus importantes.  
La question d√©sormais est : **comment obtenir ce vecteur id√©e g√©n√©rale** ? Il convient d'en chercher une approximation _suffisante_, c'est-√†-dire suffisamment bonne pour que les r√©sum√©s aient du sens et soient utilisables. Notre proposition est d'utiliser la moyenne des repr√©sentations vectorielles des phrases comme approximation de l'id√©e g√©n√©rale.   
Ce mod√®le repose donc sur l'hypoth√®se suivante : **la moyenne des repr√©sentations vectorielles des phrases constitue une approximation suffisamment bonne du vecteur de l'id√©e g√©n√©rale.**


#### 1.4 - Lead-3 et RandomSummary

### 2. R√©sultats 

## Citation

## Sources :
[Camembert: a tasty french language model](https://arxiv.org/abs/1911.03894)  
[SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://arxiv.org/abs/1808.06226)  
[MLSUM: The Multilingual Summarization Corpus](https://arxiv.org/abs/2004.14900)  
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
