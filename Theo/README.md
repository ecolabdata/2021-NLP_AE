# Dossier Th√©o - R√©sum√© automatique

Date de la derni√®re modification : **03/08/2021**

Bienvenue dans le dossier de Th√©o Roudil-Valentin contenant tous les travaux concernant le r√©sum√© automatique.

Vous trouverez tous les √©l√©ments de codes permettant de produire des r√©sum√©s.

**French Automatic Text Summarizer (fats)**  
Le code __fats.py__ est le module regroupant un ensemble de classes et fonctions li√© au projet notamment pour le nettoyage, la pr√©paration du texte et le d√©veloppement et l'application des mod√®les. Il est indispensable pour tous les fichiers qui se trouvent dans ce dossier. 

Ce dossier contient des codes et un dossier :
* **Exploration** : qui contient l'ensemble des codes pr√©liminaires qui ont amen√© au travail abouti que vous avez ci plus haut. Je les laisse √† but informatif et de compr√©hension.

## Listes des codes et applications :

* __Note_technique.pdf__ : note concernant la strat√©gie envisag√©e pour le traitement Deep Learning du r√©sum√©, expliquant l'esprit et la m√©thode du travail.
* __pipeline_final.ipynb__ :
* __pipeline_DL.ipynb__ :
* __fats.py__ : fichier **module**, c'est-√†-dire comportant l'aboutissement de tout le travail fonctionnel sur le r√©sum√©. Il rassemble toutes les fonctions utiles pour cela. Il est appel√© tr√®s souvent au sein des codes aboutis, donc pensez √† bien le mettre dans votre dossier.

## Prise en main 

Pour prendre en main ce dossier vous devez :
* **1.** d'abord cloner le repository en local (dans votre invite de commandes windows, mettez vous dans votre dossier choisi et entrez : **git clone https://github.com/ecolabdata/2021-NLP_AE.git** , attention au proxy si vous √™tes au bureau üòâ ! ) ;
* **2.** avoir install√© une version de python (conseil : la 3.6.7 64-bit) ;
* (**2.1** conseil √©galement : cr√©ez-vous un environnement virtuel, c'est plus sain, vous pouvez chercher sur le net c'est assez direct.) ;
* **3.** avoir install√© une interface python : par exemple Jupyter (classique), je conseille VS Code, qui est vraiment tr√®s utile, agr√©able et polyvalent ;
* **4.** avoir install√© les packages du document __requirements.txt__ disponible ;

Une fois que vous avez fait cela, vous avez (normalement) tout bon pour lancer le projet et ses exemples.  
Vous pouvez alors ouvrir votre interface python (Jupyter, VS Code ou autre), puis lancer les cellules (Ctrl + Enter ou shift + enter).  
Vous devrez probablement √† un moment donn√© renseigner votre chemin vers votre dossier ou vous avez t√©l√©charg√© les donn√©es, l'emplacement sera indiqu√© directement dans le code.  
N'oubliez cependant pas que tous les '\\' doivent √™tre soit remplac√©s par des '\\\\' ou des '/' ! Sinon python ne pourra pas trouver le chemin.  

Une fois que cela est fait, vous pouvez lancer les exemples.  
Vous pourrez de m√™me introduire vos propres donn√©es (attention au format et √† la dimension, mais cela est indiqu√© dans les pipelines).

## L'approche du r√©sum√© automatique

petite intro

### 1. Les approches et les m√©thodes associ√©es

* **1.1** Une famille de mod√®les bas√©es sur du __Deep Learning__
* **1.2** Un mod√®le utilisant l'algorithme TextRank
* **1.3** Un mod√®le bas√© sur la similarit√© de l'embedding des phrases
* **1.4** Enfin une famille de mod√®le __benchmark__ pour la comparaison

Avant de d√©velopper des mod√®les d'extraction, il convient de nettoyer les phrases des paragraphes que l'on va tenter de r√©sum√© automatiquement. En effet, ces phrases sont pleines de marques de ponctuations, d'accents etc... qui peuvent venir rendre plus difficile l'apprentissage ou l'inf√©rence des mod√®les.  
C'est pourquoi nous avons enlev√© les √©l√©ments suivants :
* les accents
* la ponctuation
* les chiffres
* les articles et autres mots __vides__ (c'est-√†-dire pr√©sent trop souvent pour apporter de l'information)
* 

#### 1.1 - Deep Learning Oriented Extractive Summarizer (DLOES)
Ces derni√®res ann√©es, les techniques de Deep Learning appliqu√©es au traitement du langage naturel se sont largement d√©velopp√©s et proposent des outils d√©sormais tr√®s puissants. Dans cette m√™me veine, l'Ecolab a d√©cid√© de tenter la cr√©ation d'un mod√®le de Deep Learning pour extraire les phrases importantes d'un paragraphe.  

Le travail d√©bute par plusieurs √©tapes de pre-processing :

1. Tout d'abord, il convient __tokenizer__ les mots, c'est-√†-dire de les couper en bouts (des __tokens__) encore plus petits. Pour cela on utiliser un _tokenizer_, c'est-√†-dire un mod√®le capable de rep√©rer et d√©couper les mots au bonne endroit. 
2. Ensuite, nous avons transform√© ces listes de tokens en vecteur via un _embedding_, celui du mod√®le [CamemBERT](https://huggingface.co/transformers/model_doc/camembert.html).
3. Une fois ces repr√©sentations des phrases sous forme de vecteur r√©cup√©r√©es, nous pouvons les introduire dans les diff√©rents mod√®les de DL que nous avons construit.

Ces mod√®les sont au nombre de 4. Ils sont relativement rudimentaires, par manque de temps, mais proposent et utilisent, pour certains, des m√©thodes assez modernes.  

* **Simple Linear Model** :
* **Multi Linear Model** :
* **Convolutional Network** :
* **SelfMultiHeadAttention Model** :


#### 1.2 - TextRank for Extractive Summarizer (TRES)
#### 1.3 - BertScore
#### 1.4 - Lead-3 et RandomSummary

### 2. R√©sultats 

## Citation

## Sources :
*[Camembert: a tasty french language model](https://arxiv.org/abs/1911.03894)
