{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook pour les métriques des modèles\r\n",
    "\r\n",
    "Malheureusement, une grande partie de ce NB a été perdu suite à une mauvaise manip' git... \r\n",
    "Seule la fin est à jour."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install torch\r\n",
    "!pip install gensim==3.8.3\r\n",
    "!pip install unidecode\r\n",
    "!pip install sentencepiece\r\n",
    "!pip install transformers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from joblib.parallel import cpu_count\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "import pickle\r\n",
    "import time\r\n",
    "import functools\r\n",
    "import operator\r\n",
    "from joblib import Parallel,delayed\r\n",
    "from functools import partial\r\n",
    "import sentencepiece as spm \r\n",
    "import psutil\r\n",
    "from tqdm import tqdm\r\n",
    "import pickle\r\n",
    "import os\r\n",
    "import re\r\n",
    "from transformers.utils.dummy_pt_objects import CamembertModel\r\n",
    "from transformers.utils.dummy_sentencepiece_objects import CamembertTokenizer\r\n",
    "from unidecode import unidecode\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import gensim\r\n",
    "import sys\r\n",
    "import gc\r\n",
    "import fats"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "Paragraphes=pickle.load(open('Paragraphes_.pickle','rb'))\r\n",
    "# Paragraphes=functools.reduce(operator.iconcat, Paragraphes, [])\r\n",
    "\r\n",
    "# from fats import Make_Embedding\r\n",
    "from transformers import CamembertTokenizer,CamembertModel\r\n",
    "tok=CamembertTokenizer('MLSUM_tokenizer.model')\r\n",
    "camem=CamembertModel.from_pretrained(\"camembert-base\")\r\n",
    "\r\n",
    "# nphrase=2\r\n",
    "# TR=TextRank(tok_path='MLSUM_tokenizer.model',cpu=1)\r\n",
    "# BS=BERTScore('MLSUM_tokenizer.model',cpu=1)\r\n",
    "\r\n",
    "# W2V=gensim.models.Word2Vec.load(\"W2V_all.model\")\r\n",
    "# TRB=partial(TR.make_resume,type='bert',modele=camem,k=nphrase,get_score_only=True)\r\n",
    "# TRW=partial(TR.make_resume,type='word2vec',modele=W2V,k=nphrase,get_score_only=True)\r\n",
    "# BSR=BS.make_score\r\n",
    "# L3=partial(Lead_3,k=nphrase)\r\n",
    "# RS=partial(Random_summary,get_index_only=True)\r\n",
    "\r\n",
    "# taille=100\r\n",
    "# start=[]\r\n",
    "# end=[]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "TR=TextRank(tok_path='MLSUM_tokenizer.model',cpu=1)\r\n",
    "e,d=TR.make_embedding_bert(Paragraphes[6],camem=camem)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fichiers=[i for i in os.listdir() if 'BSR_sortie' in i]\r\n",
    "a=[] if len(fichiers)==0 else pickle.load(open(fichiers[0],'rb'))\r\n",
    "for i in tqdm(range(len(a),len(Paragraphes))):\r\n",
    "    x=BSR(Paragraphes[i])\r\n",
    "    a.append(x)\r\n",
    "    del x\r\n",
    "    gc.collect()\r\n",
    "    if i%10==0:\r\n",
    "        pickle.dump(a,open(\"BSR_sortie.pickle\",'wb'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 4/23508 [00:14<22:39:13,  3.47s/it]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "start.append(time.time())\r\n",
    "TRB_sortie=Parallel(2)(delayed(TRB)(i) for i in tqdm(Paragraphes[:taille]))\r\n",
    "# TRB_sortie=TRB(Paragraphes[0])\r\n",
    "pickle.dump(TRB_sortie,open('test/TRB_sortie.pickle','wb'))\r\n",
    "end.append(time.time())\r\n",
    "print(\"TRB :\",round((end[-1]-start[-1])/60,2),\"minutes\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "TRB_sortie=TRB(Paragraphes[:10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "TRB_sortie_2=[] #pickle.load(open('TRB_sortie.pickle','rb'))\r\n",
    "longueur=[]\r\n",
    "for i in range(len(TRB_sortie_2),len(Paragraphes)):\r\n",
    "    longueur.append(len(Paragraphes[i]))\r\n",
    "    start.append(time.time())\r\n",
    "    TRB_sortie_2.append(TRB(Paragraphes[i]))\r\n",
    "    end.append(time.time())\r\n",
    "    if i%250==0:\r\n",
    "        print(\"Achevé :\",round((i/len(Paragraphes))*100,2),\"%\")\r\n",
    "        pickle.dump(TRB_sortie_2,open(\"TRB_sortie.pickle\",'wb'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Achevé : 0.0 %\n",
      "Achevé : 1.05 %\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BSR_sortie=pickle.load(open('BSR_sortie.pickle','rb'))\r\n",
    "erreur=pickle.load(open('BSR_erreur.pickle','rb'))\r\n",
    "BSR_sortie"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BSR_sortie=[]\r\n",
    "erreur=[]\r\n",
    "for i in range(len(Paragraphes)):\r\n",
    "    try:\r\n",
    "        x=BSR(Paragraphes[i])\r\n",
    "        BSR_sortie.append(x)\r\n",
    "    except:\r\n",
    "        print(i)\r\n",
    "        print(\"Unexpected error:\", sys.exc_info())\r\n",
    "        erreur.append(i)\r\n",
    "    if i%250==0:\r\n",
    "        print(\"Achevé :\",round((i/len(Paragraphes))*100,2),\"%\")\r\n",
    "        pickle.dump(BSR_sortie,open(\"BSR_sortie.pickle\",'wb'))\r\n",
    "        pickle.dump(erreur,open(\"BSR_erreur.pickle\",'wb'))\r\n",
    "        print(\"Nombre d'erreurs :\",round(len(erreur)/len(Paragraphes)*100,2),\"%\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "Unexpected error: (<class 'ValueError'>, ValueError('only one element tensors can be converted to Python scalars'), <traceback object at 0x7fafd6866f50>)\n",
      "Achevé : 0.0 %\n",
      "Nombre d'erreurs : 0.0 %\n",
      "1\n",
      "Unexpected error: (<class 'KeyboardInterrupt'>, KeyboardInterrupt(), <traceback object at 0x7fafd5c77d70>)\n",
      "2\n",
      "Unexpected error: (<class 'ValueError'>, ValueError('only one element tensors can be converted to Python scalars'), <traceback object at 0x7fafd5c40870>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "BSR(Paragraphes[torch.randint(len(Paragraphes),(1,))])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.9443, 0.9410, 0.9669, 0.6170, 0.9655, 0.9092, 0.9209, 0.9330, 0.7975,\n",
       "        0.9526, 0.9336, 0.8616, 0.7908, 0.9259, 0.9285],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "BSR_sortie=pickle.load(open('BSR_sortie.pickle','rb'))\r\n",
    "len(BSR_sortie)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import fats\r\n",
    "for i in range(1,6):\r\n",
    "    s=time.time()\r\n",
    "    resu=TR.make_resume(Paragraphes[:i],\r\n",
    "                     cpu=1,\r\n",
    "                     type_='TextRankBert',\r\n",
    "                     k=2,\r\n",
    "                     modele=camem,\r\n",
    "                     tok=\"MLSUM_tokenizer.model\",\r\n",
    "                     get_score_only=True)\r\n",
    "    print((time.time()-s)/60/i)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0869313398996989\n",
      "0.07791940172513326\n",
      "0.08362084759606254\n",
      "0.09673363069693247\n",
      "0.10170523166656494\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import fats\r\n",
    "resu=fats.Resume(Paragraphes[0:2],\r\n",
    "                     DL=False,\r\n",
    "                     cpu=1,\r\n",
    "                     type_='TextRankBert',k=2,modele=camem,\r\n",
    "                     tok=\"MLSUM_tokenizer.model\",get_score_only=True)\r\n",
    "resu"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[2, 3], [0, 10]]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "rendu=pickle.load(open('TRB_sortie.pickle','rb'))\r\n",
    "len(rendu)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2801"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rendu=pickle.load(open('TRB_sortie.pickle','rb'))#[] if \r\n",
    "print(len(rendu))\r\n",
    "import fats\r\n",
    "pas=1\r\n",
    "\r\n",
    "for i in tqdm(range(len(rendu),int(len(Paragraphes)/(pas+1)))):\r\n",
    "    resu=fats.Resume(Paragraphes[(i*2):(i+pas)*2],\r\n",
    "                     DL=False,\r\n",
    "                     cpu=1,\r\n",
    "                     type_='TextRankBert',k=2,modele=camem,\r\n",
    "                     tok=\"MLSUM_tokenizer.model\",get_score_only=True)\r\n",
    "    rendu.append(resu)\r\n",
    "               \r\n",
    "    if i%100==0:\r\n",
    "        pickle.dump(rendu,open('TRB_sortie.pickle','wb'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/9098 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2801\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  2%|▏         | 198/9098 [38:13<23:15:10,  9.41s/it] "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sortie=[]\r\n",
    "name='Net'\r\n",
    "for i in tqdm(range(len(Paragraphes))):\r\n",
    "    r=fats.make_DL_resume(Paragraphes[i:i+1],cpu=2,choose_model=name,k=2,camem=camem,get_score_only=True)\r\n",
    "    sortie.append(r)\r\n",
    "pickle.dump(sortie,open(name+'_sortie.pickle','wb'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/23799 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "  0%|          | 48/23799 [01:16<9:04:41,  1.38s/it] "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}