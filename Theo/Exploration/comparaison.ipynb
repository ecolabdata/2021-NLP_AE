{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook plus élaboré pour l'élaboration des métriques et de la comparaison des modèles\r\n",
    "\r\n",
    "Malheureusement, une grande partie de ce NB a été perdu à cause d'une fausse manip' git...\r\n",
    "Un autre fichier .py sera utilisable et plus concis."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# A faire tourner que si vous n'avez pas déjà installé les packages nécessaires\r\n",
    "!pip install gensim==3.8.3\r\n",
    "!pip install unidecode\r\n",
    "!pip install torch\r\n",
    "!pip install sentencepiece\r\n",
    "!pip install transformers\r\n",
    "!pip install bs4\r\n",
    "!pip install networkx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from joblib.parallel import cpu_count\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "import pickle\r\n",
    "import time\r\n",
    "import functools\r\n",
    "import operator\r\n",
    "from joblib import Parallel,delayed\r\n",
    "from functools import partial\r\n",
    "import sentencepiece as spm \r\n",
    "import psutil\r\n",
    "from tqdm import tqdm\r\n",
    "import pickle\r\n",
    "import os\r\n",
    "import re\r\n",
    "from transformers.utils.dummy_pt_objects import CamembertModel\r\n",
    "from transformers.utils.dummy_sentencepiece_objects import CamembertTokenizer\r\n",
    "from unidecode import unidecode\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import gensim\r\n",
    "import sys\r\n",
    "import gc\r\n",
    "import fats"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "N=[f for f in os.listdir() if (f.split('.')[-1]=='pickle') and ('sortie' in f)]\r\n",
    "N=N[1:]\r\n",
    "N"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Net_sortie_3.pickle',\n",
       " 'TRW_sortie.pickle',\n",
       " 'TRB_sortie_finale.pickle',\n",
       " 'SMHA_sortie_3.pickle',\n",
       " 'L3_sortie.pickle',\n",
       " 'BSR_sortie.pickle',\n",
       " 'Simple_sortie_finale.pickle',\n",
       " 'RS_sortie.pickle',\n",
       " 'Multi_sortie_finale.pickle']"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "index_2=pickle.load(open('index_2.pickle','rb'))\r\n",
    "trace=pickle.load(open('trace_test.pickle','rb'))\r\n",
    "score=pickle.load(open('score.pickle','rb'))\r\n",
    "score=[score[i] for i in index_2]\r\n",
    "assert len(score)==len(trace)\r\n",
    "s=Parallel(5)(delayed(fats.make_new_paragraphes)(i,j) for i,j in zip(score,trace))\r\n",
    "s=functools.reduce(operator.iconcat,s,[])\r\n",
    "P=pickle.load(open('Paragraphes_.pickle','rb'))\r\n",
    "assert len(P)==len(s)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "for n in N:\r\n",
    "    SMHA=pickle.load(open(n,'rb'))\r\n",
    "    try:\r\n",
    "        Parallel(5)(delayed(fats.make_new_sortie)(i,j) for i,j in zip(s,SMHA))\r\n",
    "    except:\r\n",
    "        print(n)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "L3_sortie_.pickle\n",
      "Net_sortie_3.pickle\n",
      "SMHA_sortie_3.pickle\n",
      "BSR_sortie.pickle\n",
      "RS_sortie.pickle\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "np.mean([len(i) for i in s])/2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9.714294718265473"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "BSR=pickle.load(open('BSR_sortie.pickle','rb'))\r\n",
    "np.sum([1 for i in range(len(s)) if len(s[i])!=len(BSR[i])])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "22152"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "sortie=pickle.load(open(N[3],'rb'))\r\n",
    "sortie[0],s[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([2, 3],\n",
       " tensor([0.1555, 0.0826, 0.1421, 0.0972, 0.1317, 0.0945, 0.1363, 0.0868, 0.0838,\n",
       "         0.0979, 0.0958, 0.0903, 0.0905, 0.0478, 0.0964, 0.1024, 0.0928]))"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "L3=pickle.load(open(N[0],'rb'))\r\n",
    "l=[i for i in range(len(s)) if len(s[i])==1]\r\n",
    "for i in l:\r\n",
    "    L3[i]=L3[i][:1]\r\n",
    "\r\n",
    "    pickle.dump(L3,open('L3_sortie.pickle','wb'))\r\n",
    "L3[l[0]]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "Net=pickle.load(open('Net_sortie_3.pickle','rb'))\r\n",
    "dico=pickle.load(open('dico_comparaison.pickle','rb'))\r\n",
    "dico.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['phrase', 'score', 'erreur', 'trace'])"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "len(dico['erreur'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5034"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "np.sum([len(dico_sortie['Simple_sortie_finale'][i])==len(s[i]) for i in range(len(s))])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "23799"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "erreur=[]\r\n",
    "B=pickle.load(open('BSR_sortie.pickle','rb'))\r\n",
    "BSR=[]\r\n",
    "for i in range(len(s)):\r\n",
    "    try:\r\n",
    "        BSR.append(fats.make_new_sortie(s[i],B[i]))\r\n",
    "    except:\r\n",
    "        erreur.append(i)\r\n",
    "erreur"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[201]"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "dico_sortie={}\r\n",
    "for n in N:\r\n",
    "    sortie=pickle.load(open(n,'rb'))\r\n",
    "    try:\r\n",
    "        dico_sortie[n.split('.')[0]]=Parallel(5)(delayed(fats.make_new_sortie)(i,j) for i,j in zip(s,sortie))\r\n",
    "    except:\r\n",
    "        continue\r\n",
    "dico_sortie.keys()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x00'.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-841a9c14298a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdico_sortie\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msortie\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdico_sortie\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_new_sortie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msortie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\x00'."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def comparaison(f,j=2):\r\n",
    "    dico=pickle.load(open(dico,'rb'))\r\n",
    "    l=[i for i in range(len(dico['score'])) if len(dico['phrase'][i])==len(dico['score'][i]) ]\r\n",
    "    score_vrai=[dico['score'][i] for i in l]\r\n",
    "#     fichiers=[i for i in os.listdir() if (name in i) and ('.pickle' in i)]\r\n",
    "    sortie=pickle.load(open(f,'rb'))\r\n",
    "    if len(sortie)==23799:\r\n",
    "        simple=[sortie[i] for i in range(len(sortie)) if i not in dico['erreur']]\r\n",
    "        simple2=[simple[i] for i in l]\r\n",
    "        simple3=Parallel(j)(delayed(make_new_sortie)(i,j) for i,j in zip(score_vrai,simple2))\r\n",
    "    \r\n",
    "    assert len(simple3)==len(score_vrai)\r\n",
    "        \r\n",
    "    F1=F1_score()\r\n",
    "    tp=[]\r\n",
    "    fp=[]\r\n",
    "    fn=[]\r\n",
    "    p=[]\r\n",
    "    r=[]\r\n",
    "    f=[]\r\n",
    "    \r\n",
    "    for i in tqdm(range(len(simple3))):\r\n",
    "        tp.append(F1.true_positive_mean(simple3[i],score_vrai[i]))\r\n",
    "        fp.append(F1.false_positive_mean(simple3[i],score_vrai[i]))\r\n",
    "        fn.append(F1.false_negative_mean(simple3[i],score_vrai[i]))\r\n",
    "        p.append(F1.precision(simple3[i],score_vrai[i]))\r\n",
    "        r.append(F1.recall(simple3[i],score_vrai[i]))\r\n",
    "        f.append(F1(simple3[i],score_vrai[i]))\r\n",
    "        \r\n",
    "    mtp=torch.mean(torch.tensor(tp))\r\n",
    "    mfp=torch.mean(torch.tensor(fp))\r\n",
    "    mfn=torch.mean(torch.tensor(fn))\r\n",
    "    mp=torch.mean(torch.tensor(p))\r\n",
    "    mr=torch.mean(torch.tensor(r))\r\n",
    "    mf=torch.mean(torch.tensor(f))\r\n",
    "    resultat=[mtp,mfp,mfn,mp,mr,mf]\r\n",
    "    \r\n",
    "    return resultat"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}