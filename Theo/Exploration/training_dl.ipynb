{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1131465e-7515-4a87-b052-b3f529f97b72",
   "metadata": {},
   "source": [
    "# Notebook d'entraînement \n",
    "L'idée de ce notebook est de faciliter le lancement et la récupération des modèles entraînés en capitalisant sur les codes existants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b79af3-fb88-4792-9569-ae2cda07d1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import sklearn\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4af9c8-a08a-49eb-bb87-a7cf5590b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unidecode\n",
    "#!pip install transformers\n",
    "#!pip install networkx\n",
    "from fats import F1_score,Weighted_Loss,Simple_Classifier,Multi_Linear_Classifier,SMHA_Linear_classifier,Net,training_loop_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faeff179-fc90-485e-98de-1c6d07927f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de batchs : 2095\n",
      "Taille du batch : 64\n"
     ]
    }
   ],
   "source": [
    "dataloader_2=pickle.load(open('train_loader_2.pickle','rb'))\n",
    "print(\"Nombre de batchs :\",len(dataloader_2))\n",
    "for _,batch in enumerate(dataloader_2):\n",
    "    print(\"Taille du batch :\",len(batch[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626ef098-fb03-4fab-988e-3c26700732b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "from transformers import CamembertModel,CamembertConfig,AdamW\n",
    "camem1=CamembertModel(CamembertConfig())\n",
    "camem2=CamembertModel.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e751923-61b9-496d-b524-bb49a232dddc",
   "metadata": {},
   "source": [
    "## Prépration des modèles et fonctions de pertes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35035b5f-91a7-49bd-a09a-3b2d4948607e",
   "metadata": {},
   "source": [
    "Ici on ne va déclarer que les modèles et pas les optimiseurs car il faut d'abord placer le modèle sur GPU puis ensuite déclarer l'optimiseur, sinon ce dernier peut se tromper entre les devices (entre GPU et CPU donc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7390bed9-0548-46cf-b985-d88cfc9f5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlc=Multi_Linear_Classifier(camem2.config.hidden_size)\n",
    "#mlc_optimizer_SGD=optim.SGD(mlc.parameters(), lr=0.001, momentum=0.09)\n",
    "#mlc_optimizer_Adam=optim.AdamW(mlc.parameters(), lr=0.009, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "\n",
    "slc=Simple_Classifier(camem2.config.hidden_size)\n",
    "#slc_optimizer=optim.AdamW(slc.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "#SGD(slc.parameters(), lr=0.001, momentum=0.09)\n",
    "\n",
    "att_lin_c=SMHA_Linear_classifier(torch.Size([512,768]),8,768)\n",
    "#path='SMHA_Linear_classifier.pt'\n",
    "#att_lin_c.load_state_dict(torch.load(path))\n",
    "#att_lin_c_optimizer=optim.AdamW(att_lin_c.parameters(), lr=0.009, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "#.SGD(att_lin_c.parameters(), lr=0.001, momentum=0.09)\n",
    "#att_lin_c_optimizer.load_state_dict(torch.load(path[:-3]+'_AdamW.pt'))\n",
    "\n",
    "convnet=Net(2**8,2**6,2,2,2,2)\n",
    "#convnet_optimizer=optim.AdamW(convnet.parameters(), lr=0.009, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "#SGD(convnet.parameters(), lr=0.001, momentum=0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa46f75e-eec9-468e-8c8f-17ed472abbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  1.1563, 256.0000])\n"
     ]
    }
   ],
   "source": [
    "score=F1_score()\n",
    "\n",
    "alpha=0.1\n",
    "weights=torch.Tensor([(1/(1-alpha))*1/((512-20)/512),(1/alpha)*1/((20)/512)])\n",
    "#weights=torch.Tensor([(1/(1-alpha))*1/((1)/512),(1/alpha)*1/((512-1)/512)])\n",
    "#weights=torch.Tensor([1,1])\n",
    "print(weights)\n",
    "loss_3=Weighted_Loss(weight=weights,loss_type='sum',binary=False)\n",
    "loss_2=Weighted_Loss(weight=weights,loss_type='L1',binary=False)\n",
    "loss=nn.MSELoss()#nn.L1Loss()\n",
    "Loss=[loss,loss_2,loss_3]\n",
    "epochs=10\n",
    "\n",
    "Models=[mlc,slc,att_lin_c,convnet]\n",
    "\n",
    "data=dataloader_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19456771-ec4c-41e6-b30a-d12830761f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def check_weights_update(model,loss,optim,dev=False):\n",
    "    param1=list(model.parameters())[0].clone()\n",
    "    input=torch.rand(torch.Size([3,512,768]))\n",
    "    target=torch.rand(torch.Size([3,512]))\n",
    "    \n",
    "    if dev:\n",
    "        input=input.to(torch.device('cuda'))\n",
    "        target=target.to(torch.device('cuda'))\n",
    "    \n",
    "    sortie=model(input)\n",
    "    optim.zero_grad()\n",
    "    ouais=loss(sortie,target)\n",
    "    ouais.backward()\n",
    "    optim.step()\n",
    "    param=list(model.parameters())[0].clone()\n",
    "    out=bool(1-torch.equal(param1.data,param.data))\n",
    "    del input,target,sortie,ouais\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return out\n",
    "#check_weights_update(Models[0],Loss[0],Optimizers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf1f6b-adce-404d-80e9-92e984d519be",
   "metadata": {},
   "source": [
    "## Première boucle d'entraînement\n",
    "D'abord un premier entraînement, on récupère les modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b175e74-28cc-4518-9569-ded9400fc7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2095 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle : Multi_Linear_Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 1453/2095 [7:43:25<3:41:58, 20.75s/it]"
     ]
    }
   ],
   "source": [
    "for num_model in range(len(Models)):\n",
    "    \n",
    "    Models[num_model]=Models[num_model].to(device)\n",
    "    optimizer=optim.AdamW(Models[num_model].parameters(), lr=0.009, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "    \n",
    "    if check_weights_update(Models[num_model],Loss[0],optimizer,dev=True):\n",
    "        \n",
    "        model,optimizer,trainin_stats=training_loop_gpu( #On entraîne les modèles au fur et à mesure\n",
    "            Models[num_model],\n",
    "            optimizer,\n",
    "            data,\n",
    "            score,\n",
    "            Loss,\n",
    "            epochs,\n",
    "            camem2,\n",
    "            device\n",
    "        )\n",
    "        \n",
    "        pickle.dump(training_stats,open('training_stats_'+str(Models[num_model]).split('(')[0],'wb'))\n",
    "        torch.save(model.state_dict(), str(Models[num_model]).split('(')[0]+\".pt\")\n",
    "        torch.save(optimizer.state_dict(), str(Models[num_model]).split('(')[0]+'_'+str(optimizer).split(' (')[0]+\".pt\")\n",
    "        print(\"Model saved!\")        \n",
    "        Models[num_model]=Models[num_model].to('cpu')\n",
    "    \n",
    "    else:\n",
    "        print(\"Le modèle\",str(Models[num_model]).split('(')[0],\"n'a pas pu être entraîné !\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d593c1a8-4120-4d5d-bc91-3302fa047b69",
   "metadata": {},
   "source": [
    "## Deuxième entraînement\n",
    "On reprend les modèles et on poursuit l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdc59e3b-f2a4-493f-a376-970cb29a1500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Multi_Linear_Classifier(\n",
       "  (linear1): Linear(in_features=768, out_features=384, bias=True)\n",
       "  (linear2): Linear(in_features=384, out_features=128, bias=True)\n",
       "  (linear3): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (Lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Models_2=pickle.load(open('Models.pickle','rb'))\n",
    "Models_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b02e5195-2235-464e-b3dc-399f6ae8bed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Models[0].load_state_dict(Models_2[0].state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e2706-fa22-46b7-92d3-6823d5d66f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2095 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle : Multi_Linear_Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 595/2095 [3:47:22<9:39:58, 23.20s/it] "
     ]
    }
   ],
   "source": [
    "Optimizer=[]\n",
    "\n",
    "for num_model in range(len(Models)):\n",
    "    \n",
    "    Models[num_model].load_state_dict(Models_2[num_model].state_dict())\n",
    "    Models[num_model]=Models[num_model].to(device)\n",
    "    optimizer=optim.AdamW(Models[num_model].parameters(), lr=0.009, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "    \n",
    "    if check_weights_update(Models[num_model],Loss[0],optimizer,dev=True):\n",
    "        \n",
    "        model,optimizer,trainin_stats=training_loop_gpu( #On entraîne les modèles au fur et à mesure\n",
    "            Models[num_model],\n",
    "            optimizer,\n",
    "            data,\n",
    "            score,\n",
    "            Loss,\n",
    "            epochs,\n",
    "            camem2,\n",
    "            device\n",
    "        )\n",
    "        \n",
    "        Optimizer.append(optimizer)\n",
    "        pickle.dump(training_stats,open('training_stats_'+str(Models[num_model]).split('(')[0],'wb'))\n",
    "        torch.save(model.state_dict(), str(Models[num_model]).split('(')[0]+\".pt\")\n",
    "        torch.save(optimizer.state_dict(), str(Models[num_model]).split('(')[0]+'_'+str(optimizer).split(' (')[0]+\".pt\")\n",
    "        print(\"Model saved!\")        \n",
    "        Models[num_model]=Models[num_model].to('cpu')\n",
    "    \n",
    "    else:\n",
    "        print(\"Le modèle\",str(Models[num_model]).split('(')[0],\"n'a pas pu être entraîné !\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f70beb-612b-45c0-bb27-31918427d9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd3f0a9b-26e0-4389-959a-8d3d96ae8fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Models,open('Models_v2.pickle','wb'))\n",
    "pickle.dump(Optimizer,open('Optimizer_v2.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27530a80-a69e-488f-9037-bb4766a00eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(trainin_stats,open('training_stat_mystere.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b51aa-7b3f-459f-b9e1-60d5c52c88b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv=camem2(ouais[0],ouais[1])\n",
    "y=Models[2](tv.last_hidden_state.to(device))\n",
    "score(y,ouais[4].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3931378e-54bb-4bcc-b71f-2c25506c8402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Multi_Linear_Classifier(\n",
       "  (linear1): Linear(in_features=768, out_features=384, bias=True)\n",
       "  (linear2): Linear(in_features=384, out_features=128, bias=True)\n",
       "  (linear3): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (Lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Models[0]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89db7b64-1d62-4a2c-9673-2e5d1dfa11d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'Training Loss MSE': 6.215948306254961e-05,\n",
       "  'Training Loss L1': 0.02474360048930941,\n",
       "  'Training Loss sum': 108.61064178539631,\n",
       "  'Training f1 score': 0.04962713702414146,\n",
       "  'Training precision score': 0.05459411998883974},\n",
       " {'epoch': 2,\n",
       "  'Training Loss MSE': 4.2184651274488835e-05,\n",
       "  'Training Loss L1': 0.02296307543314869,\n",
       "  'Training Loss sum': 100.80674801555624,\n",
       "  'Training f1 score': 0.057260481533116546,\n",
       "  'Training precision score': 0.06058804635536415},\n",
       " {'epoch': 3,\n",
       "  'Training Loss MSE': 4.08952362934527e-05,\n",
       "  'Training Loss L1': 0.022724188774432944,\n",
       "  'Training Loss sum': 99.75225820609664,\n",
       "  'Training f1 score': 0.060648992218450604,\n",
       "  'Training precision score': 0.06570418561610943},\n",
       " {'epoch': 4,\n",
       "  'Training Loss MSE': 4.012278862065399e-05,\n",
       "  'Training Loss L1': 0.022422738146561427,\n",
       "  'Training Loss sum': 98.4216074613512,\n",
       "  'Training f1 score': 0.056994129784920904,\n",
       "  'Training precision score': 0.0649281363027406},\n",
       " {'epoch': 5,\n",
       "  'Training Loss MSE': 3.958636858563888e-05,\n",
       "  'Training Loss L1': 0.02221506621537459,\n",
       "  'Training Loss sum': 97.50621345538228,\n",
       "  'Training f1 score': 0.055965465108079385,\n",
       "  'Training precision score': 0.0657690416434713},\n",
       " {'epoch': 6,\n",
       "  'Training Loss MSE': 3.9238710874879984e-05,\n",
       "  'Training Loss L1': 0.022152945015197153,\n",
       "  'Training Loss sum': 97.24299912190949,\n",
       "  'Training f1 score': 0.05784936556348084,\n",
       "  'Training precision score': 0.06621159356629791},\n",
       " {'epoch': 7,\n",
       "  'Training Loss MSE': 3.9308241063252154e-05,\n",
       "  'Training Loss L1': 0.02213570921724053,\n",
       "  'Training Loss sum': 97.17089833917368,\n",
       "  'Training f1 score': 0.05622988266777025,\n",
       "  'Training precision score': 0.06570998566238508},\n",
       " {'epoch': 8,\n",
       "  'Training Loss MSE': 3.914931556314112e-05,\n",
       "  'Training Loss L1': 0.02213378861759898,\n",
       "  'Training Loss sum': 97.15089213660337,\n",
       "  'Training f1 score': 0.05732193058601143,\n",
       "  'Training precision score': 0.0662399386671463},\n",
       " {'epoch': 9,\n",
       "  'Training Loss MSE': 3.947664153243155e-05,\n",
       "  'Training Loss L1': 0.02218249868557106,\n",
       "  'Training Loss sum': 97.38087900020626,\n",
       "  'Training f1 score': 0.057083655415471814,\n",
       "  'Training precision score': 0.06624822258140207},\n",
       " {'epoch': 10,\n",
       "  'Training Loss MSE': 3.918230366530669e-05,\n",
       "  'Training Loss L1': 0.022130215070199282,\n",
       "  'Training Loss sum': 97.15490477204607,\n",
       "  'Training f1 score': 0.05704950257015541,\n",
       "  'Training precision score': 0.06638215988749963}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainin_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd926062-c9b9-4293-afc4-295d546179ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2095 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle : Multi_Linear_Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2095 [00:35<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-721a0035724d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model,optimizer,trainin_stats=training_loop( #On entraîne les modèles au fur et à mesure\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mModels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mOptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fats.py\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, optimizer, data, score, loss, epochs, camem2, device, suppress_after)\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0;31m# On actualise les paramètres grace a l'optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;31m# Checks if the weights did update, if not, informs at which step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "model,optimizer,trainin_stats=training_loop( #On entraîne les modèles au fur et à mesure\n",
    "    Models[0],\n",
    "    Optimizers[0],\n",
    "    data,\n",
    "    score,\n",
    "    Loss,\n",
    "    epochs,\n",
    "    camem2,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b482aa4-a304-4bca-bbb8-2ab219884ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Models[3]\n",
    "optimizer=Optimizers[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c5e7b5-b832-49c6-86c0-9916b1f9d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = []\n",
    "#score_stat=[]\n",
    "# Boucle d'entrainement\n",
    "model.train()\n",
    "model.to(device)\n",
    "model.zero_grad()\n",
    "print(\"Entraînement du modèle :\",str(model).split('(')[0])\n",
    "len_loss=len(Loss)\n",
    "start=time.time()\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "\n",
    "    # On initialise la loss pour cette epoque\n",
    "    total_train_loss = 0\n",
    "    if len_loss>1:\n",
    "        total_train_loss_2 = 0\n",
    "        if len_loss>2:\n",
    "            total_train_loss_3 = 0\n",
    "    f1_score=0\n",
    "    prec_score=0\n",
    "\n",
    "    # On met le modele en mode 'training'\n",
    "    # Dans ce mode certaines couches du modele agissent differement\n",
    "\n",
    "    # Pour chaque batch\n",
    "    for step, batch in enumerate(tqdm(data)):\n",
    "\n",
    "        # On recupere les donnees du batch\n",
    "        input_id = batch[0]#.to(device)\n",
    "        mask = batch[1]#.to(device)\n",
    "        #clss = batch[2].float().to(device)\n",
    "        #mask_cls=batch[3]#.to(device)\n",
    "        output=batch[4].float().to(device)\n",
    "\n",
    "        param1=list(model.parameters())[0].clone()\n",
    "\n",
    "        # On met le gradient a 0\n",
    "        optimizer.zero_grad()#summa_parallel.zero_grad()        \n",
    "\n",
    "        # On passe la donnee au model et on recupere la loss et le logits (sortie avant fonction d'activation)\n",
    "        topvec=camem2(input_id,mask)\n",
    "        topvec=topvec.last_hidden_state.to(device)\n",
    "        #topvec=topvec.mul(mask_cls.unsqueeze(2)).to(device)\n",
    "\n",
    "        sortie=model(topvec)\n",
    "\n",
    "        #On calcule et garde le score pour information, mais le détache pour éviter de faire exploser la mémoire\n",
    "        f1_score+=score(sortie,output).detach().item()\n",
    "        prec_score+=score.precision(sortie,output).detach().item()\n",
    "\n",
    "        #output2=make_output_topk(output,k=1).long().to(device)\n",
    "        loss_train=Loss[0](sortie,output)#.detach().item() # on commente detach sur la loss par rapport à laquelle on veut optimiser\n",
    "        if len_loss>1:\n",
    "            loss_train_2=Loss[1](sortie,output).detach().item()\n",
    "            if len_loss>2:\n",
    "                loss_train_3=Loss[2](sortie,output).detach().item()\n",
    "\n",
    "        # Backpropagtion\n",
    "        loss_train.backward()\n",
    "        # On actualise les paramètres grace a l'optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Checks if the weights did update, if not, informs at which step\n",
    "        param2=list(model.parameters())[0].clone()\n",
    "        check=bool(1-torch.equal(param1.data,param2.data))\n",
    "        if check==False:\n",
    "            print(\"The weights did not update at batch\",step,\"epoch\",epoch)        \n",
    "\n",
    "        # Keep all the predictions\n",
    "        #pred.append(sortie.detach())\n",
    "\n",
    "        # .item() donne la valeur numerique de la loss\n",
    "        total_train_loss += loss_train.detach().item() \n",
    "        if len_loss>1:\n",
    "            total_train_loss_2 += loss_train_2#.detach().item() \n",
    "            if len_loss>2:\n",
    "                total_train_loss_3 += loss_train_3#.detach().item() \n",
    "\n",
    "    # On calcule les statistiques et les pertes moyennes sur toute l'epoque\n",
    "    f1_stat=f1_score/len(data)\n",
    "    prec_stat=prec_score/len(data)\n",
    "    avg_train_loss = total_train_loss / len(data)\n",
    "    if len_loss>1:\n",
    "        avg_train_loss_2 = total_train_loss_2 / len(data)   \n",
    "        if len_loss>2:\n",
    "            avg_train_loss_3 = total_train_loss_3 / len(data)   \n",
    "\n",
    "            print(\"\\nAverage training loss MSE: {0:.4f}\".format(avg_train_loss),\n",
    "                  \"\\nAverage training loss L1: {0:.4f}\".format(avg_train_loss_2),\n",
    "                  \"\\nAverage training loss sum: {0:.4f}\".format(avg_train_loss_3),\n",
    "                  \"\\nAverage f1 score: {0:.4f}\".format(f1_stat),\n",
    "                  \"\\nAverage precision score: {0:.4f}\".format(prec_stat))  \n",
    "\n",
    "            # Enregistrement des stats de l'epoque\n",
    "            training_stats.append(\n",
    "                {'epoch': epoch + 1,\n",
    "                'Training Loss MSE': avg_train_loss,\n",
    "                'Training Loss L1': avg_train_loss_2,\n",
    "                'Training Loss sum': avg_train_loss_3,\n",
    "                'Training f1 score': f1_stat,\n",
    "                'Training precision score':prec_stat})\n",
    "        else:\n",
    "\n",
    "            print(\"\\nAverage training loss MSE: {0:.4f}\".format(avg_train_loss),\n",
    "                  \"\\nAverage training loss L1: {0:.4f}\".format(avg_train_loss_2),\n",
    "                  \"\\nAverage f1 score: {0:.4f}\".format(f1_stat),\n",
    "                  \"\\nAverage precision score: {0:.4f}\".format(prec_stat))  \n",
    "\n",
    "            # Enregistrement des stats de l'epoque\n",
    "            training_stats.append(\n",
    "                {'epoch': epoch + 1,\n",
    "                'Training Loss MSE': avg_train_loss,\n",
    "                'Training Loss L1': avg_train_loss_2,\n",
    "                'Training f1 score': f1_stat,\n",
    "                'Training precision score':prec_stat})\n",
    "    else:\n",
    "\n",
    "            print(\"\\nAverage training loss MSE: {0:.4f}\".format(avg_train_loss),\n",
    "                  \"\\nAverage f1 score: {0:.4f}\".format(f1_stat),\n",
    "                  \"\\nAverage precision score: {0:.4f}\".format(prec_stat))  \n",
    "\n",
    "            # Enregistrement des stats de l'epoque\n",
    "            training_stats.append(\n",
    "                {'epoch': epoch + 1,\n",
    "                'Training Loss MSE': avg_train_loss,\n",
    "                'Training f1 score': f1_stat,\n",
    "                'Training precision score':prec_stat})\n",
    "\n",
    "end=time.time()\n",
    "print(\"L'entraînement a duré :\",round((end-start)/60,2),\"minutes.\")\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f54eaac-2afb-46a3-b1b6-401827635057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Multi_Linear_Classifier(\n",
       "   (linear1): Linear(in_features=768, out_features=384, bias=True)\n",
       "   (linear2): Linear(in_features=384, out_features=128, bias=True)\n",
       "   (linear3): Linear(in_features=128, out_features=1, bias=True)\n",
       "   (Lrelu): LeakyReLU(negative_slope=0.01)\n",
       "   (softmax): Softmax(dim=-1)\n",
       " ),\n",
       " Simple_Classifier(\n",
       "   (linear1): Linear(in_features=768, out_features=1, bias=True)\n",
       "   (relu): LeakyReLU(negative_slope=0.01)\n",
       " ),\n",
       " SMHA_Linear_classifier(\n",
       "   (MHA): MultiheadAttention(\n",
       "     (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "   )\n",
       "   (LReLu): LeakyReLU(negative_slope=0.01)\n",
       "   (sigmoid): Sigmoid()\n",
       "   (LN): LayerNorm((512, 768), eps=1e-05, elementwise_affine=True)\n",
       "   (linear1): Linear(in_features=768, out_features=384, bias=True)\n",
       "   (linear2): Linear(in_features=384, out_features=128, bias=True)\n",
       " ),\n",
       " Net(\n",
       "   (conv1): Conv1d(512, 512, kernel_size=(256,), stride=(2,))\n",
       "   (pool): MaxPool1d(kernel_size=64, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   (conv2): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "   (fc1): Linear(in_features=48, out_features=24, bias=True)\n",
       "   (fc2): Linear(in_features=24, out_features=6, bias=True)\n",
       "   (fc3): Linear(in_features=6, out_features=1, bias=True)\n",
       "   (LReLu): LeakyReLU(negative_slope=0.01)\n",
       "   (softmax): Softmax(dim=-1)\n",
       " )]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647bdf88-ad3e-4576-95d8-1ba5442b896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim==3.8.3\n",
    "!pip install unidecode\n",
    "!pip install torch\n",
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "!pip install bs4\n",
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28ec9fa8-82da-4d0d-bfb9-46a73e38dcb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618cfa5600d444bea2e106af62459533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60ac965db88438683633e8716614152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import fats\n",
    "Models=pickle.load(open('Models_v2.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "847e37d8-0fd1-4f7e-ae5d-5ca0d902e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in Models:\n",
    "    name=str(m).split('(')[0]\n",
    "    torch.save(m.state_dict(),name+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f38d4355-41ce-4b17-ab28-4f03bb1ed54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer=pickle.load(open('Optimizer_v2.pickle','rb'))\n",
    "for o in Optimizer:\n",
    "    name=str(o).split('(')[0][:-1]\n",
    "    torch.save(o.state_dict(),name+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b2d2dd9-f9c5-4812-afcc-d792535fadb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-953555c77186>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'('\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "str(Optimizer[1]).split('(')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23231260-ac29-4ef6-8d02-c72e133f43f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AdamW (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     lr: 0.009\n",
       "     weight_decay: 0.01\n",
       " )]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d41b5-d5fb-4aea-9504-ed55c9624807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
