{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement du réseau de neurones via Transfer Learning de CamemBERT\n",
    "### Avant de commencer, quelques bonnes pratiques lorsqu'on commence à modéliser un réseau via torch\n",
    "\n",
    "1. Vérifier que l'on indique bien le suivi des inputs dans nos modèles\n",
    "2. Vérifier que le gradient vient bien modifier les poids\n",
    "3. Vérifier que la fonction de perte se re-propage correctement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install torch\n",
    "#!pip install sklearn\n",
    "import torch.nn as nn\n",
    "import sklearn\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_train=pickle.load(open('dico_train_1.pickle','rb'))\n",
    "\n",
    "train_input_ids=dico_train['input']\n",
    "train_mask=dico_train['mask']\n",
    "clss=dico_train['clss']\n",
    "train_mask_cls=dico_train['mask_cls']\n",
    "train_output=dico_train['output']\n",
    "#clss_index_train=[len(i) for i in dico_train['clss']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouais=torch.as_tensor([(train_output[i]!=torch.tensor(0)).nonzero().size()[0] for i in range(len(train_output))])\n",
    "v=((train_mask_cls.sum(dim=1)>ouais)==True).nonzero()\n",
    "\n",
    "def correct_mask_cls(input_ids):\n",
    "    vec=(torch.as_tensor(input_ids)==torch.tensor(5)).nonzero()\n",
    "    mask=torch.zeros(torch.as_tensor(input_ids).size())\n",
    "    mask[vec]=1\n",
    "    return mask\n",
    "\n",
    "cpu_max=30\n",
    "train_mask_cls_2=Parallel(cpu_max)(delayed(correct_mask_cls)(train_input_ids[i]) for i in range(len(train_input_ids)))\n",
    "train_mask_cls_2=torch.stack(train_mask_cls_2)\n",
    "\n",
    "v=((train_mask_cls_2.sum(dim=1)>ouais)==True).nonzero()\n",
    "\n",
    "import numpy as np\n",
    "np.sum([int(train_mask_cls_2[i].sum())==(train_output[i]!=torch.tensor(0)).nonzero().size(0) for i in range(len(train_output))])/len(train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=torch.zeros(train_mask_cls_2.shape,dtype=torch.float64)\n",
    "\n",
    "x=(train_output!=torch.tensor(0)).nonzero()\n",
    "dim_1=torch.unique(torch.stack([x[i][0] for i in range(len(x))]))\n",
    "\n",
    "x_2=torch.index_select(x,1,torch.tensor(1)).reshape(-1)\n",
    "x_1=(x_2==0).nonzero()\n",
    "\n",
    "dim_2=[]\n",
    "from tqdm import tqdm\n",
    "for k in tqdm(range(len(x_1))):\n",
    "    if k<(len(x_1)-1):\n",
    "        dim_2.append(x_2[x_1[k]:x_1[k+1]])\n",
    "    else:\n",
    "        dim_2.append(x_2[x_1[k]:])\n",
    "    \n",
    "for k in tqdm(range(len(dim_1))):\n",
    "    out[k,(train_mask_cls_2[k]!=torch.tensor(0)).nonzero().squeeze(1)]=train_output[dim_1[k],dim_2[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_input_ids),\n",
    "    torch.tensor(train_mask),\n",
    "    clss,\n",
    "    train_mask_cls_2,\n",
    "    out)\n",
    "\n",
    "pickle.dump(train_dataset,open('train_dataset.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=pickle.load(open('train_dataset_1.pickle','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle :\n",
    "On crée nos différents modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Simple_Classifier, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, 1)\n",
    "        self.relu=nn.LeakyReLU(negative_slope= 0.01)\n",
    "\n",
    "    def forward(self, x, mask_cls=None):\n",
    "        x.requires_grad_(True)\n",
    "        h = self.linear1(x).squeeze(-1)\n",
    "        if mask_cls!=None:\n",
    "            h=torch.mul(h,mask_cls)\n",
    "        sent_scores = self.relu(h) #* mask_cls.float()\n",
    "        return sent_scores.squeeze(-1)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Multi_Linear_Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Multi_Linear_Classifier, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, int(hidden_size/2))\n",
    "        self.linear2 = nn.Linear(int(hidden_size/2),int(hidden_size/6))\n",
    "        self.linear3 = nn.Linear(int(hidden_size/6),1)\n",
    "        self.Lrelu=nn.LeakyReLU(negative_slope= 0.01)\n",
    "        self.softmax=nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "    def forward(self, x,mask_cls=None):#, mask_cls):\n",
    "        x.requires_grad_(True)\n",
    "        h = self.linear1(x).squeeze(-1)\n",
    "        if mask_cls!=None:\n",
    "            h=torch.mul(h,mask_cls)\n",
    "        h = self.softmax(h)#self.Lrelu(h) #* mask_cls.float()\n",
    "        h = self.linear2(h)\n",
    "        h = self.softmax(h)#self.Lrelu(h)\n",
    "        h = self.linear3(h)\n",
    "        #h = self.softmax(h)#self.Lrelu(h)\n",
    "        return h.squeeze(-1)\n",
    "    \n",
    "\n",
    "class SMHA_classifier(nn.Module):\n",
    "    def __init__(self, size,nhead):\n",
    "        super(SMHA_classifier, self).__init__()\n",
    "        self.MHA = nn.MultiheadAttention(size[1], nhead)\n",
    "        self.LReLu=nn.LeakyReLU(negative_slope= 0.01)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.LN=nn.LayerNorm(size)\n",
    "\n",
    "    def forward(self, x, mask_cls=None):\n",
    "        x.requires_grad_(True)\n",
    "        h,weights = self.MHA(x,x,x)\n",
    "        if mask_cls!=None:\n",
    "            h=torch.mul(h,mask_cls)\n",
    "        normalized_h=self.LN(h)\n",
    "        sent_scores = self.LReLu(normalized_h) #* mask_cls.float()\n",
    "        return sent_scores.mean(dim=2)\n",
    "    \n",
    "class SMHA_Linear_classifier(nn.Module):\n",
    "    def __init__(self, size,nhead,hidden_size):\n",
    "        super(SMHA_Linear_classifier, self).__init__()\n",
    "        self.MHA = nn.MultiheadAttention(size[1], nhead)\n",
    "        self.LReLu=nn.LeakyReLU(negative_slope= 0.01)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.LN=nn.LayerNorm(size)\n",
    "        self.linear1 = nn.Linear(hidden_size, int(hidden_size/2))\n",
    "        self.linear2 = nn.Linear(int(hidden_size/2),int(hidden_size/6))\n",
    "\n",
    "    def forward(self, x, mask_cls=None):\n",
    "        x.requires_grad_(True)\n",
    "        h,weights = self.MHA(x,x,x)\n",
    "        h=self.LN(h)\n",
    "        h=self.linear1(h)\n",
    "        h=self.LReLu(h)\n",
    "        h=self.linear2(h)\n",
    "        if mask_cls!=None:\n",
    "            h=torch.mul(h,mask_cls)\n",
    "        sent_scores = self.LReLu(h) #* mask_cls.float()\n",
    "        return sent_scores.mean(dim=2) \n",
    "    \n",
    "def select_sent(phrase,clss,K=3):\n",
    "    index_phrase=torch.topk(phrase,K)[1]\n",
    "    pred_phrase=torch.zeros(clss.shape)\n",
    "    index_1=[[clss[k].tolist().index(int(index_phrase[k][i])) \n",
    "                for i in range(len(index_phrase[k])) if int(index_phrase[k][i]) in clss[k].tolist()]\n",
    "                 for k in range(K)]\n",
    "    index_2=[[i] for i in range(K)]\n",
    "    pred_phrase[index_2,index_1]=torch.ones(index_phrase.shape)\n",
    "    return pred_phrase\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def confusion_output(sent,output,clss_index):\n",
    "    a=[confusion_matrix(output[i],sent[i]) for i in range(sent.shape[0])]\n",
    "    c=[a[i][0][0]+a[i][1][1] for i in range(sent.shape[0])]\n",
    "    score_total=[round((c[i]-(512-clss_index[i]))/(clss_index[i]),3) for i in range(sent.shape[0])]\n",
    "    score_1=[(a[i][1][1])/3 for i in range(sent.shape[0])]\n",
    "    return score_total,score_1\n",
    "\n",
    "\n",
    "class Summarizer(nn.Module):\n",
    "    def __init__(self, device,classif):#args, , load_pretrained_bert = False, bert_config = None):\n",
    "        super(Summarizer, self).__init__()\n",
    "        self.device = device\n",
    "        self.bert =CamembertModel(CamembertConfig())#.from_pretrained(\"camembert-base\")\n",
    "        #BertModel.from_pretrained('bert-base-uncased')\n",
    "        #Bert(args.temp_dir, load_pretrained_bert, bert_config)\n",
    "        self.simple_classif = Simple_Classifier(self.bert.config.hidden_size)\n",
    "        self.multi_classif = Multi_Linear_Classifier(self.bert.config.hidden_size)\n",
    "        self.select_sent=select_sent\n",
    "        self.softmax=nn.Softmax()\n",
    "        self.attclassif=SMHA_classifier(torch.Size([self.bert.config.max_position_embeddings-2,self.bert.config.hidden_size]),8)\n",
    "        self.loss=nn.functional.binary_cross_entropy\n",
    "        self.classif_type=classif\n",
    "        # self.score=confusion_output\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self,x,mask, mask_cls):#,clss,output,k=3):#,segs):#, sentence_range=None): #segs, \n",
    "        #x input_ids\n",
    "        #Segs = Segment pour phrases (0 ou 1), marche pas dans un RoBERTa\n",
    "        #clss index du début des phrases \n",
    "        #mask_cls vecteur pour passer de l'embedding au cls, en gros sélectionne le bon index des vecteurs de l'embedding qu'on va utiliser pour faire la classif\n",
    "        top_vec= self.bert(x, mask)#, segs)\n",
    "        # sents_vec=self.sent_vec(last,clss)\n",
    "\n",
    "        # sents_vec = top_vec[0][torch.arange(top_vec[0].size(0)).unsqueeze(1), clss]\n",
    "        # sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
    "        if self.classif_type=='simple_linear':\n",
    "            sent_scores = self.simple_classif(top_vec.last_hidden_state)#, mask_cls\n",
    "        elif self.classif_type=='multi_linear':\n",
    "            sent_scores = self.multi_classif(top_vec.last_hidden_state)\n",
    "        elif self.classif_type=='attention':\n",
    "            sent_scores = self.attclassif(top_vec.last_hidden_state)\n",
    "            sent_scores=sent_scores.mean(dim=2)\n",
    "        else:\n",
    "            raise ValueError(\"Attention, veuillez bien spécifier un type de classifieur.\\nSeules les valeurs 'simple_linear', 'multi_linear' ou 'attention' sont acceptées.\")\n",
    "        #sent_scores_masked = torch.mul(sent_scores,mask_cls)\n",
    "        #sent_scores_masked = self.select_sent(sent_scores_masked,clss,K=k)\n",
    "        #sent_scores_masked=\n",
    "        \n",
    "        # score=self.score(sent_scores,output)\n",
    "        return sent_scores#_masked#,sent_scores,top_vec.last_hidden_state#,score#, mask_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer_2(nn.Module):\n",
    "    def __init__(self, device,classif,mul=False):#args, , load_pretrained_bert = False, bert_config = None):\n",
    "        super(Summarizer_2, self).__init__()\n",
    "        self.device = device\n",
    "#        self.bert =CamembertModel.from_pretrained(\"camembert-base\")\n",
    "        #BertModel.from_pretrained('bert-base-uncased')\n",
    "        #Bert(args.temp_dir, load_pretrained_bert, bert_config)\n",
    "        self.simple_classif = Simple_Classifier(768)\n",
    "        self.multi_classif = Multi_Linear_Classifier(768)\n",
    "        self.select_sent=select_sent\n",
    "        self.softmax=nn.Softmax()\n",
    "        self.attclassif=SMHA_classifier(torch.Size([512,768]),8)\n",
    "        self.attlinear=SMHA_Linear_classifier(torch.Size([512,768]),8,768)\n",
    "        self.loss=nn.functional.binary_cross_entropy\n",
    "        self.classif_type=classif\n",
    "        # self.score=confusion_output\n",
    "        self.to(device)\n",
    "        self.mul=mul\n",
    "\n",
    "    def forward(self,top_vec,maks_cls=None):#,clss,output,k=3):#,segs):#, sentence_range=None): #segs, \n",
    "        #x input_ids\n",
    "        #Segs = Segment pour phrases (0 ou 1), marche pas dans un RoBERTa\n",
    "        #clss index du début des phrases \n",
    "        #mask_cls vecteur pour passer de l'embedding au cls, en gros sélectionne le bon index des vecteurs de l'embedding qu'on va utiliser pour faire la classif\n",
    "        #top_vec= self.bert(x, mask)#, segs)\n",
    "        # sents_vec=self.sent_vec(last,clss)\n",
    "\n",
    "        # sents_vec = top_vec[0][torch.arange(top_vec[0].size(0)).unsqueeze(1), clss]\n",
    "        # sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
    "        topvec.requires_grad_(True)\n",
    "        if self.classif_type=='simple_linear':\n",
    "            if self.mul==True:\n",
    "                sent_scores = self.simple_classif(top_vec,mask_cls).squeeze(-1)#, mask_cls\n",
    "            else:\n",
    "                sent_scores = self.simple_classif(top_vec).squeeze(-1)#, mask_cls\n",
    "        \n",
    "        elif self.classif_type=='multi_linear':\n",
    "            if self.mul==True:\n",
    "                sent_scores = self.multi_classif(top_vec,mask_cls).squeeze(-1)\n",
    "            else:\n",
    "                sent_scores = self.multi_classif(top_vec).squeeze(-1)\n",
    "\n",
    "        elif self.classif_type=='attention':\n",
    "            if self.mul==True:\n",
    "                sent_scores = self.attclassif(top_vec,mask_cls)\n",
    "                sent_scores=sent_scores.mean(dim=2)\n",
    "            else:\n",
    "                sent_scores = self.attclassif(top_vec)\n",
    "                sent_scores=sent_scores.mean(dim=2)\n",
    "        elif self.classif_type=='attention_linear':\n",
    "            if self.mul==True:\n",
    "                sent_scores = self.attlinear(top_vec,mask_cls)\n",
    "                sent_scores=sent_scores.mean(dim=2)\n",
    "            else:\n",
    "                sent_scores = self.attlinear(top_vec)\n",
    "                sent_scores=sent_scores.mean(dim=2)\n",
    "        else:\n",
    "            raise ValueError(\"Attention, veuillez bien spécifier un type de classifieur.\\nSeules les valeurs 'simple_linear', 'multi_linear' ou 'attention' sont acceptées.\")\n",
    "        #sent_scores_masked = torch.mul(sent_scores,mask_cls)\n",
    "        #sent_scores_masked = self.select_sent(sent_scores_masked,clss,K=k)\n",
    "        #sent_scores_masked=\n",
    "        \n",
    "        # score=self.score(sent_scores,output)\n",
    "        return sent_scores#_masked#,sent_scores,top_vec.last_hidden_state#,score#, mask_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On installe et charge le modèle Camembert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertModel,CamembertConfig,AdamW\n",
    "camem1=CamembertModel(CamembertConfig())\n",
    "camem2=CamembertModel.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score \n",
    "On crée nos fonctions de score : précision, rappel et score F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1_score:\n",
    "    \"\"\"\n",
    "    Class for f1 calculation in Pytorch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):#, average: str = 'weighted'):\n",
    "        \"\"\"\n",
    "        Init.\n",
    "\n",
    "        Args:\n",
    "            average: averaging method\n",
    "        \"\"\"\n",
    "\n",
    "        #self.average = average\n",
    "        #if average not in [None, 'micro', 'macro', 'weighted']:\n",
    "         #   raise ValueError('Wrong value of average parameter')\n",
    "    @staticmethod\n",
    "    def true_positive_mean(x,y) -> torch.tensor:\n",
    "        '''\n",
    "        Caclul le nombre moyen de vrai positif de la prediction x par rapport aux labels y (binaires).\n",
    "        '''\n",
    "        tp=torch.mul(x,y).sum()\n",
    "        tpm=torch.div(tp,y.shape[0])\n",
    "        return tpm\n",
    "    @staticmethod\n",
    "    def false_positive_mean(x,y) -> torch.tensor:\n",
    "        '''\n",
    "        Caclul le nombre moyen de faux négatif de la prediction x par rapport aux labels y (binaires).\n",
    "        '''\n",
    "        device=y.device\n",
    "        fp=torch.sub(x,y)\n",
    "        fp=torch.max(fp,torch.tensor([0.]).to(device))\n",
    "        fp=fp.sum().float()\n",
    "        fpm=torch.div(fp,y.shape[0])\n",
    "        return fpm\n",
    "    @staticmethod\n",
    "    def false_negative_mean(x,y) -> torch.tensor:\n",
    "        '''\n",
    "        Caclul le nombre moyen de faux négatif de la prediction x par rapport aux labels y (binaires).\n",
    "        '''\n",
    "        fn=torch.sub(y,x)\n",
    "        device=y.device\n",
    "        fn=torch.max(fn,torch.tensor([0.]).to(device))\n",
    "        fn=fn.sum().float()\n",
    "        fnm=torch.div(fn,y.shape[0])\n",
    "        return fnm\n",
    "    #@staticmethod\n",
    "    def precision(self,x,y) -> torch.tensor:\n",
    "        device=y.device\n",
    "        tp=self.true_positive_mean(x,y)\n",
    "        fp=self.false_positive_mean(x,y)\n",
    "        if (tp+fp)!=0:\n",
    "            prec=torch.div(tp,(tp+fp))\n",
    "            return prec\n",
    "        else:\n",
    "            return torch.tensor(0.).to(device)\n",
    "\n",
    "    def recall(self,x,y) -> torch.tensor:\n",
    "        tp=self.true_positive_mean(x,y)\n",
    "        fn=self.false_negative_mean(x,y)\n",
    "        rec=torch.div(tp,(tp+fn))\n",
    "        return rec\n",
    "    def __call__(self,x,y) -> torch.tensor:\n",
    "        device=y.device\n",
    "        rec=self.recall(x,y)\n",
    "        prec=self.precision(x,y)\n",
    "        f1=torch.mul(rec,prec)\n",
    "        f1=torch.mul(2,f1)\n",
    "        f1=torch.div(f1,prec+rec)\n",
    "        if (prec+rec)!=0:\n",
    "            return f1#prec,rec,\n",
    "        else:\n",
    "            return torch.tensor(0.).to(device)#prec,rec,\n",
    "\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de perte\n",
    "On va maintenant définir notre fonction de perte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weighted_Loss:\n",
    "    '''\n",
    "    Fonction permettant de calculer la fonction de perte Mean Absolute Error mais pondérée par des poids.\n",
    "    '''\n",
    "    def __init__(self,weight,loss_type='L1',binary=True):\n",
    "        '''\n",
    "        On initialise notre fonction de perte :\n",
    "        @weight : les poids que vous voulez pour chaque classe (dim=nombre de classe)\n",
    "        '''\n",
    "        self.weights=weight\n",
    "        self.loss_type=loss_type\n",
    "        self.binary=binary\n",
    "        \n",
    "    def Weighted_L1(self,y_hat,y) -> torch.Tensor:\n",
    "        '''\n",
    "        On calcule la fonction :\n",
    "        @y_hat : les prédictions du modèle\n",
    "        @y : les vraies valeurs\n",
    "        \n",
    "        Attention, dim(y_hat)==dim(y)\n",
    "        '''\n",
    "        if y_hat.shape!=y.shape:\n",
    "            raise ValueError(\"Attention, les deux inputs n'ont pas la même dimension !\")\n",
    "        #On met les deux tensors sur le même service (ici GPU)\n",
    "        device_yhat=y_hat.device\n",
    "        device_y=y.device\n",
    "        if device_yhat!=device_y:\n",
    "            y.to(device_yhat)\n",
    "        \n",
    "        w=torch.repeat_interleave(torch.tensor(self.weights[0]),y.shape[1])\n",
    "        w=w.repeat(y.shape[0],1)\n",
    "                \n",
    "        if self.binary:\n",
    "            w[torch.arange(y.shape[0],dtype=torch.long).unsqueeze(1),torch.topk(y,3)[1]]=self.weights[1]\n",
    "        \n",
    "        else: #On surpondère les indices qui représentent les phrases, puisque c'est cela que le modèle doit prédire\n",
    "            x=(y!=torch.tensor(0)).nonzero()\n",
    "            x_2=torch.index_select(x,1,torch.tensor(1).to(device_yhat)).reshape(-1).to(device_yhat)\n",
    "            x_1=(x_2==0).nonzero().to(device_yhat)\n",
    "            sha=torch.arange(y.shape[0],dtype=torch.long).unsqueeze(1).to(device_yhat)\n",
    "            \n",
    "            for k in range(len(x_1)):\n",
    "                if k<(len(x_1)-1):\n",
    "                    w[sha[k],x_2[x_1[k]:x_1[k+1]]]=self.weights[1]\n",
    "                else:\n",
    "                    w[sha[k],x_2[x_1[k]:]]=self.weights[1]\n",
    "               \n",
    "        sum_weights=w.sum()\n",
    "        w=w.to(device)\n",
    "        sum_weights=sum_weights.to(device)\n",
    "        errors=torch.sub(y,y_hat)\n",
    "        errors=torch.abs(errors)\n",
    "        weighted_errors=torch.mul(w,errors)\n",
    "        sum_weighted_errors=weighted_errors.sum()\n",
    "        WMAE=torch.div(sum_weighted_errors,sum_weights)\n",
    "        #WMAE.requires_grad=True\n",
    "        return Variable(WMAE,requires_grad=True)#,sum_weighted_errors,sum_weights\n",
    "    \n",
    "    def Weighted_Sum(self,y_hat,y) -> torch.Tensor:\n",
    "        '''\n",
    "        Calcule la somme pondérée de la différence de la prédiction du modèle et du vecteur cible.\n",
    "        '''\n",
    "        if y_hat.shape!=y.shape:\n",
    "            raise ValueError(\"Attention, les deux inputs n'ont pas la même dimension !\")\n",
    "        \n",
    "        #On met les deux tensors sur le même service (ici GPU)\n",
    "        device_yhat=y_hat.device\n",
    "        device_y=y.device\n",
    "        if device_yhat!=device_y:\n",
    "            y.to(device_yhat)\n",
    "        \n",
    "        w=torch.repeat_interleave(torch.tensor(self.weights[0]),y.shape[1])\n",
    "        w=w.repeat(y.shape[0],1)\n",
    "        \n",
    "        if self.binary:\n",
    "            w[torch.arange(y.shape[0],dtype=torch.long).unsqueeze(1),torch.topk(y,3)[1]]=self.weights[1]\n",
    "        \n",
    "        else: #On surpondère les indices qui représentent les phrases, puisque c'est cela que le modèle doit prédire\n",
    "            x=(y!=torch.tensor(0)).nonzero()\n",
    "            x_2=torch.index_select(x,1,torch.tensor(1).to(device_yhat)).reshape(-1).to(device_yhat)\n",
    "            x_1=(x_2==0).nonzero().to(device_yhat)\n",
    "            sha=torch.arange(y.shape[0],dtype=torch.long).unsqueeze(1).to(device_yhat)\n",
    "            \n",
    "            for k in range(len(x_1)):\n",
    "                if k<(len(x_1)-1):\n",
    "                    w[sha[k],x_2[x_1[k]:x_1[k+1]]]=self.weights[1]\n",
    "                else:\n",
    "                    w[sha[k],x_2[x_1[k]:]]=self.weights[1]\n",
    "                    \n",
    "        w=w.to(device_yhat)\n",
    "        y_diff=torch.abs(torch.sub(y,y_hat))\n",
    "        y_diff_pond=torch.mul(y_diff,w)\n",
    "        sum_y_diff_pon=torch.div(torch.sum(y_diff_pond),y_hat.shape[0])\n",
    "        return Variable(sum_y_diff_pon,requires_grad=True)\n",
    "    \n",
    "    def __call__(self,y_hat,y) -> torch.Tensor:\n",
    "        if self.loss_type=='L1':\n",
    "            loss=self.Weighted_L1(y_hat,y)\n",
    "            return loss\n",
    "        elif self.loss_type=='sum':\n",
    "            loss=self.Weighted_Sum(y_hat,y)\n",
    "            return loss\n",
    "        else:\n",
    "            raise ValueError(\"Attention, veuillez bien spécifier un type de perte.\\nSeules les valeurs 'L1' ou 'sum' sont acceptées.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=(out!=torch.tensor(0)).nonzero()\n",
    "x_2=torch.index_select(x,1,torch.tensor(1)).reshape(-1)\n",
    "x_1=(x_2==0).nonzero()\n",
    "\n",
    "from tqdm import tqdm\n",
    "w=torch.repeat_interleave(torch.tensor(1),512)\n",
    "w=w.repeat(len(x_1),1)\n",
    "sha=torch.arange(len(x_1),dtype=torch.long).unsqueeze(1)\n",
    "for k in tqdm(range(len(x_1))):\n",
    "    if k<(len(x_1)-1):\n",
    "        w[sha[k],x_2[x_1[k]:x_1[k+1]]]=4\n",
    "    else:\n",
    "        w[sha[k],x_2[x_1[k]:]]=4\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand(100,14)\n",
    "y=torch.rand(100,14)\n",
    "loss=Weighted_Loss(weight=[torch.tensor(0.5),torch.tensor(1000.)],loss_type='sum',binary=False)\n",
    "print(loss(x,y))\n",
    "perte=torch.nn.L1Loss()\n",
    "print(perte(x,y))\n",
    "print(\"On voit que la première est plus élevée que la deuxième, c'est ce que l'on cherche dans notre cas !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16317161917686462"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.rand(100,14)\n",
    "y=torch.rand(100,14)\n",
    "loss=torch.nn.MSELoss()\n",
    "loss(x,y).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "y = Variable(torch.rand(2,10),requires_grad=True)#Variable(torch.rand(5, 3), requires_grad=True)\n",
    "t = torch.rand(2,10)#Variable(torch.LongTensor(5).random_(0, 2))\n",
    "m=Weighted_Loss(weight=[torch.tensor(0.5),torch.tensor(1000.)])\n",
    "#m = nn.L1Loss()#MultiMarginLoss()\n",
    "loss = m(y, t)\n",
    "loss.backward()\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand(100,14)\n",
    "print(x)\n",
    "x=x.to(device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On prépare l'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vérifie que du GPU est disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On charge nos données d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset=pickle.load(open('train.pickle','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée notre dataloader, indispensable pour créer une boucle d'entraînement sous torch, on fixe notre taille de batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summa_parallel.to('cpu'),input_id.to('cpu'),mask.to('cpu'),mask_cls.to('cpu'),output.to('cpu')\n",
    "del summa_parallel,input_id,mask,mask_cls,output,dataloader_2\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8513"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=int(1024/64)\n",
    "print(batch_size)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size)\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "output_=torch.stack([train_dataset[i][-1] for i in range(len(train_dataset))])\n",
    "mask_cls_=torch.stack([train_dataset[i][-2] for i in range(len(train_dataset))])\n",
    "output_2=torch.mul(torch.div(output_-torch.min(output_),torch.max(output_)-torch.min(output_)),mask_cls_)\n",
    "output_2\n",
    "\n",
    "K=2000\n",
    "train_2=TensorDataset(torch.stack([train_dataset[i][0] for i in range(K)]),\n",
    "                      torch.stack([train_dataset[i][1] for i in range(K)]),\n",
    "                      torch.stack([train_dataset[i][2] for i in range(K)]),\n",
    "                      torch.stack([train_dataset[i][3] for i in range(K)]),\n",
    "                      output_2[:K])\n",
    "\n",
    "batch_size=int(1024/8/8)\n",
    "print(batch_size)\n",
    "\n",
    "dataloader_2 = DataLoader(\n",
    "            train_2,\n",
    "            sampler = RandomSampler(train_2),\n",
    "            batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ne lancer que si on veut ré-initialiser la mémoire sur GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.718575477600098"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0) \n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "f/1024/1024/1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On place le modèle sur GPU, on défini notre algorithme d'apprentissage, notre score et notre fonction de perte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = []\n",
    "epochs=5\n",
    "\n",
    "summa=Summarizer_2(device=device,classif='multi_linear')\n",
    "summa_parallel=nn.DataParallel(summa) # On va distribuer sur plusieurs GPU, 2 ici pour avoir plus de mémoire\n",
    "\n",
    "#optimizer = AdamW(summa_parallel.parameters(),\n",
    "             #     lr = 2e-4, # Learning Rate\n",
    "              #    eps = 1e-3)# Epsilon\n",
    "optimizer = optim.SGD(summa_parallel.parameters(), lr=0.00001, momentum=0.9)\n",
    "score=F1_score()\n",
    "\n",
    "alpha=0.95\n",
    "#weights=torch.Tensor([(1/(1-alpha))*1/((512-3)/512),(1/alpha)*1/((3)/512)])\n",
    "#weights=torch.Tensor([(1/(1-alpha))*1/((1)/512),(1/alpha)*1/((512-1)/512)])\n",
    "weights=torch.Tensor([1,1])\n",
    "loss_3=Weighted_Loss(weight=weights,loss_type='sum',binary=True)\n",
    "loss_2=Weighted_Loss(weight=weights,loss_type='L1',binary=True)\n",
    "loss=nn.MSELoss()#nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=F1_score()\n",
    "\n",
    "alpha=0.5\n",
    "weights=torch.Tensor([(1/(1-alpha))*1/((512-20)/512),(1/alpha)*1/((20)/512)])\n",
    "#weights=torch.Tensor([(1/(1-alpha))*1/((1)/512),(1/alpha)*1/((512-1)/512)])\n",
    "#weights=torch.Tensor([1,1])\n",
    "loss_3=Weighted_Loss(weight=weights,loss_type='sum',binary=False)\n",
    "loss_2=Weighted_Loss(weight=weights,loss_type='L1',binary=False)\n",
    "loss=nn.MSELoss()#nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 16, 50]) \n",
      " torch.Size([20, 16, 24])\n",
      "torch.Size([20, 16, 12])\n",
      "torch.Size([20, 16, 5])\n",
      "torch.Size([20, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Conv1d(16, 16, kernel_size=3, stride=2)\n",
    "input = torch.randn(20, 16, 50)\n",
    "output = m(input)\n",
    "print(input.shape,'\\n',output.shape)\n",
    "m=nn.MaxPool1d(2,2)\n",
    "output=m(output)\n",
    "print(output.shape)\n",
    "m = nn.Conv1d(16, 16, kernel_size=3, stride=2)\n",
    "output=m(output)\n",
    "print(output.shape)\n",
    "m = nn.Conv1d(16, 16, kernel_size=4, stride=2)\n",
    "output=m(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,k1,k2,k3,s1,s2,s3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(512, 512, kernel_size=k1,stride=s1)\n",
    "        self.pool = nn.MaxPool1d(k2, s2)\n",
    "        self.conv2 = nn.Conv1d(512, 512, kernel_size=k3,stride=s3)\n",
    "        self.dim=int((768-k1)/s1)+1\n",
    "        self.dim=int((self.dim-(k2-1)-1)/s2+1)\n",
    "        self.dim=int((self.dim-k3)/s3)+1\n",
    "        self.fc1 = nn.Linear(self.dim, int(self.dim/2))\n",
    "        self.fc2 = nn.Linear(int(self.dim/2), int(self.dim/8))\n",
    "        self.fc3 = nn.Linear(int(self.dim/8), 1)\n",
    "        self.LReLu=nn.LeakyReLU(negative_slope= 0.01)\n",
    "        self.softmax=nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.requires_grad_(True)\n",
    "        x = self.pool(self.LReLu(self.conv1(x)))\n",
    "        x =self.LReLu(self.conv2(x))\n",
    "        #x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.LReLu(self.fc1(x))\n",
    "        x = self.LReLu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #x=self.softmax(x)\n",
    "        return x.flatten(1)\n",
    "\n",
    "\n",
    "#net = Net(2**8,2**6,2,2,2,2).to(device)\n",
    "#import torch.optim as optim\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.09)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlc=Multi_Linear_Classifier(camem2.config.hidden_size)\n",
    "mlc_optimizer=optim.SGD(mlc.parameters(), lr=0.001, momentum=0.09)\n",
    "\n",
    "slc=Simple_Classifier(camem2.config.hidden_size)\n",
    "slc_optimizer=optim.SGD(slc.parameters(), lr=0.001, momentum=0.09)\n",
    "\n",
    "att_c=SMHA_classifier(torch.Size([512,768]),8)\n",
    "att_c_optimizer=optim.SGD(att_c.parameters(), lr=0.001, momentum=0.09)\n",
    "\n",
    "att_lin_c=SMHA_Linear_classifier(torch.Size([512,768]),8,768)\n",
    "att_lin_c_optimizer=optim.SGD(att_lin_c.parameters(), lr=0.001, momentum=0.09)\n",
    "\n",
    "convnet=Net(2**8,2**6,2,2,2,2)\n",
    "convnet_optimizer=optim.SGD(convnet.parameters(), lr=0.001, momentum=0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=torch.rand(16,512,768).to(device)\n",
    "output=torch.rand(16,512).to(device)\n",
    "\n",
    "for m,o in zip([mlc,slc,att_c,att_lin_c,convnet],[mlc_optimizer,slc_optimizer,att_c_optimizer,att_lin_c_optimizer,convnet_optimizer]):\n",
    "    print(\"\\n Modèle :\",str(m).split('(')[0],'\\n')\n",
    "    \n",
    "    model=m.to(device)\n",
    "    param1=list(model.parameters())[0].clone()\n",
    "\n",
    "    sortie=model(input)\n",
    "\n",
    "    for l in [loss,loss_2,loss_3]:\n",
    "        ouais=l(sortie,output)\n",
    "        o.zero_grad()\n",
    "        ouais.backward(retain_graph=True)\n",
    "        o.step()\n",
    "        \n",
    "        param2=list(model.parameters())[0].clone()\n",
    "        print(\"For loss,\",l,\"Did the grad updated the weights ?\",bool(1-torch.equal(param1.data,param2.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boucle d'entraînement\n",
    "On passe maintenant à l'entraînement !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output_topk(x,k=3):\n",
    "    out=torch.zeros(x.shape)\n",
    "    ind=torch.topk(x,k=k)[1]\n",
    "    ind1=torch.arange(x.shape[0]).unsqueeze(1)\n",
    "    out[ind1,ind]=1\n",
    "    return out\n",
    "\n",
    "output2=make_output_topk(output)\n",
    "pred2=make_output_topk(pred[0])\n",
    "\n",
    "LCE=torch.nn.CrossEntropyLoss()\n",
    "LCE(pred2,output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]<ipython-input-7-34ca28d4352e>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w=torch.repeat_interleave(torch.tensor(self.weights[0]),y.shape[1])\n",
      "<ipython-input-7-34ca28d4352e>:37: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  x=(y!=torch.tensor(0)).nonzero()\n",
      "<ipython-input-7-34ca28d4352e>:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w=torch.repeat_interleave(torch.tensor(self.weights[0]),y.shape[1])\n",
      "  7%|▋         | 9/125 [00:19<04:31,  2.34s/it]"
     ]
    }
   ],
   "source": [
    "#Pour enregistrer les informations de l'entraînement\n",
    "training_stats = []\n",
    "score_stat=[]\n",
    "#loss.requires_grad=True\n",
    "pred_output={}\n",
    "# Boucle d'entrainement\n",
    "data=dataloader_2\n",
    "camem2.to('cpu')\n",
    "epochs=5\n",
    "model=convnet\n",
    "optimizer=convnet_optimizer\n",
    "for epoch in range(0, epochs):\n",
    "     \n",
    "    #print(\"\")\n",
    "    #print(f'########## Epoch {epoch+1} / {epochs} ##########')\n",
    "    #print('Training...')\n",
    " \n",
    " \n",
    "    # On initialise la loss pour cette epoque\n",
    "    total_train_loss = 0\n",
    "    total_train_loss_2 = 0\n",
    "    total_train_loss_3 = 0\n",
    "    score_e=0\n",
    "    pred=[]\n",
    "    # On met le modele en mode 'training'\n",
    "    # Dans ce mode certaines couches du modele agissent differement\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    #net.train()\n",
    "    # Pour chaque batch\n",
    "    for step, batch in enumerate(tqdm(data)):\n",
    " \n",
    "        # On fait un print chaque 40 batchs\n",
    "      # if step % 150 == 0 and not step == 0:\n",
    "       #     print(f'  Batch {step}  of {len(data)}.')\n",
    "         \n",
    "        # On recupere les donnees du batch\n",
    "        input_id = batch[0]#.to(device)\n",
    "        mask = batch[1]#.to(device)\n",
    "        clss = batch[2].float().to(device)\n",
    "        mask_cls=batch[3]#.to(device)\n",
    "        output=batch[4].float().to(device)\n",
    " \n",
    "        # On met le gradient a 0\n",
    "        optimizer.zero_grad()#summa_parallel.zero_grad()        \n",
    " \n",
    "        # On passe la donnee au model et on recupere la loss et le logits (sortie avant fonction d'activation)\n",
    "        topvec=camem2(input_id,mask)\n",
    "        topvec=topvec.last_hidden_state.to(device)\n",
    "        #topvec=topvec.mul(mask_cls.unsqueeze(2)).to(device)\n",
    "        sortie=model(topvec)\n",
    "        #(x=input_id,mask=mask,mask_cls=mask_cls)#,clss=clss,output=output)\n",
    "        \n",
    "        #On indique qu'on souhaite tracer le gradient de la sortie dans la fonction de perte\n",
    "        #sortie=Variable(sortie,requires_grad=True)\n",
    "        pred.append(sortie)\n",
    "        #On calcule et garde le score pour information\n",
    "        score_e+=score(sortie,output)\n",
    "        #print(score_stat[step])\n",
    "        #output2=make_output_topk(output,k=1).long().to(device)\n",
    "        loss_train=loss(sortie,output)\n",
    "        loss_train_2=loss_2(sortie,output)\n",
    "        loss_train_3=loss_3(sortie,output)\n",
    "\n",
    "        #print(loss_train)\n",
    "        # On incremente la loss totale\n",
    "        # .item() donne la valeur numerique de la loss\n",
    "        total_train_loss += loss_train.item()\n",
    "        total_train_loss_2 += loss_train_2.item()\n",
    "        total_train_loss_3 += loss_train_3.item()\n",
    "\n",
    "        # Backpropagtion\n",
    "        loss_train_3.backward()\n",
    " \n",
    "        # On actualise les paramètres grace a l'optimizer\n",
    "        optimizer.step()\n",
    "    score_stat=score_e/len(data)\n",
    "    # On calcule la  loss moyenne sur toute l'epoque\n",
    "    avg_train_loss = total_train_loss / len(data)   \n",
    "    avg_train_loss_2 = total_train_loss_2 / len(data)   \n",
    "    avg_train_loss_3 = total_train_loss_3 / len(data)   \n",
    "    pred_output[epoch]=pred\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss MLML: {0:.4f}\".format(avg_train_loss),\n",
    "         \"  Average training loss L1: {0:.4f}\".format(avg_train_loss_2),\n",
    "         \"  Average training loss sum: {0:.4f}\".format(avg_train_loss_3),\"  Average f1 score: {0:.4f}\".format(score_stat))  \n",
    "     \n",
    "    # Enregistrement des stats de l'epoque\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch + 1,\n",
    "            'Training Loss MSE': avg_train_loss,\n",
    "            'Training Loss L1': avg_train_loss_2,\n",
    "            'Training Loss sum': avg_train_loss_3,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Model saved!\")\n",
    "torch.save(model.state_dict(), \"model_essai.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "summa=Multi_Linear_Classifier(768)#Summarizer_2(device=device,classif='multi_linear')\n",
    "#summa_parallel=nn.DataParallel(summa) # On va distribuer sur plusieurs GPU, 2 ici pour avoir plus de mémoire\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(summa.parameters(), lr=0.001, momentum=0.09)\n",
    "score=F1_score()\n",
    "\n",
    "alpha=0.95\n",
    "#weights=torch.Tensor([(1/(1-alpha))*1/((512-3)/512),(1/alpha)*1/((3)/512)])\n",
    "#weights=torch.Tensor([(1/(1-alpha))*1/((1)/512),(1/alpha)*1/((512-1)/512)])\n",
    "weights=torch.Tensor([1,1])\n",
    "loss_3=Weighted_Loss(weight=weights,loss_type='sum',binary=True)\n",
    "loss_2=Weighted_Loss(weight=weights,loss_type='L1',binary=True)\n",
    "loss=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(768, 1)\n",
    "        self.LReLu=nn.LeakyReLU(negative_slope= 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.LReLu(self.fc1(x.requires_grad_(True)))\n",
    "        return x.flatten(1)\n",
    "\n",
    "\n",
    "net = Net()#.to(device)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortie.squeeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=summa.to(device)\n",
    "param1=list(model.parameters())[0].clone()\n",
    "\n",
    "#input=torch.rand(16,512,768).to(device)\n",
    "#output=torch.rand(16,512).to(device)\n",
    "\n",
    "sortie=model(topvec).squeeze(2)#topvec.requires_grad_(True))\n",
    "#sortie=sortie.requires_grad_(True)\n",
    "ouais=loss(sortie,output)\n",
    "# Visiblement les loss faîtes mains ne sont pas optimisables pour le moment ?\n",
    "# Ah bah si pour le ConvNet ???\n",
    "optimizer.zero_grad()\n",
    "ouais.backward(retain_graph=True)\n",
    "optimizer.step()\n",
    "param2=list(model.parameters())[0].clone()\n",
    "torch.equal(param1.data,param2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimizer)\n",
    "optimizer.zero_grad()\n",
    "print(optimizer)\n",
    "ouais=loss_3(pred[0],output2)\n",
    "ouais.backward()\n",
    "optimizer.step()\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for par in summa_parallel.parameters():\n",
    "    print(par.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False])\n",
      "Parameter containing:\n",
      "tensor([1.0010], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class TEMP(nn.Module):\n",
    "\n",
    "    # Whole architecture\n",
    "    def __init__(self):\n",
    "        super(TEMP, self).__init__()\n",
    "        self.input = nn.Parameter(torch.ones(1,requires_grad = True)) # <----wrap it like this\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        wt = self.input\n",
    "        y = wt*x \n",
    "        return y\n",
    "\n",
    "model = TEMP()\n",
    "param1=list(model.parameters())[0].clone()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "x = torch.randn(100)\n",
    "y = 5*x\n",
    "loss_ex = torch.sum((y - model(x)).pow(2))\n",
    "optimizer.zero_grad()\n",
    "loss_ex.backward()\n",
    "\n",
    "optimizer.step()\n",
    "param2=list(model.parameters())[0].clone()\n",
    "print(param1==param2)\n",
    "print(model.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512, 768])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=torch.rand(16,512,768)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(768, 1)\n",
    "        self.LReLu=nn.LeakyReLU(negative_slope= 0.01)\n",
    "        #self.weights_model = nn.Parameter(torch.ones(1,requires_grad = True)) # <----wrap it like this\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.LReLu(self.fc1(x.requires_grad_(True)))\n",
    "        return x.flatten(1)\n",
    "\n",
    "\n",
    "net = Net()#.to(device)\n",
    "import torch.optim as optim\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "input=torch.rand(16,512,768)\n",
    "output=torch.rand(16,512)\n",
    "\n",
    "\n",
    "param1=list(net.parameters())[0].clone()\n",
    "sortie=net(input)\n",
    "#sortie=sortie.requires_grad_(True)\n",
    "ouais=loss(sortie,output)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "ouais.backward(retain_graph=True)\n",
    "optimizer.step()\n",
    "\n",
    "param2=list(net.parameters())[0].clone()\n",
    "torch.equal(param1.data,param2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertModel,BertModel,RobertaModel,AdamW,BertConfig\n",
    "bert=BertModel(BertConfig())\n",
    "top_vec=bert(input_id,attention_mask=mask)\n",
    "top_vec=top_vec[0]\n",
    "top_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,  36., 100.,  ...,   0.,   0.,   0.],\n",
       "        [  0.,  50.,  83.,  ...,   0.,   0.,   0.],\n",
       "        [  0.,  40.,  68.,  ...,   0.,   0.,   0.],\n",
       "        ...,\n",
       "        [  0.,  22.,  37.,  ...,   0.,   0.,   0.],\n",
       "        [  0.,  18.,  35.,  ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clss=clss.to('cpu')\n",
    "clss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5c66d83c1f1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "clss[torch.arange(clss.size(0)).unsqueeze(1),:(clss.topk(k=1)[1]+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3635, -0.3541, -0.3638, -0.3740, -0.3593, -0.3593, -0.3632, -0.3635,\n",
       "        -0.3593, -0.3698, -0.3641, -0.3642, -0.3590, -0.3635, -0.3595, -0.3830,\n",
       "        -0.3593, -0.3593, -0.3593, -0.3593, -0.3632, -0.3629, -0.3640, -0.3646,\n",
       "        -0.3593, -0.3645, -0.3597, -0.3690, -0.3593, -0.3641, -0.3664, -0.3592,\n",
       "        -0.3508, -0.3585, -0.3593, -0.3593, -0.3591, -0.3593, -0.3593, -0.3674,\n",
       "        -0.3593, -0.3593, -0.3654, -0.3560, -0.3601, -0.3593, -0.3593, -0.3626,\n",
       "        -0.3637, -0.3603, -0.3654, -0.3593, -0.3593, -0.3650, -0.3597, -0.3593,\n",
       "        -0.3593, -0.3575, -0.3642, -0.3584, -0.3667, -0.3571, -0.3594, -0.3592,\n",
       "        -0.3602, -0.3593, -0.3639, -0.3639, -0.3638, -0.3593, -0.3603, -0.3644,\n",
       "        -0.3618, -0.3644, -0.3622, -0.3594, -0.3576, -0.3603, -0.3593, -0.3653,\n",
       "        -0.3639, -0.3593, -0.3593, -0.3644, -0.3595, -0.3631, -0.3677, -0.3633,\n",
       "        -0.3645, -0.3593, -0.3638, -0.3593, -0.3701, -0.3597, -0.3593, -0.3651,\n",
       "        -0.3706, -0.3610, -0.3635, -0.3592, -0.3593, -0.3678, -0.3593, -0.3593,\n",
       "        -0.3576, -0.3643, -0.3593, -0.3605, -0.3638, -0.3604, -0.3646, -0.3599,\n",
       "        -0.3641, -0.3594, -0.3593, -0.3741, -0.3593, -0.3638, -0.3646, -0.3593,\n",
       "        -0.3639, -0.3705, -0.3593, -0.3593, -0.3593, -0.3635, -0.3601, -0.3641,\n",
       "        -0.3593, -0.3650, -0.3634, -0.3590, -0.3587, -0.3593, -0.3593, -0.3593,\n",
       "        -0.3593, -0.3593, -0.3593, -0.3595, -0.3604, -0.3525, -0.3639, -0.3595,\n",
       "        -0.3595, -0.3646, -0.3593, -0.3792, -0.3591, -0.3593, -0.3650, -0.3593,\n",
       "        -0.3642, -0.3646, -0.3636, -0.3752, -0.3652, -0.3593, -0.3593, -0.3621,\n",
       "        -0.3600, -0.3622, -0.3595, -0.3826, -0.3593, -0.3794, -0.3685, -0.3631,\n",
       "        -0.3639, -0.3593, -0.3810, -0.3646, -0.3643, -0.3595, -0.3694, -0.3593,\n",
       "        -0.3593, -0.3607, -0.3638, -0.3665, -0.3593, -0.3657, -0.3593, -0.3645,\n",
       "        -0.3635, -0.3593, -0.3593, -0.3593, -0.3593, -0.3608, -0.3593, -0.3595,\n",
       "        -0.3597, -0.3593, -0.3593, -0.3592, -0.3524, -0.3593, -0.3593, -0.3593,\n",
       "        -0.3576, -0.3593, -0.3593, -0.3643, -0.3510, -0.3593, -0.3642, -0.3611,\n",
       "        -0.3576, -0.3593, -0.3637, -0.3599, -0.3594, -0.3640, -0.3592, -0.3580,\n",
       "        -0.3646, -0.3646, -0.3575, -0.3628, -0.3643, -0.3594, -0.3645, -0.3633,\n",
       "        -0.3593, -0.3593, -0.3593, -0.3586, -0.3634, -0.3551, -0.3639, -0.3642,\n",
       "        -0.3753, -0.3593, -0.3593, -0.3595, -0.3695, -0.3593, -0.3621, -0.3691,\n",
       "        -0.3658, -0.3643, -0.3593, -0.3645, -0.3650, -0.3695, -0.3647, -0.3602,\n",
       "        -0.3593, -0.3602, -0.3593, -0.3630, -0.3593, -0.3593, -0.3771, -0.3593,\n",
       "        -0.3593, -0.3596, -0.3635, -0.3594, -0.3644, -0.3593, -0.3594, -0.3592,\n",
       "        -0.3593, -0.3590, -0.3687, -0.3593, -0.3601, -0.3593, -0.3593, -0.3643,\n",
       "        -0.3593, -0.3593, -0.3645, -0.3641, -0.3662, -0.3544, -0.3609, -0.3593,\n",
       "        -0.3757, -0.3635, -0.3593, -0.3669, -0.3638, -0.3760, -0.3691, -0.3628,\n",
       "        -0.3593, -0.3642, -0.3593, -0.3593, -0.3906, -0.3593, -0.3608, -0.3593,\n",
       "        -0.3593, -0.3593, -0.3630, -0.3643, -0.3615, -0.3601, -0.3674, -0.3594,\n",
       "        -0.3593, -0.3593, -0.3593, -0.3602, -0.3602, -0.3593, -0.3652, -0.3582,\n",
       "        -0.3604, -0.3649, -0.3593, -0.3593, -0.3593, -0.3593, -0.3650, -0.3645,\n",
       "        -0.3628, -0.3593, -0.3593, -0.3598, -0.3593, -0.3604, -0.3640, -0.3591,\n",
       "        -0.3678, -0.3594, -0.3593, -0.3703, -0.3593, -0.3742, -0.3590, -0.3590,\n",
       "        -0.3593, -0.3617, -0.3528, -0.3528, -0.3649, -0.3593, -0.3593, -0.3595,\n",
       "        -0.3609, -0.3590, -0.3635, -0.3593, -0.3543, -0.3699, -0.3597, -0.3632,\n",
       "        -0.3700, -0.3637, -0.3593, -0.3588, -0.3593, -0.3593, -0.3611, -0.3591,\n",
       "        -0.3593, -0.3806, -0.3593, -0.3643, -0.3593, -0.3590, -0.3634, -0.3779,\n",
       "        -0.3593, -0.3560, -0.3592, -0.3595, -0.3593, -0.3572, -0.3595, -0.3593,\n",
       "        -0.3646, -0.3604, -0.3593, -0.3841, -0.3638, -0.3589, -0.3697, -0.3646,\n",
       "        -0.3631, -0.3596, -0.3590, -0.3633, -0.3650, -0.3653, -0.3595, -0.3631,\n",
       "        -0.3586, -0.3638, -0.3593, -0.3604, -0.3593, -0.3595, -0.3593, -0.3647,\n",
       "        -0.3593, -0.3593, -0.3787, -0.3648, -0.3593, -0.3593, -0.3593, -0.3593,\n",
       "        -0.3635, -0.3593, -0.3591, -0.3593, -0.3712, -0.3664, -0.3650, -0.3686,\n",
       "        -0.3646, -0.3633, -0.3593, -0.3593, -0.3646, -0.3592, -0.3648, -0.3593,\n",
       "        -0.3710, -0.3635, -0.3593, -0.3593, -0.3576, -0.3593, -0.3608, -0.3643,\n",
       "        -0.3593, -0.3590, -0.3893, -0.3574, -0.3736, -0.3611, -0.3593, -0.3592,\n",
       "        -0.3635, -0.3593, -0.3593, -0.3604, -0.3605, -0.3657, -0.3593, -0.3614,\n",
       "        -0.3593, -0.3636, -0.3651, -0.3593, -0.3593, -0.3642, -0.3593, -0.3587,\n",
       "        -0.3593, -0.3627, -0.3650, -0.3650, -0.3647, -0.3593, -0.3589, -0.3626,\n",
       "        -0.3611, -0.3594, -0.3593, -0.3593, -0.3593, -0.3641, -0.3734, -0.3593,\n",
       "        -0.3593, -0.3787, -0.3603, -0.3631, -0.3593, -0.3610, -0.3593, -0.3593,\n",
       "        -0.3585, -0.3593, -0.3644, -0.3645, -0.3593, -0.3643, -0.3817, -0.3622,\n",
       "        -0.3593, -0.3641, -0.3589, -0.3593, -0.3593, -0.3683, -0.3683, -0.3645,\n",
       "        -0.3593, -0.3593, -0.3592, -0.3655, -0.3887, -0.3655, -0.3647, -0.3591,\n",
       "        -0.3637, -0.3616, -0.3593, -0.3659, -0.3636, -0.3655, -0.3593, -0.3593],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
    "sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
    "sent_scores = self.ext_layer(sents_vec, mask_cls).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=[training_stats[i]['Training Loss'] for i in range(epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_par_epoch=[torch.mean(torch.as_tensor(score_stat[x:x+57])) for x in range(0,len(score_stat),57)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,ax=plt.subplots(2,figsize=(18,14))\n",
    "ax[0].plot([i for i in range(epochs)],train_loss)\n",
    "ax[1].plot([i for i in range(epochs)],score_par_epoch)\n",
    "ax[1].set(xlabel=\"époques\",ylabel=\"score loss\",title=\"score par époques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CEL=nn.CrossEntropyLoss()\n",
    "#CEL(pred_output[0][0][0],output[0])\n",
    "[torch.sum(-output[i]+pred_output[0][i][0]) for i in range(output.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pred_output,open('pred_output.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation=pickle.load(open('validation.pickle','rb'))\n",
    "valid_dataloader = DataLoader(\n",
    "            validation,\n",
    "            sampler = RandomSampler(validation),\n",
    "            batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summa=torch.load(\"model_essai.pt\")\n",
    "device = torch.device(\"cuda\")\n",
    "summa=Summarizer(device=device)\n",
    "summa.load_state_dict(torch.load(\"model_essai.pt\"))\n",
    "#summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summa.eval()\n",
    "for step,batch in enumerate(valid_dataloader):\n",
    "    if step==0:\n",
    "        input_id_valid = batch[0].to(device)\n",
    "        mask_valid = batch[1].to(device)\n",
    "        clss_valid = batch[2].to(device)\n",
    "        mask_cls_valid=batch[3].to(device)\n",
    "        output_valid=batch[4].to(device)\n",
    "        with torch.no_grad():\n",
    "            pred=summa(x=input_id_valid,mask=mask_valid,clss=clss_valid,mask_cls=mask_cls_valid,output=output_valid)\n",
    "            print(pred)\n",
    "    #valid.append(batch)\n",
    "    else:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=F1_score()\n",
    "f1(pred,output_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "torch.tensor([1/(3/512),1/((512-3)/512)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CEL=torch.nn.CrossEntropyLoss(weight=torch.tensor([1/(3/512),1/((512-3)/512)]))\n",
    "CEL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = pred[0]#torch.randn(3, 5, requires_grad=True)\n",
    "target = output_valid[0]#torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([512])) that is different to the input size (torch.Size([16, 512])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.MSELoss()\n",
    "loss(pred[0],output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 512]), torch.Size([16, 512]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0].shape,output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "topvec=camem(input_id,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss poids égaux, type L1, binary=True : tensor(0.3859, device='cuda:0', requires_grad=True)\n",
      "Loss poids égaux, type L1, binary=False : tensor(0.3859, device='cuda:0', requires_grad=True)\n",
      "Loss poids égaux, type sum, binary=False : tensor(197.5638, device='cuda:0', requires_grad=True)\n",
      "Loss poids différents, type L1, binary=False : tensor(0.2720, device='cuda:0', requires_grad=True)\n",
      "Loss poids différents, type sum, binary=False : tensor(451.0387, device='cuda:0', requires_grad=True)\n",
      "Loss poids différents, type L1, binary=True : tensor(0.3513, device='cuda:0', requires_grad=True)\n",
      "Loss poids différents, type sum, binary=True : tensor(426.0840, device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-76-34ca28d4352e>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w=torch.repeat_interleave(torch.tensor(self.weights[0]),y.shape[1])\n",
      "<ipython-input-76-34ca28d4352e>:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w=torch.repeat_interleave(torch.tensor(self.weights[0]),y.shape[1])\n"
     ]
    }
   ],
   "source": [
    "score=F1_score()\n",
    "alpha=0.5\n",
    "weights=torch.Tensor([(1/(1-alpha))*1/((512-20)/512),(1/alpha)*1/((20)/512)])\n",
    "loss=Weighted_Loss(weight=torch.Tensor([1,1]),loss_type='L1',binary=True)#reduction = 'batchmean')\n",
    "loss_4=Weighted_Loss(weight=torch.Tensor([1,1]),loss_type='L1',binary=False)#reduction = 'batchmean')\n",
    "loss_5=Weighted_Loss(weight=torch.Tensor([1,1]),loss_type='sum',binary=False)#reduction = 'batchmean')\n",
    "loss_2=Weighted_Loss(weight=weights,loss_type='L1',binary=False)\n",
    "loss_3=Weighted_Loss(weight=weights,loss_type='sum',binary=False)\n",
    "loss_2_=Weighted_Loss(weight=weights,loss_type='L1',binary=True)\n",
    "loss_3_=Weighted_Loss(weight=weights,loss_type='sum',binary=True)\n",
    "last=topvec.last_hidden_state\n",
    "#SMHA=SMHA_Linear_classifier(torch.Size([512,768]),8,768)\n",
    "SMHA=SMHA_classifier(torch.Size([512,768]),8)\n",
    "sortie=SMHA(last).mean(dim=2).to(device)\n",
    "print(\"Loss poids égaux, type L1, binary=True :\",loss(sortie,output))\n",
    "print(\"Loss poids égaux, type L1, binary=False :\",loss_4(sortie,output))\n",
    "print(\"Loss poids égaux, type sum, binary=False :\",loss_5(sortie,output))\n",
    "print(\"Loss poids différents, type L1, binary=False :\",loss_2(sortie,output))\n",
    "print(\"Loss poids différents, type sum, binary=False :\",loss_3(sortie,output))\n",
    "print(\"Loss poids différents, type L1, binary=True :\",loss_2_(sortie,output))\n",
    "print(\"Loss poids différents, type sum, binary=True :\",loss_3_(sortie,output))\n",
    "#,loss_2(sortie,output),loss_3(sortie,output),loss_4(sortie,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.0813, 51.2000])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
