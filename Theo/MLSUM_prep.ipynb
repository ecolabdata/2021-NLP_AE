{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim==3.8.3\n",
    "!pip install unidecode\n",
    "!pip install torch\n",
    "!pip install sentencepiece\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import functools\n",
    "import operator\n",
    "import psutil\n",
    "from joblib import Parallel,delayed\n",
    "from functools import partial\n",
    "from time import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.chdir(\"/home/jovyan/work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU dispo : 56\n"
     ]
    }
   ],
   "source": [
    "from fats import Make_Extractive,Word_Cleaning\n",
    "W2V=None\n",
    "print(\"CPU dispo :\",psutil.cpu_count())\n",
    "cpu_max=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichiers=[i for i in os.listdir() if (i[-6:]=='pickle') and (i[:5]=='MLSUM')]\n",
    "fichiers_val=[i for i in fichiers if 'test' in i]\n",
    "\n",
    "for f in fichiers_val:\n",
    "    print(f)\n",
    "    base=pickle.load(open(f,'rb'))\n",
    "    name=f.split('.')[0].split('fr')[-1][1:]\n",
    "\n",
    "    print('Début du nettoyage du fichier',name,':')\n",
    "    cleaning_time=time()\n",
    "    WC=Word_Cleaning(n_jobs=cpu_max,sentence=True,threshold=True,seuil=2,lemma=False,seuil_carac=3)\n",
    "\n",
    "    summary=base.summary.values\n",
    "    summary=WC.make_summary(summary)\n",
    "    pickle.dump(summary,open('summary_clean_'+name+'.pickle','wb'))\n",
    "\n",
    "    text=base.text.values\n",
    "    text=WC.make_documents(text)\n",
    "    cleaning_end=time()\n",
    "    pickle.dump(text,open('text_clean_'+name+'.pickle','wb'))\n",
    "    print(\"Durée du nettoyage :\",round((cleaning_end-cleaning_time)/60,2),\"minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries=[i for i in os.listdir() if 'summary' in i]\n",
    "texts=[i for i in os.listdir() if 'text' in i]\n",
    "\n",
    "text=pickle.load(open(texts[1],'rb'))\n",
    "summary=pickle.load(open(summaries[1],'rb'))\n",
    "\n",
    "assert len(text)==len(summary)\n",
    "\n",
    "dim=100\n",
    "fenetre=20\n",
    "minimum=1\n",
    "epo=5\n",
    "\n",
    "ME=Make_Extractive(cpu=cpu_max)\n",
    "\n",
    "sentence=ME.make_w2v_sentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La création du dico vocab a pris: 0.0 minutes.\n",
      "Le découpage des phrases a pris: 0.09 minutes.\n"
     ]
    }
   ],
   "source": [
    "#Pour 15800, le W2V a pris 3,3 minutes \n",
    "if W2V==None:\n",
    "    import gensim\n",
    "    try:\n",
    "        W2V=gensim.models.Word2Vec(size=dim,window=fenetre,min_count=minimum)\n",
    "    except:\n",
    "        W2V=gensim.models.Word2Vec(vector_size=dim,window=fenetre,min_count=minimum)\n",
    "    W2V.build_vocab(sentence)\n",
    "    print(\"Démarrage de l'entraînement du modèle Word2Vec.\")\n",
    "    start=time()\n",
    "    W2V.train(sentence,total_examples=W2V.corpus_count,epochs=epo)\n",
    "    end=time()\n",
    "    print(\"Le modèle W2V est désormais entraîné et cela a pris :\",round((end-start)/60,2),\"minutes.\")\n",
    "\n",
    "#Pour 15800, le dico vocab a pris 0 minutes \n",
    "start=time()\n",
    "try:\n",
    "    vocab=list(W2V.wv.vocab.keys())\n",
    "except:\n",
    "    vocab=list(set(W2V.wv.key_to_index))\n",
    "end=time()\n",
    "print(\"La création du dico vocab a pris:\",round((end-start)/60,2),\"minutes.\")\n",
    "\n",
    "#Pour 15800, le découpage a pris 0 minutes \n",
    "start=time()\n",
    "text=Parallel(n_jobs=cpu_max)(delayed(ME.make_splitting)(s) for s in text)\n",
    "#text=[[i.split() for i in s] for s in docs]\n",
    "\n",
    "summary=ME.make_splitting(summary)\n",
    "\n",
    "#summary=[[i for i in s.split() if i in vocab] for s in summary]\n",
    "end=time()\n",
    "print(\"Le découpage des phrases a pris:\",round((end-start)/60,2),\"minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0067)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent=text[0]\n",
    "s=sent[0]\n",
    "torch.stack(\n",
    "             [cosim(torch.as_tensor(W2V.wv[i]),torch.as_tensor(W2V.wv[summary[text.index(sent)]]))\n",
    "             for i in s]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.__version__>'4.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosim=torch.nn.CosineSimilarity(-1)\n",
    "\n",
    "score=[]\n",
    "erreur=[]\n",
    "for sent in tqdm(text):\n",
    "    score_=[]\n",
    "    for s in sent:\n",
    "        try:\n",
    "            score_.append(torch.stack(\n",
    "             [cosim(torch.as_tensor(W2V[i]),torch.as_tensor(W2V[summary[text.index(sent)]]))\n",
    "             for i in s]).mean())\n",
    "        except:\n",
    "            erreur.append([text.index(sent),sent.index(s)])\n",
    "            print(\"Attention, l'élément\",erreur[-1],\"n'a pas pu être encodé.\")\n",
    "            continue\n",
    "    try:\n",
    "        score.append(torch.stack(score_))\n",
    "    except:\n",
    "        score.append(torch.Tensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fats import Make_Extractive\n",
    "\n",
    "summaries=[i for i in os.listdir() if ('summary_word_3' in i) and ('train' not in i)]\n",
    "texts=[i for i in os.listdir() if ('text_word_3' in i) and ('train' not in i)]\n",
    "W2V=pickle.load(open('W2V_train.pickle','rb'))\n",
    "\n",
    "cpu_max=30\n",
    "n=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/9822 [00:00<06:18, 25.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9822\n",
      "Début de la création de l'output :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9822/9822 [12:11<00:00, 13.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durée de la création d'output : 12.2 minutes.\n",
      "Nombre d'erreurs pendant l'encodage du test : 5.06 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/9823 [00:00<06:04, 26.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9822 19645\n",
      "Début de la création de l'output :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9823/9823 [12:56<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durée de la création d'output : 12.94 minutes.\n",
      "Nombre d'erreurs pendant l'encodage du test : 4.72 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/9822 [00:00<05:29, 29.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19645 29467\n",
      "Début de la création de l'output :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 6107/9822 [07:06<06:30,  9.52it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for summary,text in zip(summaries,texts):\n",
    "        name='train_'+summary.split('.')[0].split('_')[-1]\n",
    "        print(name)\n",
    "        text=pickle.load(open(text,'rb'))\n",
    "        summary=pickle.load(open(summary,'rb'))\n",
    "        score_final=[]\n",
    "        erreur_f=[]\n",
    "        for l in range(n):\n",
    "            if l<3:\n",
    "                l_1=int(len(text)*(l)/n)\n",
    "                l_2=int(len(text)*(l+1)/n)\n",
    "                print(l_1,l_2)\n",
    "                text_=text[l_1:l_2]\n",
    "                summary_=summary[l_1:l_2]\n",
    "                print(\"Début de la création de l'output :\")\n",
    "                output_time=time()\n",
    "                ME=Make_Extractive(cpu=cpu_max,fenetre=20,minimum=1,d=100,epochs=25)\n",
    "                score,text_2,summary_2,erreur=ME.make_output(text_,summary_,W2V=W2V)\n",
    "                output_end=time()\n",
    "                print(\"Durée de la création d'output :\",round((output_end-output_time)/60,2),\"minutes.\") #42 minutes pour 15,8K lignes\n",
    "                print(\"Nombre d'erreurs pendant l'encodage du test :\",round(len(erreur)/len(text_)*100,2),\"%\")\n",
    "                score_final+=score\n",
    "                erreur_f+=erreur\n",
    "                pickle.dump(score,open('score_'+name+'_'+str(l+1)+'.pickle','wb'))\n",
    "            else:\n",
    "                break\n",
    "        score=pickle.load(open('score_train_3.pickle','rb'))\n",
    "        erreur=pickle.load(open('erreur_word_train_3.pickle','rb'))\n",
    "        score_final=score+score_final\n",
    "        pickle.dump(score_final,open('score_'+name+'.pickle','wb'))\n",
    "        erreur_f=erreur_f+erreur\n",
    "        pickle.dump(erreur_f,open('erreur_word_'+name+'.pickle','wb'))\n",
    "        for l in range(n):\n",
    "            os.remove('score_'+name+'_'+str(l+1)+'.pickle')\n",
    "        #print(score_final)\n",
    "        #pickle.dump(W2V,open('W2V_'+name+'.pickle','wb'))\n",
    "        #pickle.dump(text_2,open('text_word_'+name+'.pickle','wb'))\n",
    "        #pickle.dump(summary_2,open('summary_word_'+name+'.pickle','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29467"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(score_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68758\n",
      "3477\n",
      "98225\n"
     ]
    }
   ],
   "source": [
    "score=pickle.load(open('score_train_3.pickle','rb'))\n",
    "print(len(score))\n",
    "erreur=pickle.load(open('erreur_word_train_3.pickle','rb'))\n",
    "print(len(erreur))\n",
    "score_final=score+score_final\n",
    "print(len(score_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4827\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(score_final,open('score_'+name+'.pickle','wb'))\n",
    "erreur_f=erreur_f+erreur\n",
    "print(len(erreur_f))\n",
    "pickle.dump(erreur_f,open('erreur_word_'+name+'.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(score_final,open('score_train_1_1.pickle','wb'))\n",
    "pickle.dump(erreur_f,open('erreur_word_train_1_1.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98225, 98225)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(score)+29467,len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de données : 2242804\n",
      "Nombre d'erreurs pendant l'encodage du text : 0.16 %\n"
     ]
    }
   ],
   "source": [
    "#text=pickle.load(open('text_word_3.pickle','rb'))\n",
    "\n",
    "x=0\n",
    "for t in text:\n",
    "    x+=len(t)\n",
    "print(\"Nombre de données :\",x)\n",
    "#erreur=pickle.load(open('erreur_word_train_3.pickle','rb'))\n",
    "print(\"Nombre d'erreurs pendant l'encodage du text :\",round(len(erreur)/x*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98225 98225\n",
      "95722\n",
      "98225 98225\n",
      "95709\n",
      "2516 2516\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "text=pickle.load(open('text_clean_train_3.pickle','rb'))\n",
    "score=pickle.load(open('score_train_3.pickle','rb'))\n",
    "print(len(text),len(score))\n",
    "\n",
    "relou=[i for i in range(len(score)) if len(text[i])!=len(score[i])]\n",
    "print(len(relou))\n",
    "\n",
    "def remove_empty(text):\n",
    "    while '' in text:\n",
    "          text.remove('')\n",
    "    return text\n",
    "\n",
    "text_ne=Parallel(n_jobs=cpu_max)(delayed(remove_empty)(t) for t in text)\n",
    "print(len(text_ne),len(score))\n",
    "relou=[i for i in range(len(score)) if len(text_ne[i])!=len(score[i])]\n",
    "print(len(relou))\n",
    "\n",
    "index=[i for i in range(len(text_ne)) if i not in relou]\n",
    "\n",
    "text_ne=[text_ne[i] for i in index]\n",
    "score_ne=[score[i] for i in index]\n",
    "print(len(text_ne),len(score_ne))\n",
    "\n",
    "relou=[i for i in range(len(text_ne)) if len(text_ne[i])!=len(score_ne[i])]\n",
    "print(len(relou))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V=pickle.load(open('W2V_train.pickle','rb'))\n",
    "\n",
    "erreur_text={}\n",
    "score_relou=[]\n",
    "for k in relou:\n",
    "    nul=[]\n",
    "    for i in range(len(text[k])):\n",
    "        try:\n",
    "            W2V.wv[text[k][i].split()]\n",
    "        except:\n",
    "            #print(k,i)\n",
    "            nul.append(i)\n",
    "    erreur_text[k]=nul\n",
    "    if len(erreur_text[k])>0:\n",
    "        for e in list(reversed(erreur_text[1915])):\n",
    "            score_=torch.cat([score[k][:e],torch.Tensor([0]),score[k][e:]])\n",
    "        score_relou.append(score_)\n",
    "\n",
    "erreur_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts=[i for i in os.listdir() if 'text_clean' in i]\n",
    "#scores=[i for i in os.listdir() if 'score' in i]\n",
    "\n",
    "#for text,score in zip(texts,scores):\n",
    "#name=text.split('.')[0].split('_')[-1]\n",
    "\n",
    "#text=pickle.load(open(text,'rb'))\n",
    "#score=pickle.load(open(score,'rb'))\n",
    "\n",
    "print(\"Début de la création de l'encoding\")\n",
    "encoding_time=time()\n",
    "ME=Make_Extractive(cpu_max)\n",
    "dico=ME.make_encoding(text_ne,score_ne,voc_size=12000,split=1)\n",
    "encoding_end=time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Durée de la création d'output :\",round((encoding_end-encoding_time)/60,2),\"minutes.\")\n",
    "\n",
    "pickle.dump(dico,open('dico_train_3.pickle','wb'))\n",
    "print(\"Les données du val ont été sauvegardées !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15128it [02:13, 113.61it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer\n",
    "tokenizer=CamembertTokenizer('essai.model')\n",
    "ME=Make_Extractive(cpu=cpu_max)\n",
    "doc_encod=partial(ME.document_encoding,tokenizer=tokenizer,dim=512)   \n",
    "output=[]#Parallel(n_jobs=cpu_max)(delayed(\n",
    "erreur=[]\n",
    "for t,s in tqdm(zip(text_ne,score_ne)):\n",
    "    try:\n",
    "        output.append(doc_encod(t,s))\n",
    "    except:\n",
    "        erreur.append(text_ne.index(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9102, 9151]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "erreur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(score_ne[9102]),len(text_ne[9102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ouais\n"
     ]
    }
   ],
   "source": [
    "print('ouais')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
