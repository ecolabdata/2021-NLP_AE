Corpus-based Statistical Screening for Phrase Identification
AbstractPurpose: The authors study the extraction of useful phrases from a natural language database by statistical methods. The aim is to leverage human effort by providing preprocessed phrase lists with a high percentage of useful material. Method: The approach is to develop six different scoring methods that are based on different aspects of phrase occurrence. The emphasis here is not on lexical information or syntactic structure but rather on the statistical properties of word pairs and triples that can be obtained from a large database. Measurements: The Unified Medical Language System (UMLS) incorporates a large list of humanly acceptable phrases in the medical field as a part of its structure. The authors use this list of phrases as a gold standard for validating their methods. A good method is one that ranks the UMLS phrases high among all phrases studied. Measurements are 11-point average precision values and precision-recall curves based on the rankings. Result: The authors find of six different scoring methods that each proves effective in identifying UMLS quality phrases in a large subset of MEDLINE. These methods are applicable both to word pairs and word triples. All six methods are optimally combined to produce composite scoring methods that are more effective than any single method. The quality of the composite methods appears sufficient to support the automatic placement of hyperlinks in text at the site of highly ranked phrases. Conclusion: Statistical scoring methods provide a promising approach to the extraction of useful phrases from a natural language database for the purpose of indexing or providing hyperlinks in text.

Modern computer-based retrieval systems have the potential to retrieve from a large database those documents that satisfy a Boolean query composed of virtually any words and phrases the operator may desire. Given this power, it is reasonable to ask what purpose indexing could serve. There is actually the potential for a large benefit. As shown by a number of studies,,,,,, there is great inconsistency in the terms people use to describe the same subject. In the words of Bates, "In study after study, across a wide range of environments, it has been found that for any target topic, people will use a very wide range of different terms, and no one of those terms will occur very frequently." Indexing can alleviate this problem by expanding the list of terms by which a document may be accessed. Thus, one path to improved indexing is to obtain a list of terms (words and phrases) sufficient to include a high percentage of the terms that people will actually use in querying a database, and add sufficient synonymy information to allow a query expressing a particular concept to access those documents that are indexed with an expression synonymous with the query. The Unified Medical Language System (UMLS) includes not only a large list of important terms but also a synonymy capability relating these terms in the Metathesaurus. It is intended, among other things, to provide a solution to the indexing problem just out-lined.,  The system as it stands is, of course, incomplete and will, for the forseeable future, stand to benefit from increased coverage of the latest terminology in the various fields covered by MEDLINE. Our hypothesis is that statistical information about the occurrence of phrases in MEDLINE can provide a useful screen for candidate phrases that are of similar quality to the material already in the UMLS. A person generally does not possess the kind of information that is available in this way. This information can, however, be readily obtained by automatic processing and can serve as a guide to terms that would make useful additions to UMLS. Such guidance may be important, given the limited human resources that are available to integrate terminology into the Metathesaurus. The development of a controlled vocabulary of indexing phrases is not the only use to which our methods can contribute. The ranking of phrases by quality can also be used as an aid to automatically place hyperlinks in text. We are currently involved in a project to link phrases in MEDLINE documents to appropriate sections and subsections of books in the field of bio-medicine that may provide the reader with additional information about the subjects of the phrases. Here the most useful phrases in a MEDLINE record are marked as hot links that are "clickable" to reach a book or books of potential interest. The first book, Molecular Biology of the Cell, is available at . To see the book links, the user must select a single document and, when it is displayed, click on the "book" link to the right of the document. In the applications sections of this paper, we show how this type of linkage can be produced automatically on the basis of phrase ranking. The hyperlinks viewable at  differ mainly in incorporating some human review of the phrase lists used. Several methods have been used historically in attempts to extract useful words and phrases from document collections for purposes of indexing. Since Luhn's pioneering work on indexing,,  the importance of term frequency information has been recognized. The frequencies of both phrases and the words that compose them are important in the phrase extraction method of Jones et al. A second kind of information that can be helpful in indexing is the distribution of frequencies of a term within documents. It has been proposed that non-content-bearing terms are well modeled by a single Poisson distribution, whereas content-bearing terms require a two-Poisson or some more complicated model.,, In fact, one of our scoring methods is based on the degree that a term's distribution deviates from a Poisson distribution. The greater this deviation, the more likely that the term is a useful one. In addition to this method we employ other relatively simple scoring methods based on term frequencies, co-occurrence, and word suffixes. The objective is to locate phrases that are grammatically acceptable and specific in their meaning, yet occur with sufficient frequency in the database to make them useful additions to UMLS. There are many methods of noun phrase extraction based on natural language processing that we have not examined. Proprietary methods such as CLARIT and NPtool were not of interest, since we seek to understand the methods in as much detail as possible. The transformation-based parsing developed by Brill,,  hidden Markov part-of-speech tagging as in the Xerox Tagger, and parsing based on a probabilistic grammer as in CHOPPER are potentially of greater interest. However, these are complex tools designed for a different task than ours. They seek to assign part-of-speech tags as a basis for natural language parsing, whereas we seek to identify those phrases that are not only syntactically correct but also readily recognizable by human beings as useful and descriptive of a subject area. Even if natural language parsing methods can contribute to the accomplishment of our task, we must still ask whether their complexity is necessary to its accomplishment. We seek to show in what follows that simpler methods suffice. We will examine the Xerox Tagger to show that it adds little to what can be accomplished by our scoring methods. Another phrase extraction task that has been studied is phrase extraction with the purpose of improving retrieval by expanded automatic indexing on test collections. The methods of phrase identification are based on part-of-speech tagging as well as some statistical methods. This area is exemplified by the work of Fagan and Lewis and Croft. Interestingly, while there has been some success with this approach in improving retrieval, the results are not consistently good. This led Lewis and Jones to comment that "... automatically combining single indexing terms into multiword indexing phrases or more complex structures has yielded only small and inconsistent improvements over the simple use of multiple terms in a query." We mention this area mainly to distinguish it from our own work. Different goals and different methods of evaluation characterize the two approaches. Instead of seeking to improve retrieval in some automatic system, we seek to identify those phrases that are the most user friendly, and we evaluate our success by how well we are able to identify a set of phrases (UMLS) that are maintained by human beings because they are found descriptively useful. We begin with a description of the different data sets we study and how they are constructed. We then present the scoring methods that are designed to distinguish useful phrases from simple co-locations of terms. We describe our approach to evaluation of the scoring methods and present results on the effectiveness of the scoring methods when applied to a large database of MEDLINE records. Besides the six scoring methods that we find useful, we evaluate two other methods and find that they do not add significantly to overall effectiveness. We discuss application of our methods to extraction of candidates for UMLS and also as a procedure for marking text with hyperlinks. The paper concludes with a discussion and description of future directions.
We consider word pairs and word triples from two different sources. The first source is the UMLS,  developed by the National Library of Medicine. The UMLS (9th edition, 1998) was obtained from the National Library of Medicine on CD. (Information regarding its availability for research purposes may be found at .) Our second data source is the set of 304,057 MEDLINE records with abstracts and entry dates in the year 1996. We shall refer to this document set as MED96. These two data sets are processed somewhat differently, because they differ considerably in content. However, we will use a procedure to normalize text strings that is the same for both. We normalize text in three steps: All alphabetic characters are lowercased, all non-alphanumeric characters are replaced by blanks, and multiple blank spaces between words are converted to single blank spaces. The UMLS is processes as follows: First, all text strings are obtained from the UMLS "mrcon" (concept name) file. From the resulting set of strings, any containing punctuation marks or stop words are deleted. For this purpose a list of 310 common stop words is used. Finally, the remaining strings undergo normalization and removal of any duplicates. The result is a set we denote by Uall. This is the set of all phrases that we obtain from UMLS. From Uall we extract the subset of strings consisting of two words each. The result is 156,086 word pairs, denoted by U2. In the same way we extract all three-word phrases from Uall. The result is 103,367 word triples, denoted by U3. MED96 is processed somewhat differently. We first process the titles and abstracts of the MED96 records, breaking at punctuation marks and stop words. The resulting set of strings is normalized and made unique, to produce the set Mseg. This is the set of longest phrases that we obtain from MED96. By M2 we denote the set of all contiguous word pairs that can be obtained from the members of Mseg. For example, the four-word string "escherichia coli cell growth" from Mseg yields the three overlapping two-word phrases "escherichia coli", "coli cell", and "cell growth" in M2. By M3 we denote the set of all contiguous word triples that can be obtained from the same source. The difference in the processing of the UMLS and MED96 is perhaps worth emphasizing. The strings in Uall are essentially a subset of the strings that occur in the UMLS "mrcon" file, except for lower casing, and as such by and large represent syntactically reasonable and semantically meaningful phrases. The subset U2 is just those strings in Uall that are composed of two words. There are longer phrases in Uall that could be broken up into contiguous two-word phrases and added to U2, but we do not do this because we do not know whether these would be of high quality. The same applies to the derivation of U3. The U2 and U3 sets represent our gold standard for good phrases, and we seek to keep their quality as high as possible. On the other hand, Mseg is a large set of strings that are obtained from all the text in MED96. Many of these are not, as phrases, of high quality. The M2 set is derived from Mseg by taking all those strings in Mseg that consist of two words as well as all those contiguous word pairs that may be obtained from longer phrases in Mseg. The longer phrases in Mseg are broken up in this way and added to M2 because, even if the longer phrases are of poor quality, some two-word substrings may be of good quality and such potential should not be ignored. The same basic method applies to the derivation of M3. It will then be the task of the scoring procedures that we introduce to separate the good from the bad.
In this section, we define the various scoring methods we want to apply to the word pairs in the set M2 and the word triples in the set M3 extracted from the MED96 database. Our goal is to define scoring methods that will allow us to find the most useful phrases occurring in a database. We only define the methods and give some justification for their choice here. Their systematic evaluation is the subject of the next sections. We begin by describing scoring methods for the word pairs in the set M2. When these have been described we indicate the modifications necessary for application of the same methods to M3. Method I | Given a word pair in the set M2, we perform a simple count of the number of MED96 documents that contain that word pair (phrase frequency). Dividing this count by the normalization factor N (the size of MED96), the corresponding score s1 is  The normalization factor is a constant and is optional here, but it might allow one to compare results across databases more readily. Rationale: Phrases as well as single words follow a Zipf-like distribution, with a plethora of very low frequency phrases and progressively fewer examples in the higher-frequency categories. Rare phrases are of only limited value as discriminators. Naturally, the UMLS tends to avoid very low frequency terms, which explains the utility of frequency as a scoring method.  Method II | Given a word pair in the set M2, we count the number of documents in MED96 that contain both words, even if not as a contiguous pair. The result is called the co-occurrence, and the score s2 is  It is evident that this score always lies between 0 and 1. Rationale: As an example, consider two word pairs, "diabetes mellitus" and "wide tumor." For "diabetes mellitus," the phrase frequency (the number of the MED96 documents that contain "diabetes mellitus") is 2,465, the co-occurrence (the number of the MED96 documents that contain both "diabetes" and "mellitus" but not necessarily as a contiguous pair) is 2,468, and thus the score s2 is 0.99. For "wide tumor," the phrase frequency is 3, the co-occurrence is 352, and the score s2 is 0.008. Two words that tend to co-occur only in the form of a phrase often form a high-quality phrase.  Method III | Given a phrase in the set M2, we examine all occurrences of the phrase throughout the text of MED96. As described in the previous section, the text of MED96 is broken at stop words and punctuation marks, and the resulting phrases compose the elements of Mseg. Each occurrence of a phrase that immediately precedes one of these break points (at a stop word or a punctuation mark) is counted in phraseend for that phrase. For example, the word pair "lipoprotein cholesterol" occurs in the sentence fragment"... serum total and high-density lipoprotein cholesterol, C-reactive protein, and plasma fibrinogen." Here it occurs just before a comma, and hence this occurrence will contribute 1 to the score phraseend for the phrase "lipoprotein cholesterol." The meaning of "end" in this context is that "lipoprotein cholesterol" is at the right-hand end of the longer phrase "high-density lipoprotein cholesterol" that this sentence fragment contributes to Mseg. In the same sentence fragment is also the phrase "density lipoprotein," but since this occurrence of "density lipoprotein" does not immediately precede a stop word or punctuation mark, it does not add to the score phraseend for "density lipoprotein." Again, normalizing by the total number of MED96 documents, the score s3 is  Rationale: The scoring method s3 is a quasi-syntactic categorization. The head of a phrase tends to occur at the right-hand end. The number of times that a phrase ends at a stop word or a punctuation mark is a measure of the likelihood that its last word is a head and, therefore, of whether the phrase can stand alone. For example, "central nervous" will be followed immediately by a stop word or punctuation mark much less frequently than will the phrase "nervous system."  Method IV | The score s4 is obtained as an odds ratio based on the last three characters of the last word in the phrase. The definition is  where the number p(good phrase|l1l2l3) is the probability of being a good phrase given the last three letters l1l2l3. From a simple rearrangement of the Bayes theorem, we can infer s4, i.e.,  where p(l1l2l3|good phrase) is obtained as the distribution of the last three letters of the last word over all phrases of Uall, and p(l1l2l3) is obtained as the distribution of the last three letters of the last word over all the phrases in M2. Rationale: The scoring method s4 is based on the characteristic suffixes that tend to be applicable to different word classes and different parts of speech. For example, if the last three characters of the last word in a word pair are "-ely," as in "bind cooperatively," the phrase may not be of very high quality (s4 = 0.044). However, if the last three characters of the last word in a word pair are "-ine" (often the suffix of a chemical or medicine), such as "basophil histamine," "biogenic amine," and "catalytic histadine," the phrase may be of high quality (s4 = 2.45).  Method V | Our next scoring method is based on the hypergeometric distribution. For a given word pair in the set M2, let nf equal the number of MED96 documents that contain the first word in the pair and ns equal the number of MED96 documents that contain the second word in the pair. Again, let N denote the total number of MED96 documents. If x denotes the co-occurrence of the two words and if we assume the words are randomly distributed, then x obeys the hypergeometric probability distribution, defined by  Using this distribution we may obtain the P value, i.e., the probability that the actual co-occurrence is as great as or greater than the observed co-occurrence if the words are assumed to be randomly distributed:  where min(nf, ns) is the smaller of the numbers nf and ns. Then s5 is given by  Rationale: If the observed co-occurrence of a word pair is quite above the expected value for a random incident, the phrase may be a useful one. For example, for the word pair "surgically curable" we have nf = 1,583, ns = 148, N = 304,057, and co-occurrence = 3. The estimated co-occurrence (Eco) from the hypergeometric distribution is  The words "surgically" and "curable" appear together at a near random level in the database. However, for the word pair "immunodeficiency virus," we have nf = 3,505, ns = 11,143, N = 304,057, and co-occurrence = 2,845. Also, the expected co-occurrence (Eco) from the hypergeometric distribution is  The observed co-occurrence (= 2,845) for the words "immunodeficiency" and "virus" in the MED96 database is far above the random level (Eco = 128). The score s5, which is the negative logarithm of the P value, is the measure of the discrepancy from a random incident (s5 = 1.36859 for the word pair "surgically curable" and s5 = 3,527.45 for the word pair "immunodeficiency virus").  Method VI | Our final scoring method is based on the distribution of the within-document term frequencies. We define a randomly distributed phrase as one whose distribution among documents is described by a Poisson distribution. For such a phrase, the probability P(k) that fjd, the number of occurrences of phrase j in document d, is equal to k is given by  where the parameter lambdaj is the average number of occurrences of j per document over the whole database. Therefore, we can find the probability p that the given phrase j occurs one or more times in d:  We denote by q (= 1 - p) its complement, i.e., the probability that j does not occur in d. Let us consider an experiment that consists of N repeated independent Bernoulli trials with parameter p. Let E(= N  p) refer to the expected number of documents containing the phrase considered. If a phrase occurs multiple times in few documents, we say it has a tendency to clump. We measure the tendency to clump by how much the observed number of documents containing the phrase (i.e., phrase frequency) falls below the expectation E. For a given word pair we calculate the P value, i.e., the probability that phrase frequency would be less than or equal to that observed if it were generated by the Poisson distribution of equation (9).  Then the score s6 is given by  Rationale: Intuitively, the occurrences of a term sensitive to content will have a greater tendency to clump than will those of a non-content-bearing term. This is common with names of things. Therefore, if the phrase considered carries content, we expect that the observed phrase frequency will be much less than E. The scoring method s6 is a measure of this clumping compared with a Poisson distributed phrase. For example, s6 is 168.08 for the name "ulcerative colitis," which is highly specific, but s6 is 0.65 for "common cancer," which is a general concept. The same scoring methods discussed for the word pairs in the set M2 can be applied to the word triples in the set M3 with a slightly altered definition of co-occurrence. Given a word triple in the set M3, phrase frequency is unchanged as the number of documents in MED96 that contain the word triple. However, for s2, co-occurrence is the number of documents that contain both the first word and the second and third words contiguously as a word pair. The same definition of co-occurrence applies when computing s5 and in equation (6), ns is the number of documents that contain the second and third words as a word pair. The value of phraseend in the score s3 in equation (3) is the number of phrases extracted from MED96 documents in which the given word triple in the set M3 occurs at the right-hand end. The score s4 likewise has the obvious interpretation where only p(l1l2l3) is changed to the distribution of letters (l1l2l3) appearing at the end of word triples in the set M3.
Here we assume a given set of phrases M and a scoring method S that computes a real number for each phrase in the set M. The scoring method S allows us to rank the set M so that the phrases are in order of decreasing score. We also assume that we have available a golden set of good-quality phases G (this generally requires human judgment). The evaluation methods we use are measures of how well the scoring method S moves phrases in the set M I G to the top of the listing of M by rank (the lowest ranks). In other words, we consider the phrases in M I G to be the relevant phrases we are attempting to find in the set M. This allows us to view the problem as a retrieval problem and to apply some of the standard measures used in information retrieval science. In particular, we will apply recall and precision, which are the most commonly used measures in information retrieval. Because recall and precision are generally defined for a given rank and the results are different for each rank considered, we will also use the 11-point average precision as a single summary measure for the complete ranking. We will further use interpolated recall-precision curves as a graphic way of viewing performance. The 11-point average precision and interpolated recall-precision curves are widely used in presenting the results of retrieval experiments.,, Other measures are used in the information retrieval setting, such as the E-measure, expected search length, and relevance information. While these measures have some advantages in specialized settings, they are less intuitive and less well known, and we feel they offer no advantage in our setting. Let us assume that the number of phrases in M is N and that the phrases are represented by the list  indexed in rank order, where the order is that of decreasing score S. Further, let  Then the precision (Pr) and the recall (Rr) of S for the retrieval down to rank r is defined by  and  respectively. (Here parallelXparallel denotes the number of elements in the set X.) In words, Pr is the fraction of phrases retrieved down to rank r that are in M intersection G, and Rr is the fraction of phrases in M intersection G that are found in the retrieval down to rank r. Since the precision is usually high early in the ranks and becomes progressively lower at higher ranks, and since the recall is low at the early ranks but increases with increasing rank, it is possible to gain a useful picture of performance by graphing precision as a function of recall (a so-called recall-precision curve). However, precision does not always strictly decrease as one moves down the ranks. Because of this, it has become common to perform an interpolation on the precision value associated with a given recall level, in which that precision is replaced by any higher precision that may occur at a higher recall level. For example, if the precision 0.38 is calculated from equation (14) and the corresponding recall is 0.10, but a precision of 0.43 is found at a recall level of 0.20, then the value 0.38 is replaced by 0.43 as the accepted precision at recall level 0.10. In this way noise in the data may be reduced and the curve smoothed. We apply interpolation to obtain precision values at the 11 recall values of 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, and 100 percent. (Notice that the precision at 0 recall is the highest precision found at one or more ranks of retrieval, since at zero ranks Pr in equation (14) is undefined). These 11 recall-precision pairs are used to produce interpolated recall-precision curves. We also average the 11 precision values together to produce the 11-point average precision as an overall summary performance measure.
Here we give the results of applying the scoring methods (defined under Scoring Methods) to the word pairs M2 and word triples M3 (described under Data Sources and Preparation). For the purpose of evaluation, the set of good phrases for M2 is G2 = U2 intersection M2 and the set of good phrases for M3 is G3 = U3 intersection M3. (The values U2 and U3 are defined under Data Sources and Preparation.) There are 26,131 phrases in G2 and 9,234 phrases in G3. The 11-point interpolated recall-precision curves for our scoring methods applied to the sets M2 and M3 are shown in  and . It can be clearly seen that each scoring method moves the relevant phrases toward the top of the lists. The 11-point average precision for each scoring method has been computed.  is the list of scoring methods and the 11-point average precision values on the set of word pairs M2, and  provides corresponding data for the word triples in the set M3. For either M2 or M3 it is possible to combine the different scoring methods to produce a composite score. For example, we may take the linear combination of the logarithm of each score si with a coefficient xi and denote the resulting score as s:      Because the logarithm and the exponential functions are monotonically increasing functions of their arguments, s as defined by equation (15) is equivalent to its exponential for ranking purposes, and we would have obtained the same results if we have defined s as a product of the factors .. Ranking the phrases in order of a decreasing combined score s with the coefficients xi where i = 1... 6, we can obtain the 11-point average precision. Through numeric study, we may seek the coefficients for which the combined score s gives the maximum 11-point average precision. We have iteratively maximized the 11-point average precision on one coefficient at a time. For example, x1 is varied with the remaining five coefficients x2, x3, x4, x5, and x6 fixed. This procedure is repeated until none of the coefficients can be altered to increase the 11-point average precision. We find that the combined score s with the coefficients x1 = -0.8, x2 = 3.9, x3 = 0.9, x4 = 3.1, x5 = 1.2, and x6 = 1.7 ---i.e.,   ---gives the maximum 11-point average precision we are able to achieve for the word pairs in the set M2. The result is listed in the final row of . Likewise, the combined score s with the coefficients x1 = -1.0, x2 = 1.5, x3 = 1.7, x4 = 2.6, x5 = 1.0, and x6 = 2.2 ---i.e.,   ---gives the maximum 11-point average precision, listed in the final row of , that is applicable to the word triples in the set M3. To get an idea how much each score contributes to the maximum 11-point average precision, we subtract its contribution from the combined score s. The resultant score (which is the optimal combined score s minus the contribution of an individual scoring method) and the 11-point average precision value after applying it are listed in the third and the fourth columns of  for the word pairs in the set M2. The third and the fourth columns of  provide corresponding data for the word triples in the set M3. We can see that the precision of each scoring method does not directly add to the total precision found for the combined score s. For example, although the scoring method s6 alone gives the largest 11-point average precision for both word pairs and triples, it does not make the largest contribution to the combined score s. This implies in particular that our scoring methods are not independent of each other. In computing the combined scores given in equations (16) and (17), we have employed an optimization procedure to choose the coefficients. In such a computation there is always the possibility of overtraining, so that the results are applicable only to the particular data set on which we have done the training. We suspected that this would not be a problem in the current situation, because we are training only six parameters and are employing a very large number of data, namely, 304,057 MEDLINE documents. Furthermore, in actuality there are only five independent parameters, because ranking is not changed when one multiplies all scores by a constant. To test for overtraining, we randomly split the set of documents into disjoint data sets, MED1 and MED2, where MED96 = MED1  MED2, parallelMED1parallel = 152,028, and parallelMED2parallel = 152,029. We then re-estimated the parameters for equation (16) on each subset independent of the other to produce the optimal 11-point average precision on that subset. The results are given in  along with the coefficients for the whole of MED96 for comparison.  Two things stand out here. First, we see that the coefficients obtained on MED1 and MED2 are almost identical. Second, we see that there is a significant difference between the coefficients obtained on the subsets and the whole database for scores s1 and s3. This suggests that database size might be a factor in these scores. To complete the comparison, we tested the effectiveness of each set of coefficients listed in  on the two subsets MED1 and MED2. The results are given in .  These results suggest that all three sets of coefficients have very close to the same effectiveness on MED1 and MED2. They are all within 1 percent of each other. This effectively rules out any significant overtraining. At the same time it suggests that the effectiveness of the composite scores is not very sensitive to the coefficients assigned to log(s1) and log(s3). While this is true, it is also true that we can degrade the composite quite drastically if we make these coefficients too large. A large coefficient for the contribution of a single score will cause this score to dominate the composite and the effectiveness of the composite to approach the effectiveness of the single score, as recorded in Tables  and . Figure 1 | Recall-precision curves for the scoring methods applied to the set M2. Recall-precision curves for the scoring methods applied to the set M2. The 11 precision values are interpolated so that precision is a nonincreasing function of recall. Figure 2 | Recall-precision curves for the scoring methods applied to the set M3. Recall-precision curves for the scoring methods applied to the set M3. The 11 