{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Préparation des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dataiku\n",
    "import pandas as pd, numpy as np\n",
    "from dataiku import pandasutils as pdu\n",
    "\n",
    "# Read recipe inputs\n",
    "base_id_avistxt = dataiku.Dataset(\"base_id_avistxt\")\n",
    "base_id_avistxt_df = base_id_avistxt.get_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification naïve des lignes qui sont quasiment certainement des avis non rendus\n",
    "def target(row):\n",
    "    score = row.hasAA + row.hasAO + row.hasDI + row.hasPO\n",
    "    if score >=1:\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "\n",
    "base_id_avistxt_df['target'] = base_id_avistxt_df.apply(target,axis=1)\n",
    "steps = 3\n",
    "\n",
    "#Fonction longueur \"créneau\" aux limites arbitraires...\n",
    "def creneau(rowlen,steps = 3,treshold = 4000, mini = 2000):\n",
    "    x = np.linspace(mini,treshold,num = steps)\n",
    "    k= 0\n",
    "    for rng in x:\n",
    "        if rowlen<=rng:\n",
    "             return(k)\n",
    "        else:\n",
    "            k+=1\n",
    "    return(k)\n",
    "\n",
    "base_id_avistxt_df['creneauLen'] = base_id_avistxt_df['len'].apply(creneau,steps = steps)\n",
    "base_id_avistxt_df['creneauLen'] = base_id_avistxt_df['creneauLen']/steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longueur normalisée\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax_scale = MinMaxScaler().fit(base_id_avistxt_df[['len']])\n",
    "base_id_avistxt_df['normLen'] = minmax_scale.transform(base_id_avistxt_df[['len']])\n",
    "\n",
    "\n",
    "#imprimer un retour à la ligne pour une meilleur clarete de lecture\n",
    "print('\\n********** Normalisation*********\\n')\n",
    "\n",
    "print('Moyenne apres le Min max Scaling :\\nMYCT={:.2f}'\n",
    ".format(base_id_avistxt_df['normLen'].mean()))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Valeur minimale et maximale pour la feature apres min max scaling: \\nMIN={:.2f}, MAX={:.2f}'\n",
    ".format(base_id_avistxt_df['normLen'][:].min(), base_id_avistxt_df['normLen'][:].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clustering des avis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 KMeans sur les features ajoutées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repérage de quelques mots clés, longueur normalisée, longueur créneau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = pd.concat([base_id_avistxt_df.iloc[:,3:8]]+[base_id_avistxt_df.creneauLen],axis=1)\n",
    "inst = KMeans(n_clusters = 3,random_state= 0,)\n",
    "inst.fit(X)\n",
    "\n",
    "res = pd.DataFrame(inst.labels_)\n",
    "base_id_avistxt_df['Kres'] = res\n",
    "base_id_avistxt_df.sort_values('len').head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 KMeans sur les features textuelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vecto = TfidfVectorizer()\n",
    "mat =vecto.fit_transform(base_id_avistxt_df.texte)\n",
    "\n",
    "X2 = mat\n",
    "inst2 = KMeans(n_clusters = 3,random_state= 0,)\n",
    "inst2.fit(X2)\n",
    "\n",
    "res2 = pd.DataFrame(inst2.labels_)\n",
    "base_id_avistxt_df['Kres2'] = res2\n",
    "base_id_avistxt_df.sort_values('len').head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 DBScan sur les features ajoutées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "X3 = pd.concat([base_id_avistxt_df.iloc[:,3:7],base_id_avistxt_df.creneauLen],axis=1)\n",
    "\n",
    "params = [x/100 for x in range(1,31)]\n",
    "clu = []\n",
    "nois = []\n",
    "for param in params:\n",
    "    db = DBSCAN(eps=param, min_samples=len(X3)//15).fit(X3)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    base_id_avistxt_df['DBSCAN']  = db.labels_\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    clu.append(n_clusters_)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "    nois.append(n_noise_)\n",
    "\n",
    "    #print('Estimated number of clusters: %d' % n_clusters_)\n",
    "    #print('Estimated number of noise points: %d' % n_noise_)\n",
    "\n",
    "params2 = [x for x in range(1,31)]\n",
    "nois2 = []\n",
    "clu2 = []\n",
    "for param in params2:\n",
    "    db = DBSCAN(eps=0.15, min_samples=len(X3)//param).fit(X3)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    base_id_avistxt_df['DBSCAN']  = db.labels_\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    clu2.append(n_clusters_)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "    nois2.append(n_noise_)\n",
    "\n",
    "    #print('Estimated number of clusters: %d' % n_clusters_)\n",
    "    #print('Estimated number of noise points: %d' % n_noise_)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "figure, axis = plt.subplots(2, 2)\n",
    "\n",
    "\n",
    "axis[0, 0].plot(params,nois)\n",
    "axis[0, 0].set_title(\"Noise wrto Eps\")\n",
    "\n",
    "axis[0, 1].plot(params,clu)\n",
    "axis[0, 1].set_title(\"Num cluster wrto Eps\")\n",
    "\n",
    "axis[1, 0].plot(params2,nois2)\n",
    "axis[1, 0].set_title(\"Noise wrto min_samples\")\n",
    "\n",
    "axis[1, 1].plot(params2,clu2)\n",
    "axis[1, 1].set_title(\"Num cluster wrto min_samples\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sélection de quelques paramètres intéressants : eps, min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.15, min_samples=len(X3)//30).fit(X3)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "base_id_avistxt_df['DBSCAN']  = db.labels_+1\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "\n",
    "base_id_avistxt_df.sort_values('len').head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Calcul du score de clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant qu'on a les résultats de 3 clustering, on calcule un score de 0 (avis rendu) à 3 (avis non rendu). Les 0 et 3 constituent le set d'entrainement du classifieur car on considère que le clustering a bien fait son travail. Les autres sont un set de test.\n",
    "\n",
    "L'interprétation faite des résultats est donnée dans le tableau suivant. On va transformer les colonnes pour que les résultats coincident."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Kres\tKres2\tDBSCAN\n",
    "  1\t      0 \t  1     Avis\n",
    "  2\t      2\t      2 \tBruit\n",
    "  0\t      1\t      0\t    Non Avis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_id_avistxt_df.replace(2,3,inplace = True )\n",
    "\n",
    "base_id_avistxt_df.Kres.replace(0,2,inplace = True )\n",
    "base_id_avistxt_df.Kres.replace(1,0,inplace = True )\n",
    "\n",
    "base_id_avistxt_df.Kres2.replace(1,2,inplace = True )\n",
    "\n",
    "base_id_avistxt_df.DBSCAN.replace(0,2,inplace = True )\n",
    "base_id_avistxt_df.DBSCAN.replace(1,0,inplace = True )\n",
    "\n",
    "base_id_avistxt_df.replace(3,1,inplace = True )\n",
    "\n",
    "base_id_avistxt_df.sort_values('len').head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcul du score\n",
    "base_id_avistxt_df['Score'] = (base_id_avistxt_df.Kres+base_id_avistxt_df.Kres2+base_id_avistxt_df.target+base_id_avistxt_df.DBSCAN)\n",
    "\n",
    "#Constituion de la BDD des valeurs sûres :\n",
    "#Non avis certains\n",
    "df1 = base_id_avistxt_df[base_id_avistxt_df['Score']==7]\n",
    "#Avis certains\n",
    "df0 = base_id_avistxt_df[base_id_avistxt_df['Score']==0]\n",
    "df_train = pd.concat([df1,df0])\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_train['Score'] = df_train['Score']/7\n",
    "\n",
    "#Constitution de la BDD des valeurs incertaines/test\n",
    "\n",
    "df_test = base_id_avistxt_df[(base_id_avistxt_df['Score']>0)& (base_id_avistxt_df['Score']<7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Clustering sur clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification supervisée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Entrainement sur le set d'avis correctement classés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Sur les features calculées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Séparation des matrices de features/vecteur target. On classifie sur les indicatrices + la longueur normalisée\n",
    "X_train,y_train = df_train.iloc[:,3:7],df_train.Score\n",
    "X_test,y_test = df_test.iloc[:,3:7],df_test.Score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(random_state = 0)\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "#Les non avis sont classés 1, les avis 0\n",
    "df_test['predict'] = rfc.predict(X_test)\n",
    "df_test.sort_values('len').head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Sur les features textuelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On classifie cette fois sur le texte\n",
    "vecto2 = TfidfVectorizer(ngram_range = (1,2))\n",
    "X_train2  =vecto2.fit_transform(df_train.texte)\n",
    "\n",
    "X_test2 = vecto2.transform(df_test.texte)\n",
    "\n",
    "rfc2 = RandomForestClassifier(random_state = 0)\n",
    "rfc2.fit(X_train2,y_train)\n",
    "\n",
    "#Les non avis sont classés 1, les avis 0\n",
    "df_test['predict2'] = rfc2.predict(X_test2)\n",
    "df_test.sort_values('len').head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour predict et predict2, on a 0 si l'avis est considéré comme rendu, 1 si l'avis est considéré comme non rendu. Il semble qu'il n'y ait que trop peu de features pour le premier classifieur : dès que la ligne d'indicatrices de contient pas de 1, l'avis est considéré comme rendu même si ça n'est pas le cas. Le 2eme classifieur (sur le texte) semble plus pertinent => on va conserver ces résultats.\n",
    "\n",
    "Résultat très intéressant car le classifieur arrive à éliminer des textes longs, dont on pourrait s'attendre à ce qu'ils soient pertinent, mais qui sont en réalité des listes d'avis non rendus (cf avis 2639113) ou des documents qui ressemblent à des templates non complétés (avis 1299508).\n",
    "\n",
    "En revanche quelques avis courts semblant présenter un intérêt sont également éliminés (avis 2061190)\n",
    "\n",
    "Sur la \"frontière\", les avis semblent être bien classés la plupart du temps (ex : 374757 avis long de 4700 caractères bien classé en avis non rendu, alors que l'avis 115486 qui a presque exactement les mêmes caractériques est pertinent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Création de la database finale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée la database a partir des résultats de la classification supervisée sur les features textuelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1 = df0\n",
    "part2 = df_test[df_test['predict2']==0]\n",
    "base_id_avis_txt_sorted_df = pd.concat([df0,part2]).filter(['id','texte'],axis = 1)\n",
    "\n",
    "base_id_avis_txt_sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = len(base_id_avistxt_df)\n",
    "moyleninit = base_id_avistxt_df.len.mean()\n",
    "reste = len(base_id_avis_txt_sorted_df)\n",
    "moylenfin = (part1.len.sum()+part2.len.sum())/reste\n",
    "medinit = base_id_avistxt_df.len.median()\n",
    "medfin = pd.concat([part1,part2]).len.median()\n",
    "\n",
    "lminav,lmaxav = base_id_avistxt_df.len.min(),base_id_avistxt_df.len.max()\n",
    "lminap,lmaxap = min(part1.len.min(),part2.len.min()),max(part1.len.max(),part2.len.max())\n",
    "\n",
    "avistacite = 49+275+65+0+188+65+13+112+61+52+138\n",
    "avisrendu = 416+338+286+253+251+239+114+112+104+65+40\n",
    "\n",
    "print(\" Part d'avis réellement rendus : \", round(reste/initial*100,1),\"%\\n\",\n",
    "      \"Stat du rapport annuel (cf message slack 15 Mars de Marc) : \",\n",
    "      round((avisrendu)/(avisrendu+avistacite)*100,1),\"% \\n\")\n",
    "print(\"Longueur moyenne :\",'\\n',\"Avant : \", round(moyleninit),'\\n',\"Après : \", round(moylenfin),'\\n')\n",
    "print(\"Longueur mediane :\",'\\n',\"Avant : \", round(medinit),'\\n',\"Après : \", round(medfin))\n",
    "\n",
    "print(\"\\nLongueur max :\", \"\\t \\t Longueur min :\"\n",
    "      '\\n',\"Avant : \", lmaxav,\" \\t  Avant : \",lminav,\n",
    "      '\\n',\"Après : \", lmaxap,\" \\t  Après : \",lminap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write recipe outputs\n",
    "base_id_avis_txt_sorted = dataiku.Dataset(\"base_id_avis_txt_sorted\")\n",
    "base_id_avis_txt_sorted.write_with_schema(base_id_avis_txt_sorted_df)"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "compute_base_id_avis_txt_sorted",
  "creator": "rpartouche",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python (env Ruben)",
   "language": "python",
   "name": "py-dku-venv-ruben"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
